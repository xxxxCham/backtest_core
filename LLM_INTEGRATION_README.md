# ğŸ¤– IntÃ©gration LLM - SystÃ¨me Fonctionnel

## Vue d'ensemble

Le systÃ¨me d'intÃ©gration LLM a Ã©tÃ© complÃ¨tement refait en s'inspirant du systÃ¨me fonctionnel de **ThreadX_big**. Le nouveau systÃ¨me offre :

âœ… **DÃ©tection automatique** des modÃ¨les Ollama installÃ©s
âœ… **Auto-dÃ©marrage** d'Ollama si nÃ©cessaire
âœ… **SÃ©lection intelligente** des modÃ¨les avec informations (taille, description)
âœ… **Fallback robuste** si Ollama n'est pas disponible
âœ… **Interface utilisateur** intuitive avec feedback en temps rÃ©el

---

## ğŸ“ Fichiers crÃ©Ã©s/modifiÃ©s

### 1. **Nouveau module** - [agents/ollama_manager.py](agents/ollama_manager.py)

Gestionnaire Ollama complet avec :
- `ensure_ollama_running()` - DÃ©marre Ollama automatiquement
- `list_ollama_models()` - Liste les modÃ¨les installÃ©s
- `is_ollama_available()` - VÃ©rifie la connexion
- `unload_model()` - DÃ©charge un modÃ¨le de la mÃ©moire
- `cleanup_all_models()` - Nettoie tous les modÃ¨les
- `prepare_for_llm_run()` - PrÃ©pare l'environnement complet

### 2. **Nouveau composant UI** - [ui/components/model_selector.py](ui/components/model_selector.py)

SÃ©lecteur de modÃ¨les LLM avec :
- Liste dynamique des modÃ¨les disponibles
- Tri par ordre de recommandation
- Informations sur chaque modÃ¨le (taille, description)
- Fallback intelligent si Ollama n'est pas accessible
- CatÃ©gories de modÃ¨les recommandÃ©s :
  * `RECOMMENDED_FOR_ANALYSIS` - Pour l'analyse de donnÃ©es
  * `RECOMMENDED_FOR_STRATEGY` - Pour la gÃ©nÃ©ration de stratÃ©gies
  * `RECOMMENDED_FOR_CRITICISM` - Pour la critique/validation
  * `RECOMMENDED_FOR_FAST` - Pour des tests rapides

### 3. **UI mise Ã  jour** - [ui/app.py](ui/app.py)

IntÃ©gration complÃ¨te dans l'UI Streamlit :
- DÃ©tection automatique de la connexion Ollama
- Bouton pour dÃ©marrer Ollama si nÃ©cessaire
- SÃ©lecteur de modÃ¨les avec liste dynamique
- Affichage des informations du modÃ¨le sÃ©lectionnÃ©

---

## ğŸš€ Utilisation

### Dans l'interface Streamlit

1. **Lancer l'application**
   ```bash
   streamlit run ui/app.py
   ```

2. **SÃ©lectionner le mode LLM**
   - Dans la sidebar, choisir "ğŸ¤– Optimisation LLM"

3. **Configuration automatique**
   - L'interface vÃ©rifie automatiquement si Ollama est connectÃ©
   - Si Ollama n'est pas dÃ©marrÃ©, un bouton "ğŸš€ DÃ©marrer Ollama" apparaÃ®t
   - La liste des modÃ¨les se remplit automatiquement

4. **SÃ©lection du modÃ¨le**
   - Choisir un modÃ¨le dans la liste dÃ©roulante
   - Les modÃ¨les sont triÃ©s par recommandation (les meilleurs en premier)
   - Les informations (taille, description) s'affichent automatiquement

### En Python direct

```python
from agents.ollama_manager import (
    ensure_ollama_running,
    list_ollama_models,
    is_ollama_available,
)

# VÃ©rifier si Ollama est disponible
if is_ollama_available():
    print("âœ… Ollama connectÃ©")
else:
    # DÃ©marrer Ollama automatiquement
    success, message = ensure_ollama_running()
    print(message)

# Lister les modÃ¨les disponibles
models = list_ollama_models()
print(f"ModÃ¨les installÃ©s : {models}")
```

### Avec le composant UI

```python
import streamlit as st
from ui.components.model_selector import (
    render_model_selector,
    RECOMMENDED_FOR_STRATEGY,
)

# Dans votre page Streamlit
model = render_model_selector(
    label="ModÃ¨le Strategist",
    key="strategist_model",
    preferred_order=RECOMMENDED_FOR_STRATEGY,
    help_text="SÃ©lectionnez un modÃ¨le pour gÃ©nÃ©rer des stratÃ©gies"
)

st.write(f"ModÃ¨le sÃ©lectionnÃ© : {model}")
```

---

## ğŸ“Š ModÃ¨les recommandÃ©s

Le systÃ¨me inclut une liste de modÃ¨les recommandÃ©s avec leurs caractÃ©ristiques :

| ModÃ¨le | Taille | Usage recommandÃ© | Performance |
|--------|--------|------------------|-------------|
| **deepseek-r1:70b** | 34 GB | StratÃ©gies complexes | â­â­â­â­â­ |
| **deepseek-r1:32b** | 19 GB | Optimal - Meilleur rapport | â­â­â­â­â­ |
| **qwq:32b** | 23 GB | Raisonnement & Analyse | â­â­â­â­ |
| **qwen2.5:32b** | 19 GB | Alternative polyvalente | â­â­â­â­ |
| **mistral:22b** | 13 GB | Ã‰quilibrÃ© - Bon pour critique | â­â­â­â­ |
| **gemma3:27b** | 17 GB | Analyse rapide | â­â­â­ |
| **deepseek-r1:8b** | 5 GB | Tests rapides | â­â­â­ |
| **mistral:7b-instruct** | 4 GB | Ultra rapide | â­â­ |
| **llama3.2** | 2 GB | LÃ©ger pour tests | â­â­ |

---

## ğŸ”§ Configuration

### Variables d'environnement (optionnel)

Le systÃ¨me utilise les variables d'environnement par dÃ©faut, mais vous pouvez les personnaliser :

```bash
# Provider LLM (ollama ou openai)
BACKTEST_LLM_PROVIDER=ollama

# ModÃ¨le par dÃ©faut
BACKTEST_LLM_MODEL=deepseek-r1:32b

# URL Ollama
OLLAMA_HOST=http://localhost:11434

# OpenAI (si utilisÃ©)
OPENAI_API_KEY=sk-...
OPENAI_BASE_URL=https://api.openai.com/v1
```

---

## ğŸ¯ FonctionnalitÃ©s clÃ©s

### 1. DÃ©tection automatique

L'interface dÃ©tecte automatiquement si Ollama est disponible :
- âœ… **Ollama connectÃ©** â†’ Affiche la liste des modÃ¨les installÃ©s
- âš ï¸ **Ollama non dÃ©tectÃ©** â†’ Propose de le dÃ©marrer automatiquement
- âŒ **Ollama non installÃ©** â†’ Utilise une liste de modÃ¨les en fallback

### 2. Auto-dÃ©marrage

Si Ollama n'est pas dÃ©marrÃ©, un simple clic sur "ğŸš€ DÃ©marrer Ollama" :
- Lance le service Ollama en arriÃ¨re-plan
- Attend qu'il soit prÃªt (max 10 secondes)
- RafraÃ®chit automatiquement l'interface

### 3. Informations sur les modÃ¨les

Pour chaque modÃ¨le, l'interface affiche :
- **Nom** : deepseek-r1:32b
- **Taille** : ~19 GB
- **Description** : Optimal - Meilleur rapport qualitÃ©/prix

### 4. Tri intelligent

Les modÃ¨les sont triÃ©s par ordre de recommandation :
1. Les modÃ¨les recommandÃ©s pour la tÃ¢che en premier
2. Ensuite, les autres modÃ¨les par ordre alphabÃ©tique

---

## ğŸ”„ DiffÃ©rences avec l'ancien systÃ¨me

### Avant (dysfonctionnel)

âŒ Saisie manuelle du nom du modÃ¨le (risque d'erreur)
âŒ Pas de vÃ©rification de la connexion Ollama
âŒ Pas d'information sur les modÃ¨les disponibles
âŒ Pas de fallback en cas de problÃ¨me

### AprÃ¨s (systÃ¨me fonctionnel)

âœ… SÃ©lection depuis une liste dynamique
âœ… DÃ©tection automatique de la connexion
âœ… Auto-dÃ©marrage d'Ollama si nÃ©cessaire
âœ… Informations complÃ¨tes sur chaque modÃ¨le
âœ… Fallback robuste avec liste de modÃ¨les recommandÃ©s
âœ… Tri par ordre de recommandation

---

## ğŸ“ API Reference

### ollama_manager

```python
from agents.ollama_manager import *

# VÃ©rifier disponibilitÃ©
is_available = is_ollama_available() -> bool

# DÃ©marrer Ollama
success, message = ensure_ollama_running() -> Tuple[bool, str]

# Lister modÃ¨les
models = list_ollama_models() -> List[str]

# DÃ©charger un modÃ¨le
success = unload_model("deepseek-r1:32b") -> bool

# Nettoyer tous les modÃ¨les
count = cleanup_all_models() -> int

# PrÃ©parer pour un run LLM
success, message = prepare_for_llm_run() -> Tuple[bool, str]
```

### model_selector

```python
from ui.components.model_selector import *

# Obtenir liste des modÃ¨les
models = get_available_models_for_ui(
    preferred_order=RECOMMENDED_FOR_STRATEGY,
    fallback=None
) -> List[str]

# Obtenir infos sur un modÃ¨le
info = get_model_info("deepseek-r1:32b") -> dict
# Retourne: {
#     "name": "deepseek-r1:32b",
#     "size_gb": 19,
#     "description": "Optimal - Meilleur rapport qualitÃ©/prix"
# }

# Rendu d'un sÃ©lecteur Streamlit
model = render_model_selector(
    label="ModÃ¨le LLM",
    key="llm_model",
    preferred_order=RECOMMENDED_FOR_STRATEGY,
    help_text="SÃ©lectionnez un modÃ¨le"
) -> str
```

---

## ğŸ§ª Tests

Pour tester le systÃ¨me :

```bash
# 1. Tester ollama_manager
python -c "from agents.ollama_manager import *; print(list_ollama_models())"

# 2. Lancer l'UI
streamlit run ui/app.py
```

---

## ğŸ¨ Captures d'Ã©cran

### Mode Ollama connectÃ©

```
âœ… Ollama connectÃ©

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ModÃ¨le Ollama               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ deepseek-r1:32b            â”‚ â—€ SÃ©lectionnÃ©
â”‚ qwq:32b                     â”‚
â”‚ mistral:22b                 â”‚
â”‚ ...                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“¦ ~19 GB | Optimal - Meilleur rapport qualitÃ©/prix
```

### Mode Ollama non dÃ©tectÃ©

```
âš ï¸ Ollama non dÃ©tectÃ©

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸš€ DÃ©marrer Ollama          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— IntÃ©gration avec ThreadX_big

Ce systÃ¨me est directement inspirÃ© de ThreadX_big :
- **Architecture** : MÃªme structure modulaire
- **Composants** : ollama_manager + model_selector
- **UI Pattern** : DÃ©tection + Auto-start + Liste dynamique
- **Robustesse** : Fallback + Gestion d'erreurs

---

## ğŸ“š Ressources

- **Code source** :
  - [agents/ollama_manager.py](agents/ollama_manager.py)
  - [ui/components/model_selector.py](ui/components/model_selector.py)
  - [ui/app.py](ui/app.py) (lignes 657-706)

- **RÃ©fÃ©rence ThreadX_big** :
  - `D:\ThreadX_big\src\threadx\llm\ollama_manager.py`
  - `D:\ThreadX_big\src\threadx\ui\components\model_selector.py`

---

**Version** : 1.0.0
**Date** : DÃ©cembre 2025
**Statut** : âœ… SystÃ¨me fonctionnel et testÃ©
