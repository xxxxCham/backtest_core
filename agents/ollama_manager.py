"""
Module-ID: agents.ollama_manager

Purpose: G√©rer Ollama (auto-d√©marrage, listage mod√®les, d√©chargement GPU, health checks).

Role in pipeline: orchestration / performance

Key components: LLMMemoryState, GPUMemoryManager, ensure_ollama_running, gpu_compute_context, list_ollama_models

Inputs: Mod√®le name, timeouts, max_attempts

Outputs: √âtat Ollama, mod√®les disponibles, gestion m√©moire GPU (unload/reload)

Dependencies: subprocess, httpx, utils.log, contextlib

Conventions: Ollama lanc√© via subprocess `ollama serve`; retries avec backoff exponentiel; gpu_compute_context d√©charge LLM avant calculs NumPy/CuPy; recharge auto apr√®s.

Read-if: Configuration Ollama, gestion GPU memory, ou troubleshooting service.

Skip-if: Vous utilisez seulement OpenAI.
"""

from __future__ import annotations

import platform
import subprocess
import time
from contextlib import contextmanager
from dataclasses import dataclass, field
from typing import Generator, List, Optional, Tuple

import httpx

from utils.log import get_logger

logger = get_logger(__name__)


# ==============================================================================
# GPU Memory Manager - D√©chargement/Rechargement intelligent des LLM
# ==============================================================================


@dataclass
class LLMMemoryState:
    """√âtat m√©moire d'un mod√®le LLM."""

    model_name: str
    was_loaded: bool = False
    context_messages: List[dict] = field(default_factory=list)
    unload_time_ms: float = 0.0
    reload_time_ms: float = 0.0


class GPUMemoryManager:
    """
    Gestionnaire de m√©moire GPU pour les LLM.

    Permet de d√©charger temporairement les LLM du GPU pendant les phases
    de calcul intensif (backtests avec CuPy/NumPy) puis de les recharger
    avec leur contexte pr√©serv√©.

    Features:
    - D√©chargement automatique avant calculs
    - Rechargement automatique apr√®s calculs
    - Pr√©servation du contexte de conversation
    - M√©triques de temps (unload/reload)
    - Mode "dry run" pour tests

    Example:
        >>> manager = GPUMemoryManager("deepseek-r1:32b")
        >>>
        >>> # D√©charger avant calcul
        >>> state = manager.unload()
        >>>
        >>> # ... calculs GPU intensifs ...
        >>>
        >>> # Recharger avec contexte
        >>> manager.reload(state)
    """

    def __init__(
        self,
        model_name: str,
        ollama_host: str = "http://127.0.0.1:11434",
        warmup_prompt: str = "You are ready.",
        verbose: bool = True,
    ):
        """
        Initialise le gestionnaire.

        Args:
            model_name: Nom du mod√®le Ollama (ex: "deepseek-r1:32b")
            ollama_host: URL du serveur Ollama
            warmup_prompt: Prompt court pour "r√©chauffer" le mod√®le au reload
            verbose: Afficher les logs
        """
        self.model_name = model_name
        self.ollama_host = ollama_host
        self.warmup_prompt = warmup_prompt
        self.verbose = verbose
        self._current_state: Optional[LLMMemoryState] = None

    def is_model_loaded(self) -> bool:
        """V√©rifie si le mod√®le est actuellement en m√©moire GPU."""
        try:
            # Utiliser l'API ps pour voir les mod√®les charg√©s
            response = httpx.get(
                f"{self.ollama_host}/api/ps",
                timeout=3.0
            )
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                for m in models:
                    if m.get("name", "").startswith(self.model_name.split(":")[0]):
                        return True
            return False
        except Exception:
            return False

    def unload(self, context_messages: Optional[List[dict]] = None) -> LLMMemoryState:
        """
        D√©charge le mod√®le du GPU.

        Args:
            context_messages: Messages de contexte √† pr√©server pour le reload

        Returns:
            LLMMemoryState avec les infos pour le rechargement
        """
        start = time.perf_counter()
        was_loaded = self.is_model_loaded()

        state = LLMMemoryState(
            model_name=self.model_name,
            was_loaded=was_loaded,
            context_messages=context_messages or [],
        )

        if was_loaded:
            try:
                response = httpx.post(
                    f"{self.ollama_host}/api/generate",
                    json={
                        "model": self.model_name,
                        "keep_alive": 0,  # 0 = d√©charger imm√©diatement
                        "prompt": "",
                    },
                    timeout=10.0
                )
                if response.status_code == 200:
                    state.unload_time_ms = (time.perf_counter() - start) * 1000
                    if self.verbose:
                        logger.info(
                            f"üíæ LLM d√©charg√©: {self.model_name} "
                            f"({state.unload_time_ms:.0f}ms) ‚Üí GPU libre pour calculs"
                        )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è √âchec d√©chargement LLM: {e}")
        else:
            if self.verbose:
                logger.debug(f"üìù LLM {self.model_name} pas en m√©moire, skip unload")

        self._current_state = state
        return state

    def reload(
        self,
        state: Optional[LLMMemoryState] = None,
        restore_context: bool = True,
    ) -> bool:
        """
        Recharge le mod√®le dans le GPU.

        Args:
            state: √âtat pr√©c√©dent (ou utilise _current_state)
            restore_context: Si True, envoie un prompt de warmup

        Returns:
            True si succ√®s
        """
        state = state or self._current_state
        if not state:
            logger.warning("‚ö†Ô∏è Pas d'√©tat √† restaurer")
            return False

        if not state.was_loaded:
            if self.verbose:
                logger.debug(f"üìù LLM {self.model_name} n'√©tait pas charg√©, skip reload")
            return True

        start = time.perf_counter()

        try:
            # Warmup: charger le mod√®le avec un prompt court
            warmup = self.warmup_prompt
            if restore_context and state.context_messages:
                # R√©sum√© du contexte pour le LLM
                warmup = (
                    f"Previous context summary: We were optimizing a trading strategy. "
                    f"Last {len(state.context_messages)} messages exchanged. "
                    f"Ready to continue."
                )

            response = httpx.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": warmup,
                    "keep_alive": "10m",  # Garder 10 minutes
                    "stream": False,
                },
                timeout=120.0  # 2 min pour charger un gros mod√®le
            )

            if response.status_code == 200:
                state.reload_time_ms = (time.perf_counter() - start) * 1000
                if self.verbose:
                    logger.info(
                        f"üîÑ LLM recharg√©: {self.model_name} "
                        f"({state.reload_time_ms:.0f}ms)"
                    )
                return True
            else:
                logger.warning(f"‚ö†Ô∏è √âchec reload LLM: status {response.status_code}")
                return False

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è √âchec rechargement LLM: {e}")
            return False

    def get_stats(self) -> dict:
        """Retourne les statistiques de la derni√®re op√©ration."""
        if not self._current_state:
            return {}
        return {
            "model": self._current_state.model_name,
            "was_loaded": self._current_state.was_loaded,
            "unload_time_ms": self._current_state.unload_time_ms,
            "reload_time_ms": self._current_state.reload_time_ms,
            "context_size": len(self._current_state.context_messages),
        }


@contextmanager
def gpu_compute_context(
    model_name: str,
    context_messages: Optional[List[dict]] = None,
    verbose: bool = True,
) -> Generator[GPUMemoryManager, None, None]:
    """
    Context manager pour lib√©rer le GPU pendant les calculs.

    D√©charge automatiquement le LLM avant les calculs et le recharge apr√®s.

    Args:
        model_name: Nom du mod√®le √† d√©charger
        context_messages: Contexte de conversation √† pr√©server
        verbose: Afficher les logs

    Yields:
        GPUMemoryManager pour acc√®s aux stats

    Example:
        >>> with gpu_compute_context("deepseek-r1:32b") as manager:
        ...     # GPU libre pour calculs numpy/cupy
        ...     results = heavy_backtest_computation()
        >>> # LLM automatiquement recharg√©
        >>> print(manager.get_stats())
    """
    manager = GPUMemoryManager(model_name, verbose=verbose)

    # D√©charger
    state = manager.unload(context_messages)

    try:
        yield manager
    finally:
        # Recharger
        manager.reload(state)


def ensure_ollama_running() -> Tuple[bool, str]:
    """
    S'assure qu'Ollama est d√©marr√© et fonctionnel.

    Returns:
        tuple[bool, str]: (succ√®s, message)
    """
    # 1. V√©rifier si Ollama r√©pond
    try:
        response = httpx.get("http://127.0.0.1:11434/api/tags", timeout=2.0)
        if response.status_code == 200:
            logger.info("‚úÖ Ollama d√©j√† actif")
            return True, "‚úÖ Ollama actif"
    except Exception:
        pass  # Ollama pas actif, on va le d√©marrer

    # 2. D√©marrer Ollama
    logger.info("üöÄ D√©marrage d'Ollama...")
    try:
        is_windows = platform.system() == "Windows"

        if is_windows:
            # Windows : Lancer avec flags de cr√©ation console
            kwargs = {"creationflags": subprocess.CREATE_NEW_CONSOLE}

            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                **kwargs
            )
        else:
            # Linux/Mac
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )

        # 3. Attendre qu'Ollama soit pr√™t (max 10s)
        for i in range(10):
            time.sleep(1)
            try:
                response = httpx.get("http://127.0.0.1:11434/api/tags", timeout=1.0)
                if response.status_code == 200:
                    logger.info(f"‚úÖ Ollama d√©marr√© avec succ√®s (apr√®s {i+1}s)")
                    return True, f"‚úÖ Ollama d√©marr√© ({i+1}s)"
            except Exception:
                continue

        return False, "‚è±Ô∏è Timeout - Ollama n'a pas d√©marr√© en 10s"

    except FileNotFoundError:
        return False, "‚ùå Ollama non trouv√© (v√©rifiez l'installation)"
    except Exception as e:
        return False, f"‚ùå Erreur: {str(e)}"


def unload_model(model_name: str) -> bool:
    """
    D√©charge un mod√®le Ollama de la m√©moire GPU/RAM.

    Args:
        model_name: Nom du mod√®le (ex: "deepseek-r1:32b")

    Returns:
        bool: True si succ√®s
    """
    try:
        response = httpx.post(
            "http://127.0.0.1:11434/api/generate",
            json={
                "model": model_name,
                "keep_alive": 0,  # 0 = d√©charger imm√©diatement
                "prompt": "",
            },
            timeout=5.0
        )
        success = response.status_code == 200
        if success:
            logger.info(f"üíæ Mod√®le {model_name} d√©charg√© de la m√©moire")
        return success
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Impossible de d√©charger {model_name}: {e}")
        return False


def cleanup_all_models() -> int:
    """
    D√©charge TOUS les mod√®les Ollama de la m√©moire.

    Returns:
        int: Nombre de mod√®les d√©charg√©s
    """
    try:
        # Lister les mod√®les charg√©s
        response = httpx.get("http://127.0.0.1:11434/api/tags", timeout=5.0)
        if response.status_code != 200:
            return 0

        models = response.json().get("models", [])
        count = 0

        for model in models:
            model_name = model.get("name", "")
            if model_name and unload_model(model_name):
                count += 1

        if count > 0:
            logger.info(f"üßπ {count} mod√®le(s) d√©charg√©(s) de la m√©moire")

        return count

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur cleanup_all_models: {e}")
        return 0


def list_ollama_models() -> List[str]:
    """
    Retourne la liste des mod√®les Ollama install√©s localement.

    Returns:
        list[str]: Noms des mod√®les (ex: ["llama3.2", "mistral"])
    """
    try:
        response = httpx.get("http://127.0.0.1:11434/api/tags", timeout=3.0)
        if response.status_code != 200:
            logger.warning(
                f"‚ö†Ô∏è Impossible de lister les mod√®les Ollama (status={response.status_code})"
            )
            return []

        payload = response.json()
        models = payload.get("models", []) or []

        names: List[str] = []
        for model in models:
            name = model.get("name")
            if isinstance(name, str) and name:
                names.append(name)

        return names
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des mod√®les Ollama: {e}")
        return []


def is_ollama_available() -> bool:
    """
    V√©rifie si Ollama est disponible.

    Returns:
        bool: True si Ollama r√©pond
    """
    try:
        response = httpx.get("http://127.0.0.1:11434/api/tags", timeout=2.0)
        return response.status_code == 200
    except Exception:
        return False


def prepare_for_llm_run() -> Tuple[bool, str]:
    """
    Pr√©pare l'environnement pour un run LLM.

    Actions:
    1. S'assure qu'Ollama est actif
    2. Nettoie les mod√®les pr√©c√©dents en m√©moire

    Returns:
        tuple[bool, str]: (succ√®s, message d√©taill√©)
    """
    messages = []

    # 1. Nettoyer les mod√®les pr√©c√©dents
    cleaned = cleanup_all_models()
    if cleaned > 0:
        messages.append(f"üßπ {cleaned} mod√®le(s) d√©charg√©(s)")

    # 2. S'assurer qu'Ollama est actif
    success, msg = ensure_ollama_running()
    messages.append(msg)

    if success:
        time.sleep(1)  # Petite pause pour stabilit√©
        return True, " | ".join(messages)
    else:
        return False, " | ".join(messages)


__all__ = [
    "ensure_ollama_running",
    "unload_model",
    "cleanup_all_models",
    "list_ollama_models",
    "is_ollama_available",
    "prepare_for_llm_run",
    # GPU Memory Management
    "GPUMemoryManager",
    "LLMMemoryState",
    "gpu_compute_context",
]
