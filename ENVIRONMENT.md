# Configuration - Variables d'Environnement

> **Documentation compl√®te** des variables d'environnement pour contr√¥ler le comportement du syst√®me.

---

## üîß Configuration Rapide

```bash
# Copier le template
cp .env.example .env

# √âditer avec vos valeurs
notepad .env  # Windows
nano .env     # Linux/Mac
```

---

## üìÇ Variables Disponibles

### **Donn√©es**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `DATA_DIR` | `data/sample_data` | Dossier par d√©faut pour les donn√©es d'exemple |
| `BACKTEST_DATA_DIR` | *(vide)* | Dossier personnalis√© pour fichiers Parquet/CSV |

**Exemple :**
```bash
BACKTEST_DATA_DIR=D:/Trading/Historical_Data
```

---

### **Trading & Capital**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `INITIAL_CAPITAL` | `10000` | Capital initial en USD |
| `DEFAULT_LEVERAGE` | `1` | Levier par d√©faut (1 = pas de levier) |

---

### **Performance & Parall√©lisation**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `MAX_WORKERS` | `8` | Nombre de threads pour sweep parall√®le |
| `USE_GPU` | `true` | Activer le backend GPU (CuPy) si disponible |

---

### **Logging & Observabilit√©**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `LOG_LEVEL` | `INFO` | Niveau de log g√©n√©ral : `DEBUG`, `INFO`, `WARNING`, `ERROR` |
| `BACKTEST_LOG_LEVEL` | *(vide)* | Niveau de log sp√©cifique au backtest (override LOG_LEVEL) |

**Mode Debug Complet :**
```bash
BACKTEST_LOG_LEVEL=DEBUG
```

Affiche :
- Spans chronom√©tr√©s pour chaque phase
- D√©tails des indicateurs calcul√©s
- √âtats des agents LLM
- M√©triques de performance

---

### **Configuration LLM**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `BACKTEST_LLM_PROVIDER` | `ollama` | Provider LLM : `ollama` ou `openai` |
| `BACKTEST_LLM_MODEL` | `deepseek-r1:8b` | Mod√®le par d√©faut |
| `OLLAMA_HOST` | `http://localhost:11434` | URL du serveur Ollama |
| `OPENAI_API_KEY` | *(requis si OpenAI)* | Cl√© API OpenAI |
| `BACKTEST_LLM_TEMPERATURE` | `0.7` | Temp√©rature (0.0=d√©terministe, 1.0=cr√©atif) |
| `BACKTEST_LLM_MAX_TOKENS` | `2000` | Limite de tokens par r√©ponse |

**Exemple Ollama :**
```bash
BACKTEST_LLM_PROVIDER=ollama
BACKTEST_LLM_MODEL=deepseek-r1:32b
OLLAMA_HOST=http://192.168.1.100:11434  # Serveur distant
```

**Exemple OpenAI :**
```bash
BACKTEST_LLM_PROVIDER=openai
BACKTEST_LLM_MODEL=gpt-4
OPENAI_API_KEY=sk-...
```

---

### **üî¥ GPU Memory Management (CRITIQUE)**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `UNLOAD_LLM_DURING_BACKTEST` | `False` | D√©charger LLM du GPU pendant calculs |

**‚ö†Ô∏è IMPORTANT - Comprendre cette variable :**

#### **Probl√®me :**
Les LLMs charg√©s occupent la VRAM GPU, emp√™chant les calculs NumPy/CuPy d'utiliser toute la m√©moire disponible.

#### **Solution :**
- **CPU-only systems** : `False` (d√©faut)
  - Pas de GPU ‚Üí d√©chargement inutile ‚Üí latence inutile
  - **Recommand√©** pour la plupart des utilisateurs

- **GPU systems avec CuPy** : `True`
  - Lib√®re 100% de la VRAM pour les calculs intensifs
  - Trade-off : +2-5s latence unload/reload entre it√©rations

#### **Mesure :**
```bash
# Tester sur 1 it√©ration d'optimisation
UNLOAD_LLM_DURING_BACKTEST=False  # Baseline

# Si calculs GPU OOM ou lents :
UNLOAD_LLM_DURING_BACKTEST=True   # Test avec d√©chargement
```

#### **Exemple d'impact :**

| Configuration | VRAM LLM | VRAM Calculs | Latence Iter | Recommand√© |
|---------------|----------|--------------|--------------|------------|
| False (d√©faut) | 8 GB | 16 GB restants | 0s overhead | ‚úÖ CPU-only |
| True (GPU opt) | 0 GB | 24 GB libres | +3s overhead | ‚úÖ GPU avec CuPy |

---

### **Walk-Forward Validation**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `WALK_FORWARD_WINDOWS` | `5` | Nombre de fen√™tres de validation |
| `WALK_FORWARD_MIN_TEST_SAMPLES` | `50` | Taille min du test set |

**Exemple - Validation agressive :**
```bash
WALK_FORWARD_WINDOWS=10
WALK_FORWARD_MIN_TEST_SAMPLES=100
```

---

### **Optuna Optimization**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `OPTUNA_SAMPLER` | `tpe` | Algorithme : `tpe`, `cmaes`, `random` |
| `OPTUNA_ENABLE_PRUNING` | `True` | Activer arr√™t pr√©coce des runs peu prometteurs |

---

### **Constraints & Risques**

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `MAX_OVERFITTING_RATIO` | `1.5` | Ratio max train/test avant alerte overfitting |

---

## üéØ Configurations Recommand√©es

### **1. D√©veloppement Local (CPU-only)**
```bash
# .env
DATA_DIR=data/sample_data
INITIAL_CAPITAL=10000
LOG_LEVEL=DEBUG
BACKTEST_LLM_PROVIDER=ollama
BACKTEST_LLM_MODEL=deepseek-r1:8b
UNLOAD_LLM_DURING_BACKTEST=False  # ‚ö†Ô∏è Important
```

### **2. Production GPU (NVIDIA RTX 4090)**
```bash
# .env
BACKTEST_DATA_DIR=/mnt/ssd/trading_data
INITIAL_CAPITAL=100000
USE_GPU=true
LOG_LEVEL=INFO
BACKTEST_LLM_PROVIDER=ollama
BACKTEST_LLM_MODEL=deepseek-r1:32b
UNLOAD_LLM_DURING_BACKTEST=True   # üöÄ GPU optimization ON
MAX_WORKERS=16
```

### **3. Cloud OpenAI**
```bash
# .env
BACKTEST_LLM_PROVIDER=openai
BACKTEST_LLM_MODEL=gpt-4
OPENAI_API_KEY=sk-...
UNLOAD_LLM_DURING_BACKTEST=False
LOG_LEVEL=WARNING  # R√©duire verbosit√©
```

### **4. Research Station (Multi-GPU)**
```bash
# .env
USE_GPU=true
UNLOAD_LLM_DURING_BACKTEST=True
MAX_WORKERS=32
WALK_FORWARD_WINDOWS=10
OPTUNA_SAMPLER=cmaes
LOG_LEVEL=DEBUG
```

---

## üß™ Validation Configuration

```bash
# Tester variables d'env charg√©es
python -c "import os; print(os.getenv('UNLOAD_LLM_DURING_BACKTEST', 'NOT_SET'))"

# Lancer backtest avec override
BACKTEST_LOG_LEVEL=DEBUG python __main__.py backtest -s ema_cross -d data.parquet

# V√©rifier GPU
python -c "import cupy; print(cupy.cuda.Device(0).mem_info)"
```

---

## ‚ö†Ô∏è Pi√®ges Courants

### **1. GPU Unload sur CPU**
**Sympt√¥me :** Latence +5s par it√©ration sans gain  
**Cause :** `UNLOAD_LLM_DURING_BACKTEST=True` sur syst√®me sans GPU  
**Fix :** `UNLOAD_LLM_DURING_BACKTEST=False`

### **2. VRAM OOM**
**Sympt√¥me :** `CuPy OutOfMemoryError` durant calculs  
**Cause :** LLM occupe toute la VRAM  
**Fix :** `UNLOAD_LLM_DURING_BACKTEST=True`

### **3. Logs trop verbeux**
**Sympt√¥me :** Terminal inond√© de spans  
**Cause :** `BACKTEST_LOG_LEVEL=DEBUG`  
**Fix :** `BACKTEST_LOG_LEVEL=INFO`

### **4. Mod√®le OpenAI introuvable**
**Sympt√¥me :** `AuthenticationError` ou `ModelNotFound`  
**Cause :** `OPENAI_API_KEY` manquante ou invalide  
**Fix :** V√©rifier cl√© API valide

---

## üìä Monitoring Variables

**En Python :**
```python
import os
from agents import LLMConfig

config = LLMConfig.from_env()
print(f"Provider: {config.provider}")
print(f"Model: {config.model}")
print(f"GPU Unload: {os.getenv('UNLOAD_LLM_DURING_BACKTEST', 'False')}")
```

**En CLI :**
```bash
python __main__.py validate --all  # V√©rifie toute la config
```

---

## üîó R√©f√©rences

- [Configuration LLM](LLM_INTEGRATION_README.md)
- [CLI Reference](CLI_REFERENCE.md)
- [Copilot Instructions](copilot-instructions.md)

---

*Derni√®re mise √† jour : 13/12/2025*
