# Performance Module - Guide Utilisateur

> **Module d'optimisation des performances pour backtest_core**  
> Version : 1.8.0 | Date : 13/12/2025

---

## üìä Vue d'Ensemble

Le module `performance/` fournit des outils d'optimisation pour le moteur de backtest :
- ‚ö° **Parall√©lisation CPU** : Distribuer les calculs sur plusieurs c≈ìurs
- üöÄ **Acc√©l√©ration GPU** : Utiliser CuPy/Numba pour calculs massifs
- üìà **Monitoring temps r√©el** : Surveiller CPU/RAM/GPU pendant l'ex√©cution
- üîç **Profiling** : Identifier les goulots d'√©tranglement
- üíæ **Gestion m√©moire** : Optimiser l'utilisation RAM avec chunking

---

## üìÅ Structure

```
performance/
‚îú‚îÄ‚îÄ parallel.py         ‚Üí Parall√©lisation CPU (joblib/multiprocessing)
‚îú‚îÄ‚îÄ monitor.py          ‚Üí Monitoring temps r√©el (psutil + rich)
‚îú‚îÄ‚îÄ profiler.py         ‚Üí Profiling cProfile + line_profiler
‚îú‚îÄ‚îÄ memory.py           ‚Üí Gestion m√©moire + chunking
‚îú‚îÄ‚îÄ benchmark.py        ‚Üí Suite de benchmarks v1.8.0
‚îú‚îÄ‚îÄ device_backend.py   ‚Üí Abstraction NumPy/CuPy transparente
‚îú‚îÄ‚îÄ gpu.py              ‚Üí Utilitaires GPU
‚îî‚îÄ‚îÄ __init__.py         ‚Üí Exports publics
```

---

## üöÄ Guide d'Utilisation

### 1. Parall√©lisation (`parallel.py`)

**Objectif :** Acc√©l√©rer les sweeps en distribuant les calculs sur plusieurs CPU.

```python
from performance import ParallelRunner, parallel_sweep

# M√©thode 1: Runner avec configuration
runner = ParallelRunner(n_jobs=8)
results = runner.run_sweep(strategy, param_grid, data)

# M√©thode 2: Fonction directe
results = parallel_sweep(
    strategy_class=EMACrossStrategy,
    param_grid={"fast": [5, 10, 15], "slow": [20, 30, 50]},
    data=ohlcv_df,
    n_jobs=8,
)

# Benchmark diff√©rentes configs
benchmark_parallel_configs(strategy, param_grid, data)
```

**Classes principales :**
- `ParallelRunner` : Ex√©cuteur parall√®le configurable
- `ParallelConfig` : Configuration (n_jobs, backend, timeout)
- `SweepResult` : R√©sultat d'un sweep parall√®le

**Performances typiques :**
| Workers | Speedup | CPU Usage |
|---------|---------|-----------|
| 1 | 1.0x | 12.5% |
| 4 | 3.2x | 50% |
| 8 | 5.8x | 100% |

---

### 2. Monitoring (`monitor.py`)

**Objectif :** Surveiller les ressources syst√®me en temps r√©el pendant les backtests.

```python
from performance import PerformanceMonitor, ProgressBar

# Monitor avec rich console
with PerformanceMonitor(show_bar=True) as monitor:
    for params in param_grid:
        result = engine.run(params)
        monitor.update(1)  # Avancer la barre

# Progress bar standalone
with ProgressBar(total=len(param_grid)) as pbar:
    for i, params in enumerate(param_grid):
        result = engine.run(params)
        pbar.update(1, description=f"Params {i+1}")

# Stats syst√®me
from performance import get_system_resources, print_system_info

stats = get_system_resources()  # CPU, RAM, GPU stats
print_system_info()             # Affichage format√©
```

**Classes principales :**
- `PerformanceMonitor` : Monitor avec barre de progression
- `ResourceTracker` : Tracking CPU/RAM/GPU en continu
- `ProgressBar` : Barre de progression rich

---

### 3. Profiling (`profiler.py`)

**Objectif :** Identifier les parties lentes du code.

```python
from performance import Profiler, profile_function, TimingContext

# M√©thode 1: Context manager
with Profiler() as profiler:
    engine.run_sweep(param_grid)

profiler.print_stats()
profiler.save_stats("profile.prof")

# M√©thode 2: D√©corateur
@profile_function
def my_backtest_function(params):
    return engine.run(params)

# M√©thode 3: Timing simple
with TimingContext("Calcul indicateurs"):
    indicators = compute_indicators(data)
# Output: "Calcul indicateurs: 1.234s"

# Benchmark une fonction
from performance import benchmark_function

stats = benchmark_function(
    lambda: engine.run(params),
    n_runs=100,
    warmup=10,
)
print(f"Moyenne: {stats.mean_time:.4f}s")
```

**Classes principales :**
- `Profiler` : Wrapper cProfile + line_profiler
- `ProfileResult` : R√©sultat d'un profiling
- `TimingContext` : Chronom√©trage simple

---

### 4. Gestion M√©moire (`memory.py`)

**Objectif :** Optimiser l'utilisation RAM pour datasets volumineux.

```python
from performance import (
    ChunkedProcessor,
    MemoryManager,
    DataFrameCache,
    optimize_dataframe,
    memory_efficient_mode,
)

# M√©thode 1: Chunking automatique
processor = ChunkedProcessor(chunk_size_mb=100)
results = processor.process_dataframe(large_df, compute_function)

# M√©thode 2: Cache LRU
cache = DataFrameCache(max_size_gb=2.0)
cached_df = cache.get_or_compute("key", lambda: load_heavy_data())

# M√©thode 3: Optimisation DataFrame
df_optimized = optimize_dataframe(df)  # R√©duire m√©moire 30-70%

# Context manager mode √©conomie
with memory_efficient_mode():
    # Limite RAM utilis√©e automatiquement
    results = heavy_computation()

# Stats m√©moire
from performance import get_memory_info, get_available_ram_gb

mem_stats = get_memory_info()
print(f"RAM libre: {get_available_ram_gb():.2f} GB")
```

**Classes principales :**
- `ChunkedProcessor` : D√©coupage automatique datasets
- `MemoryManager` : Gestion m√©moire globale
- `DataFrameCache` : Cache LRU avec limite GB

---

### 5. GPU (`gpu.py` + `device_backend.py`)

**Objectif :** Acc√©l√©rer calculs avec GPU (CuPy) tout en gardant compatibilit√© CPU.

```python
from performance import (
    GPUIndicatorCalculator,
    gpu_available,
    get_gpu_info,
    to_gpu,
    to_cpu,
)

# Check disponibilit√© GPU
if gpu_available():
    print(get_gpu_info())
else:
    print("GPU non disponible, fallback CPU")

# Calcul GPU transparent
calc = GPUIndicatorCalculator()
result = calc.compute_ema(prices, period=20)  # Auto GPU si dispo

# Transfer manuel CPU ‚Üî GPU
gpu_array = to_gpu(cpu_array)  # NumPy ‚Üí CuPy
cpu_array = to_cpu(gpu_array)  # CuPy ‚Üí NumPy

# Backend agnostic (v1.8.0)
from performance.device_backend import ArrayBackend

backend = ArrayBackend.auto()  # D√©tecte NumPy ou CuPy
arr = backend.array([1, 2, 3])
result = backend.mean(arr)
```

---

### 6. Benchmark Suite (`benchmark.py`)

**Objectif :** Comparer performances de diff√©rentes impl√©mentations.

```python
from performance.benchmark import (
    run_all_benchmarks,
    benchmark_indicator_calculation,
    benchmark_simulator_performance,
    benchmark_gpu_vs_cpu,
)

# Benchmark complet
run_all_benchmarks(verbose=True)

# Benchmark sp√©cifique
comp = benchmark_indicator_calculation(data_size=10000)
print(comp.summary())

# GPU vs CPU
comp = benchmark_gpu_vs_cpu(data_size=100000)
print(f"Speedup GPU: {comp.speedup:.2f}x")
```

---

## üéØ Cas d'Usage Typiques

### Cas 1 : Sweep Rapide avec Monitoring

```python
from performance import parallel_sweep, PerformanceMonitor

with PerformanceMonitor(show_bar=True) as monitor:
    results = parallel_sweep(
        strategy_class=EMACrossStrategy,
        param_grid=large_grid,
        data=df,
        n_jobs=8,
    )

print(f"Meilleur Sharpe: {max(r.sharpe for r in results)}")
```

### Cas 2 : Backtest GPU avec Gestion M√©moire

```python
from performance import GPUIndicatorCalculator, memory_efficient_mode

with memory_efficient_mode():
    calc = GPUIndicatorCalculator()
    indicators = calc.compute_all(data)
    results = engine.run(indicators)
```

### Cas 3 : Profiling d'une Strat√©gie Lente

```python
from performance import Profiler

with Profiler() as profiler:
    engine.run_sweep(param_grid)

profiler.print_stats(top=10)
# Output: Top 10 des fonctions les plus lentes
```

---

## üìä D√©pendances Optionnelles

| Package | Usage | Installation |
|---------|-------|--------------|
| `joblib` | Parall√©lisation | `pip install joblib` |
| `psutil` | Monitoring syst√®me | `pip install psutil` |
| `rich` | Console format√©e | `pip install rich` |
| `cupy` | Acc√©l√©ration GPU | `pip install cupy-cuda12x` |
| `line_profiler` | Profiling ligne par ligne | `pip install line_profiler` |
| `numba` | JIT compilation | `pip install numba` |

**Note :** Toutes les fonctionnalit√©s ont un **fallback gracieux** si la d√©pendance est absente.

---

## üîß Configuration

Variables d'environnement disponibles :

```bash
# GPU
BACKTEST_USE_GPU=True                    # Activer GPU (d√©faut: False)
CUPY_CACHE_DIR=/path/to/cache            # Cache CuPy

# Parall√©lisation
BACKTEST_N_JOBS=8                        # Workers par d√©faut
BACKTEST_PARALLEL_BACKEND=multiprocessing # joblib ou multiprocessing

# M√©moire
BACKTEST_MEMORY_LIMIT_GB=16.0            # Limite RAM
BACKTEST_CHUNK_SIZE_MB=100               # Taille chunks
```

---

## üìà Performances Attendues

| Op√©ration | Sans Optimisation | Avec Optimisation | Speedup |
|-----------|-------------------|-------------------|---------|
| Sweep 1000 params | 120s | 21s (8 workers) | **5.7x** |
| Calcul indicateurs | 5s | 0.3s (GPU) | **16.6x** |
| Backtest 10M rows | OOM | 45s (chunking) | ‚úÖ Fonctionne |

---

## üêõ Troubleshooting

### Erreur : "No module named 'cupy'"
```bash
pip install cupy-cuda12x  # CUDA 12.x
# Ou d√©sactiver GPU
BACKTEST_USE_GPU=False python script.py
```

### Erreur : "MemoryError"
```python
from performance import memory_efficient_mode

# Activer mode √©conomie m√©moire
with memory_efficient_mode():
    results = heavy_computation()
```

### Parall√©lisation lente
```python
# V√©rifier overhead communication
from performance import benchmark_parallel_configs

benchmark_parallel_configs(strategy, param_grid, data)
# Choisir le meilleur nombre de workers
```

---

## üìö R√©f√©rences

- [joblib documentation](https://joblib.readthedocs.io/)
- [CuPy user guide](https://docs.cupy.dev/en/stable/user_guide/)
- [Numba JIT guide](https://numba.pydata.org/numba-doc/latest/user/jit.html)
- [psutil documentation](https://psutil.readthedocs.io/)

---

*Derni√®re mise √† jour : 13/12/2025 | Version : 1.8.0*
