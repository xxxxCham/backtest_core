#!/usr/bin/env python3
"""
Module-ID: analyze_data

Purpose: Analyser dÃ©tail fichier OHLCV unique - dates, continuitÃ©, gaps, complÃ©tude de barres.

Role in pipeline: data quality assurance

Key components: analyze_parquet_file(), affichage stats

Inputs: Chemin fichier parquet OHLCV

Outputs: Statistiques: nombre barres, plage dates, gaps, continuitÃ©, colonnes

Dependencies: pandas, pathlib, collections

Conventions: Index pandas.DatetimeIndex; colonnes [open, high, low, close, volume]

Read-if: VÃ©rifier qualitÃ©/couverture fichier avant backtest.

Skip-if: DonnÃ©es prÃ©-validÃ©es.
"""

import sys
from pathlib import Path

import pandas as pd


def analyze_parquet_file(file_path: str):
    """Analyse un fichier Parquet OHLCV."""

    print(f"\n{'='*70}")
    print(f"ANALYSE: {Path(file_path).name}")
    print(f"{'='*70}\n")

    try:
        # Charger le fichier
        df = pd.read_parquet(file_path)

        # Informations de base
        print("ðŸ“Š STATISTIQUES GÃ‰NÃ‰RALES")
        print(f"   Nombre total de barres : {len(df):,}")
        print(f"   Taille mÃ©moire         : {df.memory_usage(deep=True).sum() / 1024:.1f} KB")
        print(f"   Colonnes               : {list(df.columns)}")

        # VÃ©rifier l'index et convertir les timestamps
        if isinstance(df.index, pd.DatetimeIndex):
            index_col = df.index
        elif 'timestamp' in df.columns:
            # Essayer de dÃ©tecter le format du timestamp
            sample_ts = float(df['timestamp'].iloc[0])  # Convertir en float pour Ã©viter problÃ¨me numpy
            if sample_ts > 1e12:
                # Timestamp en millisecondes
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            elif sample_ts > 1e9:
                # Timestamp en secondes
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
            else:
                # Format datetime normal
                df['timestamp'] = pd.to_datetime(df['timestamp'])

            index_col = df['timestamp']
        else:
            print("âŒ Pas d'index datetime trouvÃ©!")
            return

        # Plage de dates
        start_date = index_col.min()
        end_date = index_col.max()
        duration = end_date - start_date

        print("\nðŸ“… PLAGE DE DATES")
        print(f"   DÃ©but  : {start_date}")
        print(f"   Fin    : {end_date}")
        print(f"   DurÃ©e  : {duration.days} jours ({duration.days / 365.25:.1f} annÃ©es)")

        # VÃ©rifier la continuitÃ© (gaps)
        print("\nðŸ” VÃ‰RIFICATION CONTINUITÃ‰ (gaps)")

        # Calculer les diffÃ©rences entre timestamps consÃ©cutifs
        if isinstance(df.index, pd.DatetimeIndex):
            time_diffs = df.index.to_series().diff()
        else:
            time_diffs = index_col.diff()

        # Mode attendu (1h = 3600s)
        expected_diff = pd.Timedelta(hours=1)
        tolerance = pd.Timedelta(minutes=5)  # TolÃ©rance de 5 min

        # Identifier les gaps
        gaps = time_diffs[(time_diffs > expected_diff + tolerance) | (time_diffs < expected_diff - tolerance)]
        gaps = gaps.dropna()

        if len(gaps) == 0:
            print("   âœ… Aucun gap dÃ©tectÃ© - donnÃ©es continues!")
        else:
            print(f"   âš ï¸  {len(gaps)} gaps dÃ©tectÃ©s:")
            for i, (idx, gap) in enumerate(gaps.items()):
                if i < 10:  # Montrer max 10 gaps
                    print(f"      â€¢ {idx}: Ã©cart de {gap}")
                elif i == 10:
                    print(f"      ... et {len(gaps) - 10} autres gaps")
                    break

        # Statistiques OHLCV
        print("\nðŸ“ˆ STATISTIQUES OHLCV")
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in df.columns:
                values = df[col]
                print(
                    f"   {col.upper():8s}: min={values.min():.2f}  max={values.max():.2f}  "
                    f"mean={values.mean():.2f}  null={values.isna().sum()}"
                )

        # VÃ©rifier les valeurs manquantes
        print("\nðŸ”Ž VALEURS MANQUANTES")
        null_counts = df.isnull().sum()
        if null_counts.sum() == 0:
            print("   âœ… Aucune valeur manquante")
        else:
            print("   âš ï¸  Valeurs manquantes dÃ©tectÃ©es:")
            for col, count in null_counts[null_counts > 0].items():
                print(f"      â€¢ {col}: {count} ({count/len(df)*100:.2f}%)")

        # VÃ©rifier cohÃ©rence OHLC
        print("\nâœ“ COHÃ‰RENCE OHLC")
        if all(col in df.columns for col in ['open', 'high', 'low', 'close']):
            invalid_high = (
                (df['high'] < df['open'])
                | (df['high'] < df['close'])
                | (df['high'] < df['low'])
            )
            invalid_low = (
                (df['low'] > df['open'])
                | (df['low'] > df['close'])
                | (df['low'] > df['high'])
            )

            if invalid_high.sum() == 0 and invalid_low.sum() == 0:
                print("   âœ… Toutes les barres OHLC sont cohÃ©rentes")
            else:
                print(f"   âš ï¸  Barres incohÃ©rentes: high={invalid_high.sum()}, low={invalid_low.sum()}")

        # RÃ©sumÃ© final
        print(f"\n{'='*70}")
        print("RÃ‰SUMÃ‰")
        print(f"{'='*70}")
        print(f"âœ… DonnÃ©es chargÃ©es    : {len(df):,} barres")
        print(f"âœ… PÃ©riode couverte    : {start_date.date()} â†’ {end_date.date()}")
        print(f"{'âœ…' if len(gaps) == 0 else 'âš ï¸ '} ContinuitÃ©         : {'OK' if len(gaps) == 0 else f'{len(gaps)} gaps'}")
        print(f"{'âœ…' if null_counts.sum() == 0 else 'âš ï¸ '} Valeurs manquantes : {'Aucune' if null_counts.sum() == 0 else 'PrÃ©sentes'}")
        print(f"{'='*70}\n")

    except Exception as e:
        print(f"âŒ ERREUR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
    else:
        file_path = r"D:\backtest_core\docs\AAVEUSDC_1h.parquet"

    analyze_parquet_file(file_path)
