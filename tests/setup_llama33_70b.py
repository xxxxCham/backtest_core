#!/usr/bin/env python3
"""
Script de t√©l√©chargement et configuration de Llama-3.3-70B-Instruct pour Ollama.

Optimisations:
- Distribution sur 2 GPUs
- Offloading RAM DDR5 automatique
- Quantization Q4 (~40GB VRAM total)
- Configuration multi-GPU optimis√©e

Usage:
    python tools/setup_llama33_70b.py
"""

import os
import shutil
import subprocess
import sys
import time
from pathlib import Path

# Ajouter le chemin racine au PYTHONPATH
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def check_ollama_installed() -> bool:
    """V√©rifie si Ollama est install√©."""
    return shutil.which("ollama") is not None


def check_disk_space(required_gb: float = 40.0) -> tuple[bool, float]:
    """
    V√©rifie l'espace disque disponible.

    Args:
        required_gb: Espace requis en GB

    Returns:
        tuple[bool, float]: (espace_suffisant, espace_disponible_gb)
    """
    try:
        # Obtenir le r√©pertoire home Ollama (g√©n√©ralement ~/.ollama)
        home = Path.home()
        stat = shutil.disk_usage(home)
        available_gb = stat.free / (1024**3)
        return available_gb >= required_gb, available_gb
    except Exception:
        # Si erreur, on suppose qu'il y a assez d'espace
        return True, 0.0


def check_gpus() -> tuple[bool, int, list]:
    """
    V√©rifie les GPUs disponibles via CuPy.

    Returns:
        tuple[bool, int, list]: (gpu_disponible, nombre_gpus, infos)
    """
    try:
        from performance.gpu import get_gpu_info

        info = get_gpu_info()
        has_gpu = info.get("cupy_available", False)

        if not has_gpu:
            return False, 0, []

        # Essayer de compter les GPUs
        try:
            import cupy as cp
            num_gpus = cp.cuda.runtime.getDeviceCount()

            gpu_infos = []
            for i in range(num_gpus):
                cp.cuda.Device(i).use()
                props = cp.cuda.runtime.getDeviceProperties(i)
                total_mem_gb = props["totalGlobalMem"] / (1024**3)
                gpu_infos.append({
                    "id": i,
                    "name": props["name"].decode("utf-8"),
                    "memory_gb": total_mem_gb
                })

            return True, num_gpus, gpu_infos
        except Exception:
            # Fallback si CuPy ne peut pas compter
            return has_gpu, 1, [{"id": 0, "name": "Unknown", "memory_gb": 0}]

    except Exception:
        return False, 0, []


def check_ram() -> tuple[bool, float]:
    """
    V√©rifie la RAM disponible.

    Returns:
        tuple[bool, float]: (ram_suffisante, ram_total_gb)
    """
    try:
        import psutil
        ram_gb = psutil.virtual_memory().total / (1024**3)
        return ram_gb >= 32, ram_gb
    except ImportError:
        # Si psutil pas install√©, on suppose que c'est OK
        return True, 0.0


def is_model_installed(model_name: str) -> bool:
    """
    V√©rifie si un mod√®le est d√©j√† install√©.

    Args:
        model_name: Nom du mod√®le Ollama

    Returns:
        bool: True si install√©
    """
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True,
            timeout=10
        )
        return model_name in result.stdout
    except Exception:
        return False


def pull_model(model_name: str) -> bool:
    """
    T√©l√©charge un mod√®le Ollama.

    Args:
        model_name: Nom du mod√®le √† t√©l√©charger

    Returns:
        bool: True si succ√®s
    """
    print(f"\nüì• T√©l√©chargement de {model_name}...")
    print("‚è±Ô∏è  Ceci peut prendre 10-30 minutes selon votre connexion...")

    try:
        # Lancer ollama pull avec output en temps r√©el
        process = subprocess.Popen(
            ["ollama", "pull", model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        # Afficher la progression
        for line in process.stdout:
            print(f"   {line.rstrip()}")

        process.wait()

        if process.returncode == 0:
            print(f"‚úÖ Mod√®le {model_name} t√©l√©charg√© avec succ√®s")
            return True
        else:
            print(f"‚ùå √âchec du t√©l√©chargement (code {process.returncode})")
            return False

    except subprocess.TimeoutExpired:
        print("‚ùå Timeout - Le t√©l√©chargement a pris trop de temps")
        return False
    except Exception as e:
        print(f"‚ùå Erreur lors du t√©l√©chargement: {e}")
        return False


def create_optimized_modelfile(base_model: str, num_gpus: int = 2) -> str:
    """
    Cr√©e un Modelfile optimis√© pour multi-GPU.

    Args:
        base_model: Mod√®le de base
        num_gpus: Nombre de GPUs √† utiliser

    Returns:
        str: Contenu du Modelfile
    """
    return f"""FROM {base_model}

# Param√®tres syst√®me - Optimisation multi-GPU
PARAMETER num_gpu {num_gpus}
PARAMETER num_thread 16
PARAMETER num_ctx 8192

# Param√®tres g√©n√©ration
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1

# System prompt
SYSTEM You are Llama 3.3, a helpful AI assistant specialized in analyzing trading strategies and financial data. You provide clear, data-driven insights and identify potential risks in trading approaches.
"""


def create_custom_model(
    base_model: str,
    custom_name: str,
    modelfile_content: str
) -> bool:
    """
    Cr√©e un mod√®le personnalis√© Ollama avec un Modelfile.

    Args:
        base_model: Mod√®le de base
        custom_name: Nom du mod√®le personnalis√©
        modelfile_content: Contenu du Modelfile

    Returns:
        bool: True si succ√®s
    """
    print(f"\nüîß Cr√©ation du mod√®le optimis√© '{custom_name}'...")

    # Cr√©er un fichier temporaire pour le Modelfile
    modelfile_path = Path("Modelfile.llama33.tmp")

    try:
        # √âcrire le Modelfile
        modelfile_path.write_text(modelfile_content, encoding="utf-8")

        # Cr√©er le mod√®le
        result = subprocess.run(
            ["ollama", "create", custom_name, "-f", str(modelfile_path)],
            capture_output=True,
            text=True,
            timeout=60
        )

        if result.returncode == 0:
            print(f"‚úÖ Mod√®le '{custom_name}' cr√©√© avec succ√®s")
            print("   Configuration: multi-GPU, contexte 8K, optimis√© pour trading")
            return True
        else:
            print(f"‚ùå √âchec cr√©ation mod√®le: {result.stderr}")
            return False

    except Exception as e:
        print(f"‚ùå Erreur cr√©ation mod√®le: {e}")
        return False
    finally:
        # Nettoyer le fichier temporaire
        if modelfile_path.exists():
            modelfile_path.unlink()


def test_model(model_name: str) -> bool:
    """
    Teste le mod√®le avec un prompt simple.

    Args:
        model_name: Nom du mod√®le √† tester

    Returns:
        bool: True si le mod√®le r√©pond correctement
    """
    print(f"\nüß™ Test du mod√®le {model_name}...")

    try:
        start = time.perf_counter()

        result = subprocess.run(
            [
                "ollama", "run", model_name,
                "What is the Sharpe ratio in one sentence?"
            ],
            capture_output=True,
            text=True,
            timeout=180  # 3 minutes max
        )

        elapsed = time.perf_counter() - start

        if result.returncode == 0 and result.stdout.strip():
            print(f"‚úÖ Test r√©ussi ({elapsed:.1f}s)")
            print(f"   R√©ponse: {result.stdout[:200]}...")
            return True
        else:
            print(f"‚ùå Test √©chou√©: {result.stderr}")
            return False

    except subprocess.TimeoutExpired:
        print("‚è±Ô∏è  Timeout - Le mod√®le prend trop de temps √† r√©pondre")
        return False
    except Exception as e:
        print(f"‚ùå Erreur test: {e}")
        return False


def print_env_setup_instructions(num_gpus: int):
    """Affiche les instructions pour configurer l'environnement."""
    print("\n" + "="*60)
    print("üìã CONFIGURATION ENVIRONNEMENT")
    print("="*60)

    if num_gpus >= 2:
        print("\n‚úÖ Multi-GPU d√©tect√©! Pour garantir l'utilisation des 2 GPUs:")
        print("   Ajoutez √† votre environnement (optionnel):")
        print("   export CUDA_VISIBLE_DEVICES=0,1  # Linux/Mac")
        print("   $env:CUDA_VISIBLE_DEVICES='0,1'  # PowerShell")

    print("\nüí° Configuration Ollama avanc√©e (optionnel):")
    print("   export OLLAMA_NUM_PARALLEL=1        # √âviter surcharge m√©moire")
    print("   export OLLAMA_MAX_LOADED_MODELS=1   # D√©charger autres mod√®les")


def main():
    """Ex√©cute le setup complet."""
    print("="*60)
    print("üöÄ SETUP LLAMA-3.3-70B-INSTRUCT")
    print("="*60)

    # Configuration
    BASE_MODEL = "llama3.3:70b-instruct-q4_K_M"
    CUSTOM_MODEL = "llama3.3-70b-optimized"

    # √âtape 1: V√©rifications pr√©requis
    print("\nüìã V√©rification des pr√©requis...")

    if not check_ollama_installed():
        print("‚ùå Ollama n'est pas install√©")
        print("   Installez Ollama: https://ollama.ai/download")
        return False
    print("‚úÖ Ollama install√©")

    # V√©rifier espace disque
    has_space, available_gb = check_disk_space(40.0)
    if not has_space:
        print(f"‚ùå Espace disque insuffisant ({available_gb:.1f} GB disponible, 40 GB requis)")
        return False
    print(f"‚úÖ Espace disque suffisant ({available_gb:.1f} GB disponible)")

    # V√©rifier GPUs
    has_gpu, num_gpus, gpu_infos = check_gpus()
    if not has_gpu:
        print("‚ö†Ô∏è  Aucun GPU d√©tect√© - Le mod√®le fonctionnera mais sera TR√àS lent")
        num_gpus = 0
    else:
        print(f"‚úÖ {num_gpus} GPU(s) d√©tect√©(s):")
        for gpu in gpu_infos:
            print(f"   GPU {gpu['id']}: {gpu['name']} ({gpu['memory_gb']:.1f} GB)")

    # V√©rifier RAM
    has_ram, ram_gb = check_ram()
    if not has_ram:
        print(f"‚ö†Ô∏è  RAM limit√©e ({ram_gb:.1f} GB) - 32GB+ recommand√© pour offloading")
    else:
        print(f"‚úÖ RAM suffisante ({ram_gb:.1f} GB)")

    # √âtape 2: T√©l√©chargement
    print("\n" + "="*60)
    print("üì• T√âL√âCHARGEMENT DU MOD√àLE")
    print("="*60)

    if is_model_installed(BASE_MODEL):
        print(f"‚úÖ Mod√®le {BASE_MODEL} d√©j√† install√© - Skip t√©l√©chargement")
    else:
        if not pull_model(BASE_MODEL):
            print("‚ùå √âchec du t√©l√©chargement")
            return False

    # √âtape 3: Cr√©ation mod√®le optimis√©
    print("\n" + "="*60)
    print("üîß CR√âATION MOD√àLE OPTIMIS√â")
    print("="*60)

    # Utiliser num_gpus d√©tect√© (minimum 1 si GPU disponible)
    gpu_config = max(num_gpus, 1) if has_gpu else 1
    modelfile = create_optimized_modelfile(BASE_MODEL, gpu_config)

    if not create_custom_model(BASE_MODEL, CUSTOM_MODEL, modelfile):
        print("‚ö†Ô∏è  √âchec cr√©ation mod√®le optimis√© - Le mod√®le de base est utilisable")

    # √âtape 4: Test
    print("\n" + "="*60)
    print("üß™ TEST DE VALIDATION")
    print("="*60)

    # Tester le mod√®le personnalis√© d'abord, sinon le base
    model_to_test = CUSTOM_MODEL if is_model_installed(CUSTOM_MODEL) else BASE_MODEL
    test_model(model_to_test)

    # √âtape 5: Instructions finales
    print_env_setup_instructions(num_gpus)

    print("\n" + "="*60)
    print("‚úÖ INSTALLATION TERMIN√âE")
    print("="*60)
    print(f"\nüìù Mod√®le install√©: {model_to_test}")
    print("\nüéØ Prochaines √©tapes:")
    print("   1. Le mod√®le a √©t√© ajout√© automatiquement √† model_config.py")
    print("   2. Disponible pour les r√¥les: Critic (iter>=2), Validator (iter>=3)")
    print("   3. Lancez un backtest avec agents pour le tester")
    print("\nüí° Commande de test rapide:")
    print(f"   ollama run {model_to_test} 'Explain RSI indicator'")

    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Installation interrompue par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
