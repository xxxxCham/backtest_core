#!/usr/bin/env python3
"""
Script de test et validation pour Llama-3.3-70B-Instruct.

V√©rifie:
1. Mod√®le disponible dans la configuration
2. S√©lection correcte pour les r√¥les Critic/Validator
3. Inf√©rence fonctionnelle
4. Distribution GPU et utilisation m√©moire
5. Int√©gration avec GPUMemoryManager

Usage:
    python tools/test_llama33_70b.py
"""

import os
import sys
import time

# Ajouter le chemin racine au PYTHONPATH
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def check_model_in_config():
    """V√©rifie que Llama-3.3-70B est dans la configuration."""
    try:
        from agents.model_config import KNOWN_MODELS

        print("\nüìã V√©rification de la configuration...")

        # V√©rifier les deux variantes
        models_to_check = [
            "llama3.3:70b-instruct-q4_K_M",
            "llama3.3-70b-optimized"
        ]

        found = []
        for model_name in models_to_check:
            if model_name in KNOWN_MODELS:
                info = KNOWN_MODELS[model_name]
                print(f"‚úÖ {model_name} trouv√© dans KNOWN_MODELS")
                print(f"   Cat√©gorie: {info.category.value}")
                print(f"   Description: {info.description}")
                print(f"   Recommand√© pour: {', '.join(info.recommended_for)}")
                print(f"   Temps moyen: {info.avg_response_time_s}s")
                found.append(model_name)
            else:
                print(f"‚ùå {model_name} NON trouv√© dans KNOWN_MODELS")

        return len(found) > 0

    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification de la config: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_role_assignment():
    """V√©rifie que le mod√®le est assign√© aux bons r√¥les."""
    try:
        from agents.model_config import RoleModelConfig

        print("\nüéØ V√©rification de l'assignation par r√¥le...")

        config = RoleModelConfig()

        # V√©rifier Critic
        critic_models = config.critic.models
        if "llama3.3-70b-optimized" in critic_models:
            print("‚úÖ llama3.3-70b-optimized dans Critic.models")
            print(f"   Allow heavy apr√®s it√©ration: {config.critic.allow_heavy_after_iteration}")
        else:
            print("‚ùå llama3.3-70b-optimized PAS dans Critic.models")
            return False

        # V√©rifier Validator
        validator_models = config.validator.models
        if "llama3.3-70b-optimized" in validator_models:
            print("‚úÖ llama3.3-70b-optimized dans Validator.models")
            print(f"   Allow heavy apr√®s it√©ration: {config.validator.allow_heavy_after_iteration}")
        else:
            print("‚ùå llama3.3-70b-optimized PAS dans Validator.models")
            return False

        return True

    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification des r√¥les: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_model_selection():
    """Teste la s√©lection du mod√®le via get_model()."""
    try:
        from agents.model_config import RoleModelConfig

        print("\nüîç Test de s√©lection du mod√®le...")

        config = RoleModelConfig()

        # V√©rifier si le mod√®le est install√©
        installed = config.get_installed_models()
        print(f"   Mod√®les install√©s: {len(installed)}")

        is_installed = "llama3.3-70b-optimized" in installed
        if is_installed:
            print("‚úÖ llama3.3-70b-optimized est install√©")
        else:
            print("‚ö†Ô∏è  llama3.3-70b-optimized n'est pas encore install√©")
            print("   Lancez: python tools/setup_llama33_70b.py")
            return False

        # Tester s√©lection pour Critic (iteration=3 pour autoriser HEAVY)
        model_critic = config.get_model("critic", iteration=3, allow_heavy=True)
        print(f"   S√©lection Critic (iter=3, heavy=True): {model_critic}")

        # Tester s√©lection pour Validator (iteration=4 pour autoriser HEAVY)
        model_validator = config.get_model("validator", iteration=4, allow_heavy=True)
        print(f"   S√©lection Validator (iter=4, heavy=True): {model_validator}")

        return True

    except Exception as e:
        print(f"‚ùå Erreur lors du test de s√©lection: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_ollama_availability():
    """V√©rifie qu'Ollama est actif et que le mod√®le est disponible."""
    try:
        from agents.ollama_manager import is_ollama_available, list_ollama_models

        print("\nüîå V√©rification d'Ollama...")

        if not is_ollama_available():
            print("‚ùå Ollama n'est pas actif")
            print("   Lancez: ollama serve")
            return False

        print("‚úÖ Ollama actif")

        models = list_ollama_models()
        print(f"   {len(models)} mod√®le(s) install√©(s)")

        # Chercher les variantes Llama 3.3
        llama33_models = [m for m in models if "llama3.3" in m.lower()]
        if llama33_models:
            print("‚úÖ Mod√®les Llama 3.3 trouv√©s:")
            for m in llama33_models:
                print(f"   - {m}")
            return True
        else:
            print("‚ö†Ô∏è  Aucun mod√®le Llama 3.3 trouv√©")
            print("   Lancez: python tools/setup_llama33_70b.py")
            return False

    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification Ollama: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_inference():
    """Teste une inf√©rence simple avec le mod√®le."""
    try:
        from agents.llm_client import LLMConfig, create_llm_client

        print("\nüß™ Test d'inf√©rence...")

        # Cr√©er le client LLM
        config = LLMConfig(
            model="llama3.3-70b-optimized",
            provider="ollama",
            temperature=0.7
        )

        client = create_llm_client(config)

        if not client.is_available():
            print("‚ùå Le client LLM n'est pas disponible")
            return False

        print("‚úÖ Client LLM cr√©√©")

        # Test avec un prompt simple
        prompt = "Explain the Sharpe ratio in one sentence."
        print(f"   Prompt: {prompt}")

        start = time.perf_counter()

        try:
            response = client.simple_chat(prompt)
            elapsed = time.perf_counter() - start

            if response and hasattr(response, 'content') and response.content:
                print(f"‚úÖ R√©ponse re√ßue ({elapsed:.1f}s)")
                print(f"   R√©ponse: {response.content[:200]}...")
                print(f"   Longueur: {len(response.content)} caract√®res")
                return True
            else:
                print("‚ùå R√©ponse vide ou invalide")
                return False

        except TimeoutError:
            print("‚è±Ô∏è  Timeout - Le mod√®le prend trop de temps")
            return False

    except Exception as e:
        print(f"‚ùå Erreur lors du test d'inf√©rence: {e}")
        import traceback
        traceback.print_exc()
        return False


def monitor_gpu_usage():
    """Affiche l'utilisation GPU actuelle."""
    try:
        from performance.gpu import get_gpu_info

        print("\nüìä Monitoring GPU...")

        info = get_gpu_info()

        if not info.get("cupy_available", False):
            print("‚ö†Ô∏è  CuPy non disponible - Monitoring GPU limit√©")
            return True

        print(f"‚úÖ GPU d√©tect√©: {info.get('cupy_device_name', 'Unknown')}")
        print(f"   VRAM totale: {info.get('cupy_memory_total_gb', 0):.2f} GB")
        print(f"   VRAM libre: {info.get('cupy_memory_free_gb', 0):.2f} GB")
        print(f"   VRAM utilis√©e: {info.get('cupy_memory_total_gb', 0) - info.get('cupy_memory_free_gb', 0):.2f} GB")

        return True

    except Exception as e:
        print(f"‚ö†Ô∏è  Impossible de monitorer le GPU: {e}")
        return True  # Non-bloquant


def test_gpu_memory_manager():
    """Teste le GPUMemoryManager avec le mod√®le."""
    try:
        from agents.ollama_manager import GPUMemoryManager

        print("\nüîÑ Test GPUMemoryManager...")

        manager = GPUMemoryManager(
            model_name="llama3.3-70b-optimized",
            verbose=True
        )

        # Test is_model_loaded
        is_loaded = manager.is_model_loaded()
        print(f"   Mod√®le charg√©: {is_loaded}")

        # Test unload/reload
        print("   Test unload...")
        state = manager.unload()
        print(f"   √âtat: was_loaded={state.was_loaded}, unload_time={state.unload_time_ms:.0f}ms")

        time.sleep(2)  # Pause

        print("   Test reload...")
        success = manager.reload(state)
        print(f"   Reload: {'‚úÖ Succ√®s' if success else '‚ùå √âchec'}")

        stats = manager.get_stats()
        print(f"   Stats: {stats}")

        return success

    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur GPUMemoryManager: {e}")
        import traceback
        traceback.print_exc()
        return True  # Non-bloquant


def print_recommendations():
    """Affiche des recommandations d'utilisation."""
    print("\n" + "="*60)
    print("üí° RECOMMANDATIONS D'UTILISATION")
    print("="*60)

    print("\nüìå Utilisation dans un backtest:")
    print("   - Le mod√®le sera automatiquement s√©lectionn√© pour Critic (iter>=2)")
    print("   - Et pour Validator (iter>=3)")
    print("   - Utilisez allow_heavy=True pour forcer l'utilisation d√®s iter=1")

    print("\n‚ö° Performance:")
    print("   - Temps de r√©ponse attendu: ~5 min pour analyses complexes")
    print("   - Distribution automatique sur 2 GPUs via Ollama")
    print("   - Offloading RAM pour les layers lourds")

    print("\nüîß Optimisations:")
    print("   - Utilisez gpu_compute_context() pour lib√©rer la VRAM avant backtests")
    print("   - D√©chargez avec GPUMemoryManager si besoin de VRAM")
    print("   - Variables d'environnement optionnelles:")
    print("     export CUDA_VISIBLE_DEVICES=0,1")
    print("     export OLLAMA_NUM_PARALLEL=1")


def main():
    """Ex√©cute tous les tests."""
    print("="*60)
    print("üß™ TEST LLAMA-3.3-70B-INSTRUCT")
    print("="*60)

    results = {}

    # Test 1: Configuration
    results["config"] = check_model_in_config()

    # Test 2: R√¥les
    results["roles"] = check_role_assignment()

    # Test 3: S√©lection
    results["selection"] = check_model_selection()

    # Test 4: Ollama disponibilit√©
    results["ollama"] = check_ollama_availability()

    # Test 5: Inf√©rence (si Ollama OK)
    if results["ollama"]:
        results["inference"] = test_inference()
    else:
        results["inference"] = None
        print("\n‚è≠Ô∏è  Skip test d'inf√©rence (Ollama non disponible)")

    # Test 6: Monitoring GPU
    results["gpu"] = monitor_gpu_usage()

    # Test 7: GPUMemoryManager (si mod√®le install√©)
    if results["ollama"]:
        results["gpu_manager"] = test_gpu_memory_manager()
    else:
        results["gpu_manager"] = None
        print("\n‚è≠Ô∏è  Skip test GPUMemoryManager (mod√®le non install√©)")

    # R√©sum√©
    print("\n" + "="*60)
    print("üìã R√âSUM√â DES TESTS")
    print("="*60)

    passed = sum(1 for v in results.values() if v is True)
    failed = sum(1 for v in results.values() if v is False)
    skipped = sum(1 for v in results.values() if v is None)
    total = len(results)

    for test_name, result in results.items():
        if result is True:
            print(f"‚úÖ {test_name}: PASS")
        elif result is False:
            print(f"‚ùå {test_name}: FAIL")
        else:
            print(f"‚è≠Ô∏è  {test_name}: SKIP")

    print(f"\nüìä Total: {passed}/{total} r√©ussis, {failed} √©checs, {skipped} skip")

    # Recommandations
    if passed >= 4:  # Au moins config, roles, selection, gpu
        print_recommendations()

    print("\n" + "="*60)

    if failed == 0:
        print("‚úÖ TOUS LES TESTS PASS√âS")
        print("="*60)
        return True
    else:
        print("‚ö†Ô∏è  CERTAINS TESTS ONT √âCHOU√â")
        print("="*60)
        print("\nüí° Actions recommand√©es:")
        if not results["config"]:
            print("   - V√©rifiez que model_config.py a √©t√© modifi√© correctement")
        if not results["ollama"]:
            print("   - Installez le mod√®le: python tools/setup_llama33_70b.py")
        if not results["inference"]:
            print("   - V√©rifiez les logs Ollama pour plus de d√©tails")
        return False


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Tests interrompus par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
