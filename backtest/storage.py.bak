"""
Module-ID: backtest.storage

Purpose: Persister et indexer les r√©sultats de backtests pour rechargement/recherche rapide.

Role in pipeline: persistence / reporting

Key components: ResultStorage, StoredResultMetadata, get_storage

Inputs: RunResult, run_id, auto_cleanup flag

Outputs: Fichiers JSON/Parquet dans backtest_results/{run_id}/, index.json

Dependencies: pandas, pathlib, json, optionnel: pyarrow (Parquet)

Conventions: Structure run_id/metadata.json + equity.parquet + trades.parquet; index.json catalogue; auto_cleanup garde N derniers runs.

Read-if: Persistance r√©sultats, recherche historique, ou gestion stockage.

Skip-if: Backtests ponctuels sans sauvegarde.
"""

from __future__ import annotations

import json
import shutil
from dataclasses import asdict, dataclass
from datetime import datetime
import os
from pathlib import Path
import tempfile
import uuid
from typing import Any, Dict, List, Optional, Union

import pandas as pd

from backtest.engine import RunResult
from backtest.sweep import SweepResults
from metrics_types import PerformanceMetricsPct, normalize_metrics
from utils.log import get_logger

logger = get_logger(__name__)


# =============================================================================
# CONFIGURATION
# =============================================================================

DEFAULT_STORAGE_DIR = Path("backtest_results")
MAX_RESULTS_TO_KEEP = 1000  # Nombre maximum de r√©sultats √† garder


# =============================================================================
# TEMP DIR FIX (sandbox compatibility)
# =============================================================================

def _ensure_writable_tempdir() -> None:
    """
    Assure que tempfile utilise un r√©pertoire writable dans les environnements sandbox.
    """
    def _set_local_temp() -> Path:
        fallback = Path.cwd() / ".tmp"
        fallback.mkdir(parents=True, exist_ok=True)
        tempfile.tempdir = str(fallback)
        os.environ["TMP"] = str(fallback)
        os.environ["TEMP"] = str(fallback)
        return fallback

    def _safe_mkdtemp(suffix: Optional[str] = None, prefix: Optional[str] = None, dir: Optional[str] = None) -> str:
        base = Path(dir or tempfile.gettempdir())
        base.mkdir(parents=True, exist_ok=True)
        prefix_val = prefix or "tmp"
        suffix_val = suffix or ""
        for _ in range(1000):
            name = f"{prefix_val}{uuid.uuid4().hex}{suffix_val}"
            candidate = base / name
            try:
                candidate.mkdir()
                return str(candidate)
            except FileExistsError:
                continue
        raise FileExistsError("Unable to create temporary directory")

    try:
        temp_root = Path(tempfile.gettempdir())
        temp_root.mkdir(parents=True, exist_ok=True)
        repo_root = Path.cwd().resolve()
        if repo_root not in temp_root.resolve().parents and temp_root.resolve() != repo_root:
            temp_root = _set_local_temp()

        probe_dir = Path(tempfile.mkdtemp(dir=temp_root))
        nested = probe_dir / "nested_probe"
        nested.mkdir(parents=True, exist_ok=True)
        test_path = nested / "write_test.txt"
        test_path.write_text("ok", encoding="utf-8")
        test_path.unlink(missing_ok=True)
        nested.rmdir()
        probe_dir.rmdir()
    except Exception:
        temp_root = _set_local_temp()

    # V√©rifier que mkdtemp cr√©e un dossier r√©ellement writable
    try:
        probe_dir = Path(tempfile.mkdtemp(dir=temp_root))
        test_path = probe_dir / "write_test.txt"
        test_path.write_text("ok", encoding="utf-8")
        test_path.unlink(missing_ok=True)
        probe_dir.rmdir()
    except Exception:
        tempfile.mkdtemp = _safe_mkdtemp


_ensure_writable_tempdir()

# =============================================================================
# HELPERS
# =============================================================================

def _safe_to_parquet(
    df: pd.DataFrame,
    path: Path,
    *,
    compression: Optional[str] = None,
    index: Optional[bool] = None,
) -> None:
    try:
        if index is None:
            index = True
        df.to_parquet(path, compression=compression, index=index)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Parquet non √©crit ({path.name}): {e}")


def _write_series_csv(series: pd.Series, path: Path, name: str) -> None:
    df = series.to_frame(name=name)
    df.to_csv(path, index=True, encoding="utf-8")


def _write_dataframe_csv(df: pd.DataFrame, path: Path) -> None:
    df.to_csv(path, index=False, encoding="utf-8")


# =============================================================================
# DATACLASSES
# =============================================================================

@dataclass
class StoredResultMetadata:
    """M√©tadonn√©es d'un r√©sultat sauvegard√©."""
    run_id: str
    timestamp: str
    strategy: str
    symbol: str
    timeframe: str
    params: Dict[str, Any]
    metrics: PerformanceMetricsPct
    n_bars: int
    n_trades: int
    period_start: str
    period_end: str
    duration_sec: float

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dict pour s√©rialisation."""
        payload = asdict(self)
        payload["metrics"] = normalize_metrics(self.metrics, "pct")
        return payload

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "StoredResultMetadata":
        """Cr√©e depuis un dict."""
        metrics = normalize_metrics(data.get("metrics", {}), "pct")
        return cls(
            run_id=data["run_id"],
            timestamp=data["timestamp"],
            strategy=data["strategy"],
            symbol=data["symbol"],
            timeframe=data["timeframe"],
            params=data.get("params", {}),
            metrics=metrics,
            n_bars=data["n_bars"],
            n_trades=data["n_trades"],
            period_start=data.get("period_start", ""),
            period_end=data.get("period_end", ""),
            duration_sec=data.get("duration_sec", 0.0),
        )


# =============================================================================
# STORAGE ENGINE
# =============================================================================

class ResultStorage:
    """
    Gestionnaire de stockage des r√©sultats de backtests.

    Features:
    - Sauvegarde automatique avec structure organis√©e
    - Index pour recherche rapide
    - Compression optionnelle
    - Nettoyage automatique des anciens r√©sultats

    Example:
        >>> storage = ResultStorage()
        >>>
        >>> # Sauvegarder un r√©sultat
        >>> storage.save_result(run_result)
        >>>
        >>> # Lister tous les r√©sultats
        >>> all_results = storage.list_results()
        >>>
        >>> # Rechercher
        >>> best_runs = storage.search_results(min_sharpe=2.0)
        >>>
        >>> # Charger un r√©sultat sp√©cifique
        >>> result = storage.load_result(run_id)
    """

    def __init__(
        self,
        storage_dir: Optional[Union[str, Path]] = None,
        auto_save: bool = True,
        compress: bool = False,
    ):
        """
        Initialise le gestionnaire de stockage.

        Args:
            storage_dir: R√©pertoire de stockage (d√©faut: backtest_results/)
            auto_save: Activer la sauvegarde automatique
            compress: Compresser les fichiers Parquet
        """
        self.storage_dir = Path(storage_dir) if storage_dir else DEFAULT_STORAGE_DIR
        self.auto_save = auto_save
        self.compress = compress

        # Cr√©er le r√©pertoire si n√©cessaire
        self.storage_dir.mkdir(parents=True, exist_ok=True)

        # Chemin de l'index
        self.index_path = self.storage_dir / "index.json"

        # Charger ou cr√©er l'index
        self._index: Dict[str, StoredResultMetadata] = self._load_index()

        logger.info(f"ResultStorage initialis√©: {self.storage_dir} ({len(self._index)} r√©sultats)")

    # =========================================================================
    # SAUVEGARDE
    # =========================================================================

    def save_result(
        self,
        result: RunResult,
        run_id: Optional[str] = None,
        auto_cleanup: bool = False,
    ) -> str:
        """
        Sauvegarde un r√©sultat de backtest.

        Args:
            result: RunResult √† sauvegarder
            run_id: ID personnalis√© (sinon utilise result.meta['run_id'])
            auto_cleanup: Nettoyer les anciens r√©sultats si trop nombreux

        Returns:
            run_id du r√©sultat sauvegard√©
        """
        # R√©cup√©rer ou g√©n√©rer le run_id
        if run_id is None:
            run_id = result.meta.get("run_id", f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}")

        # Cr√©er le r√©pertoire du run
        run_dir = self.storage_dir / run_id
        run_dir.mkdir(parents=True, exist_ok=True)

        try:
            # 1. Sauvegarder les m√©tadonn√©es
            metrics_pct = normalize_metrics(result.metrics, "pct")
            metadata = StoredResultMetadata(
                run_id=run_id,
                timestamp=datetime.now().isoformat(),
                strategy=result.meta.get("strategy", "unknown"),
                symbol=result.meta.get("symbol", "unknown"),
                timeframe=result.meta.get("timeframe", "unknown"),
                params=result.meta.get("params", {}),
                metrics=metrics_pct,
                n_bars=result.meta.get("n_bars", len(result.equity)),
                n_trades=len(result.trades),
                period_start=result.meta.get("period_start", ""),
                period_end=result.meta.get("period_end", ""),
                duration_sec=result.meta.get("duration_sec", 0.0),
            )

            metadata_path = run_dir / "metadata.json"
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata.to_dict(), f, indent=2, ensure_ascii=False)

            # 2. Sauvegarder la courbe d'√©quit√©
            equity_path = run_dir / "equity.parquet"
            equity_df = result.equity.to_frame(name="equity")
            _safe_to_parquet(
                equity_df,
                equity_path,
                compression="snappy" if self.compress else None,
                index=True,
            )
            _write_series_csv(result.equity, run_dir / "equity.csv", "equity")

            # 3. Sauvegarder les trades
            trades_path = run_dir / "trades.parquet"
            _safe_to_parquet(
                result.trades,
                trades_path,
                compression="snappy" if self.compress else None,
                index=False,
            )
            _write_dataframe_csv(result.trades, run_dir / "trades.csv")

            # 4. Sauvegarder les returns
            returns_path = run_dir / "returns.parquet"
            returns_df = result.returns.to_frame(name="returns")
            _safe_to_parquet(
                returns_df,
                returns_path,
                compression="snappy" if self.compress else None,
                index=True,
            )
            _write_series_csv(result.returns, run_dir / "returns.csv", "returns")

            # 4b. Rapport JSON/MD (artefacts unifi√©s)
            report_payload = {
                "run_id": run_id,
                "timestamp": metadata.timestamp,
                "strategy": metadata.strategy,
                "symbol": metadata.symbol,
                "timeframe": metadata.timeframe,
                "params": metadata.params,
                "metrics": metadata.to_dict().get("metrics", metrics_pct),
                "n_bars": metadata.n_bars,
                "n_trades": metadata.n_trades,
                "period_start": metadata.period_start,
                "period_end": metadata.period_end,
                "duration_sec": metadata.duration_sec,
            }
            report_json_path = run_dir / "report.json"
            with open(report_json_path, "w", encoding="utf-8") as f:
                json.dump(report_payload, f, indent=2, ensure_ascii=False)

            report_md_path = run_dir / "report.md"
            report_lines = [
                "# Rapport Backtest",
                "",
                f"- Run ID: `{run_id}`",
                f"- Strat√©gie: **{metadata.strategy}**",
                f"- Symbole: **{metadata.symbol}**",
                f"- Timeframe: **{metadata.timeframe}**",
                f"- P√©riode: {metadata.period_start} ‚Üí {metadata.period_end}",
                f"- Trades: {metadata.n_trades}",
                "",
                "## M√©triques cl√©s",
                "",
                f"- Return %: {metrics_pct.get('total_return_pct', 0):.2f}%",
                f"- Sharpe: {metrics_pct.get('sharpe_ratio', 0):.2f}",
                f"- Max DD %: {metrics_pct.get('max_drawdown_pct', metrics_pct.get('max_drawdown', 0)):.2f}%",
            ]
            report_md_path.write_text("\n".join(report_lines), encoding="utf-8")

            # 5. Mettre √† jour l'index
            self._index[run_id] = metadata
            self._save_index()

            # NOTE: build_catalogs() n'est PAS appel√© automatiquement ici pour pr√©server
            # les performances. Appelez-le manuellement ou via UI si n√©cessaire.

            logger.info(f"‚úÖ R√©sultat sauvegard√©: {run_id} ({metadata.strategy})")

            # Nettoyage optionnel
            if auto_cleanup:
                self._cleanup_old_results()

            return run_id

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde: {e}")
            # Nettoyer en cas d'erreur
            if run_dir.exists():
                shutil.rmtree(run_dir)
            raise

    def save_sweep_results(
        self,
        sweep_results: SweepResults,
        sweep_id: Optional[str] = None,
    ) -> str:
        """
        Sauvegarde les r√©sultats d'un sweep.

        Args:
            sweep_results: SweepResults √† sauvegarder
            sweep_id: ID personnalis√© du sweep

        Returns:
            sweep_id
        """
        if sweep_id is None:
            sweep_id = f"sweep_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        sweep_dir = self.storage_dir / sweep_id
        sweep_dir.mkdir(parents=True, exist_ok=True)

        try:
            # Sauvegarder le r√©sum√©
            summary = {
                "sweep_id": sweep_id,
                "timestamp": datetime.now().isoformat(),
                "n_completed": sweep_results.n_completed,
                "n_failed": sweep_results.n_failed,
                "total_time": sweep_results.total_time,
                "best_params": sweep_results.best_params,
                "best_metrics": normalize_metrics(sweep_results.best_metrics, "pct"),
                "resource_stats": sweep_results.resource_stats,
            }

            summary_path = sweep_dir / "summary.json"
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)

            # Sauvegarder tous les r√©sultats en DataFrame
            results_df = sweep_results.to_dataframe()
            results_path = sweep_dir / "all_results.parquet"
            results_df.to_parquet(
                results_path,
                compression="snappy" if self.compress else None,
                index=False,
            )

            logger.info(f"‚úÖ Sweep sauvegard√©: {sweep_id} ({sweep_results.n_completed} r√©sultats)")

            return sweep_id

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde du sweep: {e}")
            if sweep_dir.exists():
                shutil.rmtree(sweep_dir)
            raise

    # =========================================================================
    # CHARGEMENT
    # =========================================================================

    def load_result(self, run_id: str) -> RunResult:
        """
        Charge un r√©sultat de backtest.

        Args:
            run_id: ID du run √† charger

        Returns:
            RunResult reconstruit

        Raises:
            FileNotFoundError: Si le run_id n'existe pas
        """
        run_dir = self.storage_dir / run_id

        if not run_dir.exists():
            raise FileNotFoundError(f"Run inexistant: {run_id}")

        try:
            # 1. Charger les m√©tadonn√©es
            metadata_path = run_dir / "metadata.json"
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata_dict = json.load(f)
            metadata = StoredResultMetadata.from_dict(metadata_dict)

            # 2. Charger l'√©quit√©
            equity_path = run_dir / "equity.parquet"
            if equity_path.exists():
                equity_df = pd.read_parquet(equity_path)
                equity = equity_df["equity"]
            else:
                equity_csv = run_dir / "equity.csv"
                equity_df = pd.read_csv(equity_csv, index_col=0)
                equity = equity_df["equity"]

            # 3. Charger les trades
            trades_path = run_dir / "trades.parquet"
            if trades_path.exists():
                trades = pd.read_parquet(trades_path)
            else:
                trades = pd.read_csv(run_dir / "trades.csv")

            # 4. Charger les returns
            returns_path = run_dir / "returns.parquet"
            if returns_path.exists():
                returns_df = pd.read_parquet(returns_path)
                returns = returns_df["returns"]
            else:
                returns_df = pd.read_csv(run_dir / "returns.csv", index_col=0)
                returns = returns_df["returns"]

            # 5. Reconstruire le RunResult
            result = RunResult(
                equity=equity,
                returns=returns,
                trades=trades,
                metrics=metadata.metrics,
                meta={
                    "run_id": metadata.run_id,
                    "strategy": metadata.strategy,
                    "symbol": metadata.symbol,
                    "timeframe": metadata.timeframe,
                    "params": metadata.params,
                    "n_bars": metadata.n_bars,
                    "period_start": metadata.period_start,
                    "period_end": metadata.period_end,
                    "duration_sec": metadata.duration_sec,
                    "loaded_from_storage": True,
                    "loaded_at": datetime.now().isoformat(),
                }
            )

            logger.info(f"‚úÖ R√©sultat charg√©: {run_id}")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de {run_id}: {e}")
            raise

    def load_sweep_results(self, sweep_id: str) -> Dict[str, Any]:
        """
        Charge les r√©sultats d'un sweep.

        Args:
            sweep_id: ID du sweep

        Returns:
            Dict avec summary et results_df
        """
        sweep_dir = self.storage_dir / sweep_id

        if not sweep_dir.exists():
            raise FileNotFoundError(f"Sweep inexistant: {sweep_id}")

        try:
            # Charger le r√©sum√©
            summary_path = sweep_dir / "summary.json"
            with open(summary_path, "r", encoding="utf-8") as f:
                summary = json.load(f)
            summary["best_metrics"] = normalize_metrics(
                summary.get("best_metrics", {}), "pct"
            )

            # Charger les r√©sultats
            results_path = sweep_dir / "all_results.parquet"
            results_df = pd.read_parquet(results_path)

            logger.info(f"‚úÖ Sweep charg√©: {sweep_id}")

            return {
                "summary": summary,
                "results_df": results_df,
                "sweep_id": sweep_id,
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du sweep {sweep_id}: {e}")
            raise

    # =========================================================================
    # RECHERCHE & LISTAGE
    # =========================================================================

    def list_results(
        self,
        limit: Optional[int] = None,
        sort_by: str = "timestamp",
        reverse: bool = True,
    ) -> List[StoredResultMetadata]:
        """
        Liste tous les r√©sultats disponibles.

        Args:
            limit: Limiter le nombre de r√©sultats
            sort_by: Champ de tri (timestamp, sharpe_ratio, etc.)
            reverse: Tri descendant

        Returns:
            Liste de m√©tadonn√©es
        """
        results = list(self._index.values())

        # Tri
        if sort_by == "timestamp":
            results.sort(key=lambda x: x.timestamp, reverse=reverse)
        elif sort_by == "sharpe_ratio":
            results.sort(
                key=lambda x: x.metrics.get("sharpe_ratio", 0),
                reverse=reverse
            )
        elif sort_by == "total_return":
            results.sort(
                key=lambda x: x.metrics.get("total_return_pct", 0),
                reverse=reverse
            )

        # Limite
        if limit:
            results = results[:limit]

        return results

    def search_results(
        self,
        strategy: Optional[str] = None,
        symbol: Optional[str] = None,
        timeframe: Optional[str] = None,
        min_sharpe: Optional[float] = None,
        max_drawdown: Optional[float] = None,
        min_trades: Optional[int] = None,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None,
    ) -> List[StoredResultMetadata]:
        """
        Recherche des r√©sultats avec filtres.

        Args:
            strategy: Nom de la strat√©gie
            symbol: Symbole
            timeframe: Timeframe
            min_sharpe: Sharpe ratio minimum
            max_drawdown: Drawdown maximum (%)
            min_trades: Nombre minimum de trades
            date_from: Date minimum (ISO format)
            date_to: Date maximum (ISO format)

        Returns:
            Liste de m√©tadonn√©es filtr√©es
        """
        results = list(self._index.values())

        # Filtres
        if strategy:
            results = [r for r in results if r.strategy == strategy]

        if symbol:
            results = [r for r in results if r.symbol == symbol]

        if timeframe:
            results = [r for r in results if r.timeframe == timeframe]

        if min_sharpe is not None:
            results = [
                r for r in results
                if r.metrics.get("sharpe_ratio", 0) >= min_sharpe
            ]

        if max_drawdown is not None:
            results = [
                r for r in results
                if r.metrics.get("max_drawdown_pct", 100) <= max_drawdown
            ]

        if min_trades is not None:
            results = [r for r in results if r.n_trades >= min_trades]

        if date_from:
            results = [r for r in results if r.timestamp >= date_from]

        if date_to:
            results = [r for r in results if r.timestamp <= date_to]

        return results

    def get_best_results(
        self,
        n: int = 10,
        metric: str = "sharpe_ratio",
    ) -> List[StoredResultMetadata]:
        """
        Retourne les N meilleurs r√©sultats selon une m√©trique.

        Args:
            n: Nombre de r√©sultats
            metric: M√©trique de tri

        Returns:
            Liste des meilleurs r√©sultats
        """
        results = list(self._index.values())
        results.sort(
            key=lambda x: x.metrics.get(metric, float("-inf")),
            reverse=True
        )
        return results[:n]

    # =========================================================================
    # GESTION
    # =========================================================================

    def delete_result(self, run_id: str) -> bool:
        """
        Supprime un r√©sultat.

        Args:
            run_id: ID du run √† supprimer

        Returns:
            True si supprim√©, False sinon
        """
        run_dir = self.storage_dir / run_id

        if not run_dir.exists():
            logger.warning(f"‚ö†Ô∏è Run inexistant: {run_id}")
            return False

        try:
            shutil.rmtree(run_dir)

            if run_id in self._index:
                del self._index[run_id]
                self._save_index()

            logger.info(f"üóëÔ∏è R√©sultat supprim√©: {run_id}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la suppression: {e}")
            return False

    def _cleanup_old_results(self, keep_last: int = MAX_RESULTS_TO_KEEP) -> int:
        """
        Nettoie les anciens r√©sultats pour √©viter l'accumulation.

        Args:
            keep_last: Nombre de r√©sultats √† garder

        Returns:
            Nombre de r√©sultats supprim√©s
        """
        results = list(self._index.values())
        results.sort(key=lambda x: x.timestamp, reverse=True)

        to_delete = results[keep_last:]
        deleted_count = 0

        for result in to_delete:
            if self.delete_result(result.run_id):
                deleted_count += 1

        if deleted_count > 0:
            logger.info(f"üßπ Nettoyage: {deleted_count} anciens r√©sultats supprim√©s")

        return deleted_count

    def clear_all(self) -> bool:
        """
        Supprime TOUS les r√©sultats (attention!).

        Returns:
            True si succ√®s
        """
        try:
            if self.storage_dir.exists():
                shutil.rmtree(self.storage_dir)
                self.storage_dir.mkdir(parents=True, exist_ok=True)

            self._index = {}
            self._save_index()

            logger.warning("üßπ TOUS les r√©sultats ont √©t√© supprim√©s")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du nettoyage: {e}")
            return False

    # =========================================================================
    # INDEX
    # =========================================================================

    def _load_index(self) -> Dict[str, StoredResultMetadata]:
        """Charge l'index depuis le disque."""
        if not self.index_path.exists():
            return {}

        try:
            with open(self.index_path, "r", encoding="utf-8") as f:
                index_data = json.load(f)

            # Reconstruire les objets StoredResultMetadata
            index = {}
            for run_id, meta_dict in index_data.items():
                try:
                    index[run_id] = StoredResultMetadata.from_dict(meta_dict)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è M√©tadonn√©e corrompue pour {run_id}: {e}")

            return index

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de l'index: {e}")
            return {}

    def _save_index(self) -> None:
        """Sauvegarde l'index sur le disque."""
        try:
            index_data = {
                run_id: meta.to_dict()
                for run_id, meta in self._index.items()
            }

            with open(self.index_path, "w", encoding="utf-8") as f:
                json.dump(index_data, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde de l'index: {e}")

    def rebuild_index(self) -> int:
        """
        Reconstruit l'index en scannant tous les r√©pertoires.

        Utile en cas de corruption ou si des fichiers ont √©t√© ajout√©s manuellement.

        Returns:
            Nombre de r√©sultats index√©s
        """
        logger.info("üîÑ Reconstruction de l'index...")

        self._index = {}
        count = 0

        for run_dir in self.storage_dir.iterdir():
            if not run_dir.is_dir():
                continue

            metadata_path = run_dir / "metadata.json"
            if not metadata_path.exists():
                continue

            try:
                with open(metadata_path, "r", encoding="utf-8") as f:
                    meta_dict = json.load(f)

                metadata = StoredResultMetadata.from_dict(meta_dict)
                self._index[metadata.run_id] = metadata
                count += 1

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de charger {run_dir.name}: {e}")

        self._save_index()
        logger.info(f"‚úÖ Index reconstruit: {count} r√©sultats")

        return count

    def build_catalogs(self, force: bool = False) -> Path:
        """
        G√©n√®re un catalogue CSV pour exploration rapide des r√©sultats.

        IMPORTANT: Cette m√©thode est co√ªteuse et N'EST PAS appel√©e automatiquement
        pendant save_result() pour pr√©server les performances des backtests.

        Appels recommand√©s:
        - Manuellement via CLI: storage.build_catalogs()
        - Via UI avec bouton "Refresh"
        - P√©riodiquement (ex: toutes les 100 sauvegardes)

        Args:
            force: Forcer la r√©g√©n√©ration m√™me si le catalogue est √† jour

        Returns:
            Path du fichier overview.csv g√©n√©r√©

        Example:
            >>> storage = get_storage()
            >>> catalog_path = storage.build_catalogs()
            >>> print(f"Catalogue: {catalog_path}")
        """
        catalog_dir = self.storage_dir / "_catalog"
        catalog_dir.mkdir(parents=True, exist_ok=True)

        overview_path = catalog_dir / "overview.csv"

        # Lazy update: v√©rifier si le catalogue est d√©j√† √† jour
        if not force and overview_path.exists():
            catalog_mtime = overview_path.stat().st_mtime
            index_mtime = self.index_path.stat().st_mtime if self.index_path.exists() else 0

            if catalog_mtime > index_mtime:
                logger.info("‚úÖ Catalogue d√©j√† √† jour (lazy skip)")
                return overview_path

        logger.info("üìä G√©n√©ration du catalogue CSV...")

        # Construire le DataFrame depuis l'index
        rows = []
        for run_id, metadata in self._index.items():
            row = {
                "type": "run",
                "id": run_id,
                "run_id": run_id,
                "timestamp": metadata.timestamp,
                "strategy": metadata.strategy,
                "symbol": metadata.symbol,
                "timeframe": metadata.timeframe,
                "n_bars": metadata.n_bars,
                "n_trades": metadata.n_trades,
                "duration_sec": metadata.duration_sec,
                "period_start": metadata.period_start,
                "period_end": metadata.period_end,
            }

            # Params -> params_*
            for key, value in (metadata.params or {}).items():
                row[f"params_{key}"] = value

            # Metrics -> metrics_*
            for key, value in (metadata.metrics or {}).items():
                row[f"metrics_{key}"] = value

            # Flags -> flags_*
            account_ruined = bool(metadata.metrics.get("account_ruined", False))
            row["flags_account_ruined"] = account_ruined
            rows.append(row)

        if not rows:
            logger.warning("‚ö†Ô∏è Aucun r√©sultat √† cataloguer")
            # Cr√©er un CSV vide avec headers
            df = pd.DataFrame(
                columns=[
                    "type",
                    "id",
                    "run_id",
                    "timestamp",
                    "strategy",
                    "symbol",
                    "timeframe",
                    "flags_account_ruined",
                ]
            )
        else:
            df = pd.DataFrame(rows)

            # Tri par timestamp d√©croissant
            if "timestamp" in df.columns:
                df = df.sort_values("timestamp", ascending=False)

        # Sauvegarder
        df.to_csv(overview_path, index=False, encoding="utf-8")

        logger.info(f"‚úÖ Catalogue g√©n√©r√©: {overview_path} ({len(rows)} r√©sultats)")

        return overview_path

    def validate_integrity(self, auto_fix: bool = True) -> Dict[str, List[str]]:
        """
        Valide la coh√©rence du stockage et r√©pare si n√©cessaire.

        V√©rifications:
        - Index.json coh√©rent avec les dossiers r√©els
        - Fichiers Parquet requis pr√©sents (equity, trades, returns)
        - M√©tadonn√©es valides

        Args:
            auto_fix: Tenter de r√©parer automatiquement les probl√®mes

        Returns:
            Dict avec cl√©s:
            - errors: Liste des erreurs critiques
            - warnings: Liste des avertissements
            - fixed: Liste des probl√®mes r√©par√©s

        Example:
            >>> storage = get_storage()
            >>> report = storage.validate_integrity()
            >>> if report["errors"]:
            ...     print(f"Erreurs: {report['errors']}")
        """
        logger.info("üîç Validation de l'int√©grit√© du stockage...")

        errors: List[str] = []
        warnings: List[str] = []
        fixed: List[str] = []

        # 1. V√©rifier que l'index existe
        if not self.index_path.exists():
            warnings.append("Index.json manquant")
            if auto_fix:
                self._save_index()
                fixed.append("Index.json cr√©√©")

        # 2. Scanner les dossiers r√©els
        actual_dirs = set()
        for item in self.storage_dir.iterdir():
            if item.is_dir() and item.name not in ["_catalog", "__pycache__"]:
                actual_dirs.add(item.name)

        # 3. Comparer index vs dossiers r√©els
        indexed_runs = set(self._index.keys())

        # Runs dans l'index mais dossier manquant
        missing_dirs = indexed_runs - actual_dirs
        for run_id in missing_dirs:
            warnings.append(f"Dossier manquant pour run_id index√©: {run_id}")
            if auto_fix:
                del self._index[run_id]
                fixed.append(f"Supprim√© de l'index: {run_id}")

        # Dossiers pr√©sents mais non index√©s
        unindexed_dirs = actual_dirs - indexed_runs
        for dir_name in unindexed_dirs:
            warnings.append(f"Dossier non index√©: {dir_name}")
            if auto_fix:
                # Tenter de charger et indexer
                try:
                    metadata_path = self.storage_dir / dir_name / "metadata.json"
                    if metadata_path.exists():
                        with open(metadata_path, "r", encoding="utf-8") as f:
                            meta_dict = json.load(f)
                        metadata = StoredResultMetadata.from_dict(meta_dict)
                        self._index[metadata.run_id] = metadata
                        fixed.append(f"Ajout√© √† l'index: {dir_name}")
                    else:
                        warnings.append(f"Pas de metadata.json dans {dir_name}")
                except Exception as e:
                    errors.append(f"Impossible d'indexer {dir_name}: {e}")

        # 4. V√©rifier les fichiers requis pour chaque run index√©
        required_files = ["metadata.json", "equity.parquet", "trades.parquet", "returns.parquet"]

        for run_id in list(self._index.keys()):
            run_dir = self.storage_dir / run_id
            if not run_dir.exists():
                continue  # D√©j√† trait√© ci-dessus

            for filename in required_files:
                file_path = run_dir / filename
                if not file_path.exists():
                    warnings.append(f"{run_id}: Fichier manquant {filename}")

        # 5. Sauvegarder l'index si des corrections ont √©t√© apport√©es
        if fixed and auto_fix:
            self._save_index()
            logger.info(f"‚úÖ Index mis √† jour apr√®s r√©paration")

        # Rapport final
        logger.info(
            f"Validation termin√©e: {len(errors)} erreurs, "
            f"{len(warnings)} avertissements, {len(fixed)} r√©parations"
        )

        return {
            "errors": errors,
            "warnings": warnings,
            "fixed": fixed,
        }


# =============================================================================
# INSTANCE GLOBALE
# =============================================================================

_storage_instance: Optional[ResultStorage] = None


def get_storage(
    storage_dir: Optional[Union[str, Path]] = None,
    auto_save: bool = True,
    compress: bool = False,
) -> ResultStorage:
    """
    Retourne l'instance globale de ResultStorage (singleton).

    Args:
        storage_dir: R√©pertoire de stockage
        auto_save: Activer la sauvegarde automatique
        compress: Compresser les fichiers

    Returns:
        ResultStorage instance
    """
    global _storage_instance
    if _storage_instance is None:
        _storage_instance = ResultStorage(
            storage_dir=storage_dir,
            auto_save=auto_save,
            compress=compress,
        )
    return _storage_instance


__all__ = [
    "ResultStorage",
    "StoredResultMetadata",
    "get_storage",
]


# Docstring update summary
# - Docstring de module normalis√©e (LLM-friendly) centr√©e sur persistance/indexation
# - Conventions structure r√©pertoires et index.json explicit√©es
# - Read-if/Skip-if ajout√©s pour tri rapide