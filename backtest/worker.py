"""
Module worker isolÃ© pour les backtests parallÃ¨les.

Ce module est sÃ©parÃ© de l'UI Streamlit pour Ã©viter les problÃ¨mes de pickling
quand Streamlit recharge ses modules (hot-reload).

La fonction `run_backtest_worker` est stable et ne change pas de rÃ©fÃ©rence
pendant l'exÃ©cution d'un sweep.

PERFORMANCE NOTE (#3): Imports lourds ~100-300ms par worker au dÃ©marrage.
C'est une limitation intrinsÃ¨que de Windows multiprocessing 'spawn' mode.
Avec 24 workers: ~7s de latence initiale (coÃ»t fixe unique).
"""
from __future__ import annotations

import os
from typing import Any, Dict

# Ces imports sont faits au niveau du module pour Ãªtre disponibles dans les workers
# âš ï¸ FIX #3: Import coÃ»teux mais inÃ©vitable avec Windows spawn mode
try:
    from backtest.engine import BacktestEngine
except ImportError:
    BacktestEngine = None

# Variable globale pour le DataFrame (partagÃ©e entre tous les backtests d'un worker)
# InitialisÃ©e une seule fois par worker via init_worker_with_dataframe()
_worker_dataframe = None
_worker_strategy_key = None
_worker_symbol = None
_worker_timeframe = None
_worker_initial_capital = None
_worker_debug_enabled = False
_worker_fast_metrics = False
_worker_period_days = None
_worker_engine = None
_worker_indicator_cache = None  # Cache des indicateurs calculÃ©s

# GPU Queue globals pour calcul parallÃ¨le des indicateurs
_worker_gpu_request_queue = None
_worker_gpu_response_queue = None


def init_worker_with_dataframe(
    df_or_path,
    strategy_key: str,
    symbol: str,
    timeframe: str,
    initial_capital: float,
    debug_enabled: bool,
    thread_limit: int,
    fast_metrics: bool = False,
    is_path: bool = False,
):
    """
    Initializer pour ProcessPoolExecutor - charge le DataFrame une seule fois.

    Cette fonction est appelÃ©e une fois par worker au dÃ©marrage du pool.
    Le DataFrame est stockÃ© en variable globale pour Ã©viter la sÃ©rialisation pickle
    rÃ©pÃ©tÃ©e Ã  chaque soumission de tÃ¢che.

    IMPORTANT: Application ROBUSTE des limites de threads BLAS pour Ã©viter
    nested parallelism (8 workers Ã— 16 threads BLAS = 128 threads â†’ surcharge CPU).
    La limitation est appliquÃ©e PROGRAMMATIQUEMENT, pas seulement via env vars.

    Args:
        df_or_path: DataFrame OHLCV complet OU chemin vers fichier parquet (si is_path=True)
        strategy_key: Nom de la stratÃ©gie
        symbol: Symbole (ex: BTCUSDC)
        timeframe: Timeframe (ex: 1h)
        initial_capital: Capital initial
        debug_enabled: Activer logs de debug
        thread_limit: Limite de threads CPU (0 = auto-detect, recommandÃ©: 1)
        fast_metrics: Utiliser les mÃ©triques rapides pour les sweeps
        is_path: Si True, df_or_path est un chemin de fichier Ã  charger

    Note:
        GPU queues sont automatiquement rÃ©cupÃ©rÃ©es depuis GPUContextManager
        si initialisÃ©es par SweepEngine.run_sweep() avant le lancement des workers.
    """
    global _worker_dataframe, _worker_strategy_key, _worker_symbol
    global _worker_timeframe, _worker_initial_capital, _worker_debug_enabled
    global _worker_fast_metrics, _worker_period_days, _worker_engine
    global _worker_gpu_request_queue, _worker_gpu_response_queue
    global _worker_sweep_ready

    # Charger le DataFrame depuis le fichier ou utiliser celui fourni
    if is_path:
        import pandas as pd
        _worker_dataframe = pd.read_parquet(df_or_path)
    else:
        _worker_dataframe = df_or_path
    _worker_strategy_key = strategy_key
    _worker_symbol = symbol
    _worker_timeframe = timeframe
    _worker_initial_capital = initial_capital
    _worker_debug_enabled = debug_enabled
    _worker_fast_metrics = fast_metrics
    _worker_engine = None

    # RÃ©initialiser le cache d'indicateurs pour ce worker
    # âš ï¸ FIX #6: Cache existant mais non utilisÃ© efficacement
    # TODO: ImplÃ©menter cache smart avec clÃ©s basÃ©es sur (indicator_name, params, data_hash)
    # Gain potentiel: 20-30% de performance sur stratÃ©gies avec indicateurs rÃ©pÃ©tÃ©s
    global _worker_indicator_cache
    _worker_indicator_cache = {}

    # Mode CPU-only: pas de queues GPU

    # PrÃ©-calcul du nombre de jours (Ã©vite coÃ»t rÃ©pÃ©tÃ©)
    _worker_period_days = None
    try:
        import pandas as pd
        start_day = pd.to_datetime(_worker_dataframe.index[0]).date()
        end_day = pd.to_datetime(_worker_dataframe.index[-1]).date()
        days = (end_day - start_day).days
        _worker_period_days = days if days > 0 else None
    except (ImportError, AttributeError, IndexError, TypeError):
        _worker_period_days = None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # LIMITATION THREADS BLAS - APPLICATION PROGRAMMATIQUE ROBUSTE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PROBLÃˆME: Sans limitation, NumPy/SciPy utilise tous les cÅ“urs par worker
    #   â†’ 8 workers Ã— 16 threads BLAS = 128 threads â†’ surcharge CPU massive
    # SOLUTION: Forcer 1 thread BLAS par worker de maniÃ¨re programmatique
    #   â†’ 8 workers Ã— 1 thread BLAS = 8 threads â†’ charge CPU optimale
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # DÃ©terminer la limite effective (dÃ©faut: 1 thread si non spÃ©cifiÃ©)
    effective_limit = thread_limit if thread_limit > 0 else 1

    # 1ï¸âƒ£ Variables d'environnement (fallback pour bibliothÃ¨ques qui ne supportent pas threadpoolctl)
    os.environ["BACKTEST_WORKER_THREADS"] = str(effective_limit)
    for var in (
        "OMP_NUM_THREADS",
        "MKL_NUM_THREADS",
        "OPENBLAS_NUM_THREADS",
        "NUMEXPR_NUM_THREADS",
        "VECLIB_MAXIMUM_THREADS",
        "BLIS_NUM_THREADS",
        "NUMBA_NUM_THREADS",  # ğŸš€ CRITIQUE: Ã‰viter nested parallelism Numba dans workers
    ):
        os.environ[var] = str(effective_limit)

    # 2ï¸âƒ£ threadpoolctl - APPLICATION PROGRAMMATIQUE (prioritÃ© haute)
    # C'est LA mÃ©thode robuste pour contrÃ´ler BLAS mÃªme sans env vars
    threadpoolctl_applied = False
    try:
        import threadpoolctl

        # Appliquer limite sur TOUS les thread pools (BLAS, OpenMP, TBB, etc.)
        threadpoolctl.threadpool_limits(limits=effective_limit)
        threadpoolctl_applied = True

        # VÃ©rification (optionnel en debug)
        if debug_enabled:
            info = threadpoolctl.threadpool_info()
            total_threads = sum(pool.get("num_threads", 0) for pool in info)
            import logging
            logger = logging.getLogger(__name__)
            logger.debug(
                "Worker thread limit applied: %d thread(s) across %d pool(s)",
                total_threads,
                len(info),
            )
    except ImportError:
        # threadpoolctl non installÃ© - fallback env vars uniquement
        if debug_enabled:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(
                "threadpoolctl not available - using env vars only (less reliable)"
            )
    except Exception as e:
        if debug_enabled:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning("threadpoolctl configuration failed: %s", e)

    # 3ï¸âƒ£ PyTorch (si disponible)
    try:
        import torch
        torch.set_num_threads(effective_limit)
        torch.set_num_interop_threads(max(1, effective_limit // 2))
    except (ImportError, AttributeError):
        pass  # Torch non utilisÃ© ou indisponible

    # 4ï¸âƒ£ VÃ©rification finale (optionnel - seulement en debug)
    if debug_enabled and not threadpoolctl_applied:
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(
            "âš ï¸ Thread limiting applied via env vars only - "
            "consider installing threadpoolctl for robust control: "
            "pip install threadpoolctl"
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5ï¸âƒ£ PRÃ‰-INITIALISATION ENGINE + MODE SWEEP RAPIDE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    _worker_sweep_ready = False
    try:
        if BacktestEngine is not None:
            _worker_engine = BacktestEngine(initial_capital=initial_capital)
            _worker_engine.prepare_sweep(
                _worker_dataframe,
                strategy_key,
                timeframe,
            )
            _worker_sweep_ready = True
    except Exception:
        _worker_sweep_ready = False
        _worker_engine = None


# Flag pour le mode sweep rapide (initialisÃ© dans init_worker_with_dataframe)
_worker_sweep_ready = False


def run_backtest_worker(param_combo: Dict[str, Any]) -> Dict[str, Any]:
    """
    Worker function pour ProcessPoolExecutor - isolÃ© du hot-reload Streamlit.

    Utilise automatiquement le mode sweep ultra-rapide si prepare_sweep()
    a Ã©tÃ© appelÃ© dans l'init. Sinon, fallback vers le chemin legacy.

    Args:
        param_combo: Dictionnaire des paramÃ¨tres de la stratÃ©gie Ã  tester

    Returns:
        Dict avec rÃ©sultats du backtest ou erreur
    """
    global _worker_engine

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # âš¡ MODE SWEEP ULTRA-RAPIDE (Ã©limine ~80% overhead Python)
    # Pas de: safe_run_backtest, logger, PerfCounters, strategy lookup,
    #         validation, RunResult, 20+ champs pickle
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if _worker_sweep_ready and _worker_engine is not None:
        try:
            metrics = _worker_engine.run_sweep_iteration(param_combo)

            total_trades = metrics.get("total_trades", 0)
            return {
                "params_dict": param_combo,
                "total_pnl": metrics.get("total_pnl", 0.0),
                "sharpe": metrics.get("sharpe_ratio", 0.0),
                "win_rate": metrics.get("win_rate_pct", metrics.get("win_rate", 0.0)),
                "max_dd": metrics.get("max_drawdown_pct", metrics.get("max_drawdown", 0.0)),
                "account_ruined": metrics.get("account_ruined", False),
                "total_trades": total_trades,
                "trades": total_trades,
                "profit_factor": metrics.get("profit_factor", 0.0),
                "period_days": _worker_period_days,
            }
        except Exception as e:
            if _worker_debug_enabled:
                import traceback as tb
                err = tb.format_exc()
            else:
                err = str(e)
            return {
                "params_dict": param_combo,
                "error": f"[sweep_fast] {err}",
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”„ CHEMIN LEGACY (fallback si prepare_sweep a Ã©chouÃ©)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    df = _worker_dataframe
    strategy_key = _worker_strategy_key
    symbol = _worker_symbol
    timeframe = _worker_timeframe
    initial_capital = _worker_initial_capital
    debug_enabled = _worker_debug_enabled
    fast_metrics = _worker_fast_metrics

    # Validation
    if df is None:
        return {
            "params_dict": param_combo,
            "error": "Worker not initialized - DataFrame is None",
        }

    period_days = _worker_period_days

    try:
        if BacktestEngine is None:
            return {
                "params_dict": param_combo,
                "error": "BacktestEngine not available in worker",
            }

        # CrÃ©er l'engine localement (pas picklable donc recrÃ©Ã© dans chaque process)
        # Note: Les limites de threads sont dÃ©jÃ  appliquÃ©es par init_worker_with_dataframe()
        if _worker_engine is None:
            _worker_engine = BacktestEngine(initial_capital=initial_capital)
        engine = _worker_engine

        # Import local pour Ã©viter les dÃ©pendances circulaires
        from ui.helpers import safe_run_backtest

        # IMPORTANT: ordre des arguments = (engine, df, strategy, params, symbol, timeframe)
        result_i, msg_i = safe_run_backtest(
            engine,
            df,
            strategy_key,
            param_combo,
            symbol=symbol,
            timeframe=timeframe,
            silent_mode=True,
            fast_metrics=fast_metrics,
        )

        if result_i is None:
            return {
                "params_dict": param_combo,
                "error": msg_i or "Backtest failed",
            }

        # Extraire les mÃ©triques avec fallback robuste
        m = {}
        if hasattr(result_i, "metrics") and result_i.metrics:
            m = result_i.metrics
        elif isinstance(result_i, dict):
            m = result_i.get("metrics", result_i)

        total_trades = m.get("total_trades", 0)
        return {
            "params_dict": param_combo,
            "total_pnl": m.get("total_pnl", 0.0),
            "sharpe": m.get("sharpe_ratio", 0.0),
            "win_rate": m.get("win_rate_pct", m.get("win_rate", 0.0)),
            "max_dd": m.get("max_drawdown_pct", m.get("max_drawdown", 0.0)),
            "account_ruined": m.get("account_ruined", False),
            "min_equity": m.get("min_equity", 0.0),
            "total_trades": total_trades,
            "trades": total_trades,
            "profit_factor": m.get("profit_factor", 0.0),
            "liquidation_total_pnl": m.get("liquidation_total_pnl", m.get("total_pnl", 0.0)),
            "liquidation_total_return_pct": m.get("liquidation_total_return_pct", m.get("total_return_pct", 0.0)),
            "liquidation_sharpe_ratio": m.get("liquidation_sharpe_ratio", m.get("sharpe_ratio", 0.0)),
            "liquidation_max_drawdown_pct": m.get("liquidation_max_drawdown_pct", m.get("max_drawdown_pct", 0.0)),
            "liquidation_triggered": m.get("liquidation_triggered", False),
            "liquidation_time": m.get("liquidation_time"),
            "consecutive_losses_max": m.get("consecutive_losses_max", 0),
            "avg_win_loss_ratio": m.get("avg_win_loss_ratio", 0.0),
            "robustness_score": m.get("robustness_score", 0.0),
            "data_coverage_pct": m.get("data_coverage_pct"),
            "period_days": period_days,
        }

    except Exception as e:
        # Capturer toute erreur d'exÃ©cution
        error_msg = str(e)
        if debug_enabled:
            import traceback
            error_msg = traceback.format_exc()
        return {
            "params_dict": param_combo,
            "error": error_msg,
        }
