"""
Backtest Core - Storage System
==============================

SystÃ¨me de sauvegarde et chargement automatique des rÃ©sultats de backtests.

Features:
- Sauvegarde automatique dans backtest_results/{run_id}/
- Format JSON pour mÃ©tadonnÃ©es + Parquet pour sÃ©ries temporelles
- Index/catalogue des rÃ©sultats pour recherche rapide
- Compression optionnelle
- Chargement rapide avec filtres

Structure de stockage:
    backtest_results/
    â”œâ”€â”€ index.json                  # Catalogue de tous les runs
    â”œâ”€â”€ {run_id_1}/
    â”‚   â”œâ”€â”€ metadata.json          # ParamÃ¨tres et mÃ©triques
    â”‚   â”œâ”€â”€ equity.parquet         # Courbe d'Ã©quitÃ©
    â”‚   â””â”€â”€ trades.parquet         # Historique des trades
    â”œâ”€â”€ {run_id_2}/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ...

Usage:
    >>> from backtest.storage import ResultStorage
    >>>
    >>> # Sauvegarder
    >>> storage = ResultStorage()
    >>> storage.save_result(run_result, auto_cleanup_old=True)
    >>>
    >>> # Charger
    >>> result = storage.load_result(run_id)
    >>>
    >>> # Rechercher
    >>> results = storage.search_results(strategy="bollinger_atr", min_sharpe=1.5)
"""

from __future__ import annotations

import json
import shutil
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import pandas as pd

from backtest.engine import RunResult
from backtest.sweep import SweepResults
from utils.log import get_logger

logger = get_logger(__name__)


# =============================================================================
# CONFIGURATION
# =============================================================================

DEFAULT_STORAGE_DIR = Path("backtest_results")
MAX_RESULTS_TO_KEEP = 1000  # Nombre maximum de rÃ©sultats Ã  garder


# =============================================================================
# DATACLASSES
# =============================================================================

@dataclass
class StoredResultMetadata:
    """MÃ©tadonnÃ©es d'un rÃ©sultat sauvegardÃ©."""
    run_id: str
    timestamp: str
    strategy: str
    symbol: str
    timeframe: str
    params: Dict[str, Any]
    metrics: Dict[str, Any]
    n_bars: int
    n_trades: int
    period_start: str
    period_end: str
    duration_sec: float

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dict pour sÃ©rialisation."""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "StoredResultMetadata":
        """CrÃ©e depuis un dict."""
        return cls(**data)


# =============================================================================
# STORAGE ENGINE
# =============================================================================

class ResultStorage:
    """
    Gestionnaire de stockage des rÃ©sultats de backtests.

    Features:
    - Sauvegarde automatique avec structure organisÃ©e
    - Index pour recherche rapide
    - Compression optionnelle
    - Nettoyage automatique des anciens rÃ©sultats

    Example:
        >>> storage = ResultStorage()
        >>>
        >>> # Sauvegarder un rÃ©sultat
        >>> storage.save_result(run_result)
        >>>
        >>> # Lister tous les rÃ©sultats
        >>> all_results = storage.list_results()
        >>>
        >>> # Rechercher
        >>> best_runs = storage.search_results(min_sharpe=2.0)
        >>>
        >>> # Charger un rÃ©sultat spÃ©cifique
        >>> result = storage.load_result(run_id)
    """

    def __init__(
        self,
        storage_dir: Optional[Union[str, Path]] = None,
        auto_save: bool = True,
        compress: bool = False,
    ):
        """
        Initialise le gestionnaire de stockage.

        Args:
            storage_dir: RÃ©pertoire de stockage (dÃ©faut: backtest_results/)
            auto_save: Activer la sauvegarde automatique
            compress: Compresser les fichiers Parquet
        """
        self.storage_dir = Path(storage_dir) if storage_dir else DEFAULT_STORAGE_DIR
        self.auto_save = auto_save
        self.compress = compress

        # CrÃ©er le rÃ©pertoire si nÃ©cessaire
        self.storage_dir.mkdir(parents=True, exist_ok=True)

        # Chemin de l'index
        self.index_path = self.storage_dir / "index.json"

        # Charger ou crÃ©er l'index
        self._index: Dict[str, StoredResultMetadata] = self._load_index()

        logger.info(f"ResultStorage initialisÃ©: {self.storage_dir} ({len(self._index)} rÃ©sultats)")

    # =========================================================================
    # SAUVEGARDE
    # =========================================================================

    def save_result(
        self,
        result: RunResult,
        run_id: Optional[str] = None,
        auto_cleanup: bool = False,
    ) -> str:
        """
        Sauvegarde un rÃ©sultat de backtest.

        Args:
            result: RunResult Ã  sauvegarder
            run_id: ID personnalisÃ© (sinon utilise result.meta['run_id'])
            auto_cleanup: Nettoyer les anciens rÃ©sultats si trop nombreux

        Returns:
            run_id du rÃ©sultat sauvegardÃ©
        """
        # RÃ©cupÃ©rer ou gÃ©nÃ©rer le run_id
        if run_id is None:
            run_id = result.meta.get("run_id", f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}")

        # CrÃ©er le rÃ©pertoire du run
        run_dir = self.storage_dir / run_id
        run_dir.mkdir(parents=True, exist_ok=True)

        try:
            # 1. Sauvegarder les mÃ©tadonnÃ©es
            metadata = StoredResultMetadata(
                run_id=run_id,
                timestamp=datetime.now().isoformat(),
                strategy=result.meta.get("strategy", "unknown"),
                symbol=result.meta.get("symbol", "unknown"),
                timeframe=result.meta.get("timeframe", "unknown"),
                params=result.meta.get("params", {}),
                metrics=result.metrics,
                n_bars=result.meta.get("n_bars", len(result.equity)),
                n_trades=len(result.trades),
                period_start=result.meta.get("period_start", ""),
                period_end=result.meta.get("period_end", ""),
                duration_sec=result.meta.get("duration_sec", 0.0),
            )

            metadata_path = run_dir / "metadata.json"
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata.to_dict(), f, indent=2, ensure_ascii=False)

            # 2. Sauvegarder la courbe d'Ã©quitÃ©
            equity_path = run_dir / "equity.parquet"
            equity_df = result.equity.to_frame(name="equity")
            equity_df.to_parquet(
                equity_path,
                compression="snappy" if self.compress else None,
                index=True,
            )

            # 3. Sauvegarder les trades
            trades_path = run_dir / "trades.parquet"
            result.trades.to_parquet(
                trades_path,
                compression="snappy" if self.compress else None,
                index=False,
            )

            # 4. Sauvegarder les returns
            returns_path = run_dir / "returns.parquet"
            returns_df = result.returns.to_frame(name="returns")
            returns_df.to_parquet(
                returns_path,
                compression="snappy" if self.compress else None,
                index=True,
            )

            # 5. Mettre Ã  jour l'index
            self._index[run_id] = metadata
            self._save_index()

            logger.info(f"âœ… RÃ©sultat sauvegardÃ©: {run_id} ({metadata.strategy})")

            # Nettoyage optionnel
            if auto_cleanup:
                self._cleanup_old_results()

            return run_id

        except Exception as e:
            logger.error(f"âŒ Erreur lors de la sauvegarde: {e}")
            # Nettoyer en cas d'erreur
            if run_dir.exists():
                shutil.rmtree(run_dir)
            raise

    def save_sweep_results(
        self,
        sweep_results: SweepResults,
        sweep_id: Optional[str] = None,
    ) -> str:
        """
        Sauvegarde les rÃ©sultats d'un sweep.

        Args:
            sweep_results: SweepResults Ã  sauvegarder
            sweep_id: ID personnalisÃ© du sweep

        Returns:
            sweep_id
        """
        if sweep_id is None:
            sweep_id = f"sweep_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        sweep_dir = self.storage_dir / sweep_id
        sweep_dir.mkdir(parents=True, exist_ok=True)

        try:
            # Sauvegarder le rÃ©sumÃ©
            summary = {
                "sweep_id": sweep_id,
                "timestamp": datetime.now().isoformat(),
                "n_completed": sweep_results.n_completed,
                "n_failed": sweep_results.n_failed,
                "total_time": sweep_results.total_time,
                "best_params": sweep_results.best_params,
                "best_metrics": sweep_results.best_metrics,
                "resource_stats": sweep_results.resource_stats,
            }

            summary_path = sweep_dir / "summary.json"
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)

            # Sauvegarder tous les rÃ©sultats en DataFrame
            results_df = sweep_results.to_dataframe()
            results_path = sweep_dir / "all_results.parquet"
            results_df.to_parquet(
                results_path,
                compression="snappy" if self.compress else None,
                index=False,
            )

            logger.info(f"âœ… Sweep sauvegardÃ©: {sweep_id} ({sweep_results.n_completed} rÃ©sultats)")

            return sweep_id

        except Exception as e:
            logger.error(f"âŒ Erreur lors de la sauvegarde du sweep: {e}")
            if sweep_dir.exists():
                shutil.rmtree(sweep_dir)
            raise

    # =========================================================================
    # CHARGEMENT
    # =========================================================================

    def load_result(self, run_id: str) -> RunResult:
        """
        Charge un rÃ©sultat de backtest.

        Args:
            run_id: ID du run Ã  charger

        Returns:
            RunResult reconstruit

        Raises:
            FileNotFoundError: Si le run_id n'existe pas
        """
        run_dir = self.storage_dir / run_id

        if not run_dir.exists():
            raise FileNotFoundError(f"Run inexistant: {run_id}")

        try:
            # 1. Charger les mÃ©tadonnÃ©es
            metadata_path = run_dir / "metadata.json"
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata_dict = json.load(f)
            metadata = StoredResultMetadata.from_dict(metadata_dict)

            # 2. Charger l'Ã©quitÃ©
            equity_path = run_dir / "equity.parquet"
            equity_df = pd.read_parquet(equity_path)
            equity = equity_df["equity"]

            # 3. Charger les trades
            trades_path = run_dir / "trades.parquet"
            trades = pd.read_parquet(trades_path)

            # 4. Charger les returns
            returns_path = run_dir / "returns.parquet"
            returns_df = pd.read_parquet(returns_path)
            returns = returns_df["returns"]

            # 5. Reconstruire le RunResult
            result = RunResult(
                equity=equity,
                returns=returns,
                trades=trades,
                metrics=metadata.metrics,
                meta={
                    "run_id": metadata.run_id,
                    "strategy": metadata.strategy,
                    "symbol": metadata.symbol,
                    "timeframe": metadata.timeframe,
                    "params": metadata.params,
                    "n_bars": metadata.n_bars,
                    "period_start": metadata.period_start,
                    "period_end": metadata.period_end,
                    "duration_sec": metadata.duration_sec,
                    "loaded_from_storage": True,
                    "loaded_at": datetime.now().isoformat(),
                }
            )

            logger.info(f"âœ… RÃ©sultat chargÃ©: {run_id}")
            return result

        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement de {run_id}: {e}")
            raise

    def load_sweep_results(self, sweep_id: str) -> Dict[str, Any]:
        """
        Charge les rÃ©sultats d'un sweep.

        Args:
            sweep_id: ID du sweep

        Returns:
            Dict avec summary et results_df
        """
        sweep_dir = self.storage_dir / sweep_id

        if not sweep_dir.exists():
            raise FileNotFoundError(f"Sweep inexistant: {sweep_id}")

        try:
            # Charger le rÃ©sumÃ©
            summary_path = sweep_dir / "summary.json"
            with open(summary_path, "r", encoding="utf-8") as f:
                summary = json.load(f)

            # Charger les rÃ©sultats
            results_path = sweep_dir / "all_results.parquet"
            results_df = pd.read_parquet(results_path)

            logger.info(f"âœ… Sweep chargÃ©: {sweep_id}")

            return {
                "summary": summary,
                "results_df": results_df,
                "sweep_id": sweep_id,
            }

        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement du sweep {sweep_id}: {e}")
            raise

    # =========================================================================
    # RECHERCHE & LISTAGE
    # =========================================================================

    def list_results(
        self,
        limit: Optional[int] = None,
        sort_by: str = "timestamp",
        reverse: bool = True,
    ) -> List[StoredResultMetadata]:
        """
        Liste tous les rÃ©sultats disponibles.

        Args:
            limit: Limiter le nombre de rÃ©sultats
            sort_by: Champ de tri (timestamp, sharpe_ratio, etc.)
            reverse: Tri descendant

        Returns:
            Liste de mÃ©tadonnÃ©es
        """
        results = list(self._index.values())

        # Tri
        if sort_by == "timestamp":
            results.sort(key=lambda x: x.timestamp, reverse=reverse)
        elif sort_by == "sharpe_ratio":
            results.sort(
                key=lambda x: x.metrics.get("sharpe_ratio", 0),
                reverse=reverse
            )
        elif sort_by == "total_return":
            results.sort(
                key=lambda x: x.metrics.get("total_return_pct", 0),
                reverse=reverse
            )

        # Limite
        if limit:
            results = results[:limit]

        return results

    def search_results(
        self,
        strategy: Optional[str] = None,
        symbol: Optional[str] = None,
        timeframe: Optional[str] = None,
        min_sharpe: Optional[float] = None,
        max_drawdown: Optional[float] = None,
        min_trades: Optional[int] = None,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None,
    ) -> List[StoredResultMetadata]:
        """
        Recherche des rÃ©sultats avec filtres.

        Args:
            strategy: Nom de la stratÃ©gie
            symbol: Symbole
            timeframe: Timeframe
            min_sharpe: Sharpe ratio minimum
            max_drawdown: Drawdown maximum (%)
            min_trades: Nombre minimum de trades
            date_from: Date minimum (ISO format)
            date_to: Date maximum (ISO format)

        Returns:
            Liste de mÃ©tadonnÃ©es filtrÃ©es
        """
        results = list(self._index.values())

        # Filtres
        if strategy:
            results = [r for r in results if r.strategy == strategy]

        if symbol:
            results = [r for r in results if r.symbol == symbol]

        if timeframe:
            results = [r for r in results if r.timeframe == timeframe]

        if min_sharpe is not None:
            results = [
                r for r in results
                if r.metrics.get("sharpe_ratio", 0) >= min_sharpe
            ]

        if max_drawdown is not None:
            results = [
                r for r in results
                if r.metrics.get("max_drawdown", 100) <= max_drawdown
            ]

        if min_trades is not None:
            results = [r for r in results if r.n_trades >= min_trades]

        if date_from:
            results = [r for r in results if r.timestamp >= date_from]

        if date_to:
            results = [r for r in results if r.timestamp <= date_to]

        return results

    def get_best_results(
        self,
        n: int = 10,
        metric: str = "sharpe_ratio",
    ) -> List[StoredResultMetadata]:
        """
        Retourne les N meilleurs rÃ©sultats selon une mÃ©trique.

        Args:
            n: Nombre de rÃ©sultats
            metric: MÃ©trique de tri

        Returns:
            Liste des meilleurs rÃ©sultats
        """
        results = list(self._index.values())
        results.sort(
            key=lambda x: x.metrics.get(metric, float("-inf")),
            reverse=True
        )
        return results[:n]

    # =========================================================================
    # GESTION
    # =========================================================================

    def delete_result(self, run_id: str) -> bool:
        """
        Supprime un rÃ©sultat.

        Args:
            run_id: ID du run Ã  supprimer

        Returns:
            True si supprimÃ©, False sinon
        """
        run_dir = self.storage_dir / run_id

        if not run_dir.exists():
            logger.warning(f"âš ï¸ Run inexistant: {run_id}")
            return False

        try:
            shutil.rmtree(run_dir)

            if run_id in self._index:
                del self._index[run_id]
                self._save_index()

            logger.info(f"ðŸ—‘ï¸ RÃ©sultat supprimÃ©: {run_id}")
            return True

        except Exception as e:
            logger.error(f"âŒ Erreur lors de la suppression: {e}")
            return False

    def _cleanup_old_results(self, keep_last: int = MAX_RESULTS_TO_KEEP) -> int:
        """
        Nettoie les anciens rÃ©sultats pour Ã©viter l'accumulation.

        Args:
            keep_last: Nombre de rÃ©sultats Ã  garder

        Returns:
            Nombre de rÃ©sultats supprimÃ©s
        """
        results = list(self._index.values())
        results.sort(key=lambda x: x.timestamp, reverse=True)

        to_delete = results[keep_last:]
        deleted_count = 0

        for result in to_delete:
            if self.delete_result(result.run_id):
                deleted_count += 1

        if deleted_count > 0:
            logger.info(f"ðŸ§¹ Nettoyage: {deleted_count} anciens rÃ©sultats supprimÃ©s")

        return deleted_count

    def clear_all(self) -> bool:
        """
        Supprime TOUS les rÃ©sultats (attention!).

        Returns:
            True si succÃ¨s
        """
        try:
            if self.storage_dir.exists():
                shutil.rmtree(self.storage_dir)
                self.storage_dir.mkdir(parents=True, exist_ok=True)

            self._index = {}
            self._save_index()

            logger.warning("ðŸ§¹ TOUS les rÃ©sultats ont Ã©tÃ© supprimÃ©s")
            return True

        except Exception as e:
            logger.error(f"âŒ Erreur lors du nettoyage: {e}")
            return False

    # =========================================================================
    # INDEX
    # =========================================================================

    def _load_index(self) -> Dict[str, StoredResultMetadata]:
        """Charge l'index depuis le disque."""
        if not self.index_path.exists():
            return {}

        try:
            with open(self.index_path, "r", encoding="utf-8") as f:
                index_data = json.load(f)

            # Reconstruire les objets StoredResultMetadata
            index = {}
            for run_id, meta_dict in index_data.items():
                try:
                    index[run_id] = StoredResultMetadata.from_dict(meta_dict)
                except Exception as e:
                    logger.warning(f"âš ï¸ MÃ©tadonnÃ©e corrompue pour {run_id}: {e}")

            return index

        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement de l'index: {e}")
            return {}

    def _save_index(self) -> None:
        """Sauvegarde l'index sur le disque."""
        try:
            index_data = {
                run_id: meta.to_dict()
                for run_id, meta in self._index.items()
            }

            with open(self.index_path, "w", encoding="utf-8") as f:
                json.dump(index_data, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"âŒ Erreur lors de la sauvegarde de l'index: {e}")

    def rebuild_index(self) -> int:
        """
        Reconstruit l'index en scannant tous les rÃ©pertoires.

        Utile en cas de corruption ou si des fichiers ont Ã©tÃ© ajoutÃ©s manuellement.

        Returns:
            Nombre de rÃ©sultats indexÃ©s
        """
        logger.info("ðŸ”„ Reconstruction de l'index...")

        self._index = {}
        count = 0

        for run_dir in self.storage_dir.iterdir():
            if not run_dir.is_dir():
                continue

            metadata_path = run_dir / "metadata.json"
            if not metadata_path.exists():
                continue

            try:
                with open(metadata_path, "r", encoding="utf-8") as f:
                    meta_dict = json.load(f)

                metadata = StoredResultMetadata.from_dict(meta_dict)
                self._index[metadata.run_id] = metadata
                count += 1

            except Exception as e:
                logger.warning(f"âš ï¸ Impossible de charger {run_dir.name}: {e}")

        self._save_index()
        logger.info(f"âœ… Index reconstruit: {count} rÃ©sultats")

        return count


# =============================================================================
# INSTANCE GLOBALE
# =============================================================================

_storage_instance: Optional[ResultStorage] = None


def get_storage(
    storage_dir: Optional[Union[str, Path]] = None,
    auto_save: bool = True,
    compress: bool = False,
) -> ResultStorage:
    """
    Retourne l'instance globale de ResultStorage (singleton).

    Args:
        storage_dir: RÃ©pertoire de stockage
        auto_save: Activer la sauvegarde automatique
        compress: Compresser les fichiers

    Returns:
        ResultStorage instance
    """
    global _storage_instance
    if _storage_instance is None:
        _storage_instance = ResultStorage(
            storage_dir=storage_dir,
            auto_save=auto_save,
            compress=compress,
        )
    return _storage_instance


__all__ = [
    "ResultStorage",
    "StoredResultMetadata",
    "get_storage",
]
