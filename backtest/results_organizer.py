"""
Module-ID: backtest.results_organizer

Purpose: Organiser automatiquement les r√©sultats de backtest en structure hi√©rarchique claire.

Role in pipeline: organization / archiving

Key components: organize_results, create_category_structure, archive_old_results

Inputs: R√©pertoire backtest_results

Outputs: Structure hi√©rarchique organis√©e par cat√©gorie/strat√©gie/performance

Dependencies: pathlib, shutil, json

Conventions: Organisation: backtest_results/{category}/{strategy}/{symbol_tf}/{run_id}

Read-if: Organisation ou archivage des r√©sultats.

Skip-if: Structure flat convient √† votre usage.
"""

import sys
import json
import shutil
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime, timedelta

# Ajouter le r√©pertoire racine au PYTHONPATH
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from backtest.report_generator import classify_result
from utils.log import get_logger

logger = get_logger(__name__)


# =============================================================================
# CONFIGURATION
# =============================================================================

CATEGORIES = {
    "excellent": "üèÜ_Excellent",
    "good": "‚úÖ_Good",
    "mediocre": "üìä_Mediocre",
    "unprofitable": "‚ùå_Unprofitable",
    "failed": "‚ùå_Failed",  # Alias pour unprofitable
    "ruined": "üíÄ_Ruined",
    "insufficient_data": "‚ö†Ô∏è_Insufficient_Data",
}

ARCHIVE_AFTER_DAYS = 90  # Archiver r√©sultats > 90 jours


# =============================================================================
# FONCTIONS D'ORGANISATION
# =============================================================================

def organize_results(
    results_dir: Path = Path("backtest_results"),
    organized_dir: Optional[Path] = None,
    dry_run: bool = False,
) -> Dict[str, int]:
    """
    Organise les r√©sultats de backtest en structure hi√©rarchique.

    Structure cible:
        backtest_results_organized/
        ‚îú‚îÄ‚îÄ üèÜ_Excellent/
        ‚îÇ   ‚îú‚îÄ‚îÄ ema_cross/
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BTCUSDC_1h/
        ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ run_20260103_abc123/
        ‚îÇ   ‚îî‚îÄ‚îÄ rsi_reversal/
        ‚îú‚îÄ‚îÄ ‚úÖ_Good/
        ‚îú‚îÄ‚îÄ üìä_Mediocre/
        ‚îú‚îÄ‚îÄ ‚ùå_Unprofitable/
        ‚îî‚îÄ‚îÄ üíÄ_Ruined/

    Args:
        results_dir: R√©pertoire source des r√©sultats
        organized_dir: R√©pertoire destination (d√©faut: backtest_results_organized)
        dry_run: Si True, affiche les actions sans les ex√©cuter

    Returns:
        Dict compteur par cat√©gorie

    Example:
        >>> stats = organize_results(dry_run=True)
        >>> print(stats)
        {"excellent": 5, "good": 12, ...}
    """
    if organized_dir is None:
        organized_dir = results_dir.parent / "backtest_results_organized"

    # Charger l'index
    index_path = results_dir / "index.json"
    if not index_path.exists():
        logger.error(f"Index introuvable: {index_path}")
        return {}

    with open(index_path, "r") as f:
        index_data = json.load(f)

    results = list(index_data.values())
    logger.info(f"üìä {len(results)} r√©sultats √† organiser")

    stats = {cat: 0 for cat in CATEGORIES.keys()}

    for result in results:
        run_id = result.get("run_id", "unknown")
        metrics = result.get("metrics", {})
        strategy = result.get("strategy", "unknown")
        symbol = result.get("symbol", "unknown")
        timeframe = result.get("timeframe", "unknown")
        timestamp = result.get("timestamp", "")

        # Classifier
        category, _ = classify_result(metrics)
        stats[category] += 1

        # Construire chemins
        category_name = CATEGORIES.get(category, "Unknown")
        symbol_tf = f"{symbol}_{timeframe}"

        # Extraire date du timestamp
        try:
            ts_dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
            date_str = ts_dt.strftime("%Y%m%d_%H%M%S")
        except (ValueError, TypeError, AttributeError) as e:
            logger.warning(f"Timestamp invalide: {timestamp}, erreur: {e}")
            date_str = "unknown"

        run_folder_name = f"{date_str}_{run_id[:8]}"

        source_path = results_dir / run_id
        target_path = organized_dir / category_name / strategy / symbol_tf / run_folder_name

        # Copier ou afficher
        if dry_run:
            logger.info(f"[DRY-RUN] {source_path.name} ‚Üí {target_path}")
        else:
            if source_path.exists():
                target_path.parent.mkdir(parents=True, exist_ok=True)
                if target_path.exists():
                    shutil.rmtree(target_path)
                shutil.copytree(source_path, target_path)
                logger.debug(f"‚úÖ Copi√©: {run_id} ‚Üí {target_path}")
            else:
                logger.warning(f"‚ö†Ô∏è Source introuvable: {source_path}")

    # Cr√©er README dans chaque cat√©gorie
    if not dry_run:
        create_category_readmes(organized_dir, stats)

    # Afficher statistiques
    logger.info("\n=== Statistiques d'organisation ===")
    for category, count in stats.items():
        emoji_name = CATEGORIES.get(category, category)
        logger.info(f"{emoji_name}: {count} r√©sultats")

    return stats


def create_category_readmes(organized_dir: Path, stats: Dict[str, int]):
    """
    Cr√©e des fichiers README.md dans chaque cat√©gorie.

    Args:
        organized_dir: R√©pertoire organis√©
        stats: Statistiques par cat√©gorie
    """
    descriptions = {
        "excellent": "üèÜ Configurations exceptionnelles (Return > 5% ET Sharpe > 2.0). **Pr√™tes pour production.**",
        "good": "‚úÖ Bonnes configurations (Return > 5% ET Sharpe > 1.0). √Ä valider sur d'autres timeframes/symboles.",
        "mediocre": "üìä Configurations rentables mais m√©diocres. N√©cessitent optimisation.",
        "unprofitable": "‚ùå Configurations non rentables. √Ä analyser pour comprendre les √©checs.",
        "ruined": "üíÄ **DANGER** : Ces configurations ont men√© √† la ruine du compte. √Ä √©viter absolument.",
        "insufficient_data": "‚ö†Ô∏è R√©sultats avec trop peu de trades (< 10). Donn√©es insuffisantes pour juger.",
    }

    for category, category_name in CATEGORIES.items():
        category_path = organized_dir / category_name
        if not category_path.exists():
            continue

        readme_path = category_path / "README.md"
        count = stats.get(category, 0)

        content = [
            f"# {category_name}",
            "",
            descriptions.get(category, "Cat√©gorie de r√©sultats."),
            "",
            f"**Nombre de r√©sultats:** {count}",
            "",
            "## Structure",
            "",
            "Les r√©sultats sont organis√©s par :",
            "1. **Strat√©gie** (ex: ema_cross, rsi_reversal)",
            "2. **Symbole + Timeframe** (ex: BTCUSDC_1h)",
            "3. **Date + Run ID** (ex: 20260103_123456_abc12345)",
            "",
            "## Fichiers dans chaque r√©sultat",
            "",
            "- `metadata.json` : Param√®tres et m√©triques compl√®tes",
            "- `equity.parquet` : Courbe d'√©quit√© (si disponible)",
            "- `trades.parquet` : Liste d√©taill√©e des trades (si disponible)",
            "",
            "---",
            f"*G√©n√©r√© le {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
        ]

        readme_path.write_text("\n".join(content), encoding="utf-8")
        logger.debug(f"üìÑ README cr√©√©: {readme_path}")


def archive_old_results(
    results_dir: Path = Path("backtest_results"),
    archive_dir: Optional[Path] = None,
    days_threshold: int = ARCHIVE_AFTER_DAYS,
    dry_run: bool = False,
) -> int:
    """
    Archive les r√©sultats anciens (> days_threshold jours).

    Args:
        results_dir: R√©pertoire des r√©sultats
        archive_dir: R√©pertoire d'archivage (d√©faut: backtest_results_archive)
        days_threshold: Seuil en jours pour archivage
        dry_run: Si True, affiche sans archiver

    Returns:
        Nombre de r√©sultats archiv√©s

    Example:
        >>> count = archive_old_results(days_threshold=90, dry_run=True)
        >>> print(f"{count} r√©sultats √† archiver")
    """
    if archive_dir is None:
        archive_dir = results_dir.parent / "backtest_results_archive"

    # Charger index
    index_path = results_dir / "index.json"
    if not index_path.exists():
        logger.error(f"Index introuvable: {index_path}")
        return 0

    with open(index_path, "r") as f:
        index_data = json.load(f)

    cutoff_date = datetime.now() - timedelta(days=days_threshold)
    archived_count = 0

    for run_id, result in index_data.items():
        timestamp_str = result.get("timestamp", "")
        try:
            ts_dt = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
        except (ValueError, TypeError, AttributeError) as e:
            logger.warning(f"‚ö†Ô∏è Timestamp invalide pour {run_id}: {timestamp_str}, erreur: {e}")
            continue

        if ts_dt < cutoff_date:
            source_path = results_dir / run_id
            target_path = archive_dir / run_id

            if dry_run:
                logger.info(f"[DRY-RUN] Archiver: {run_id} ({ts_dt.strftime('%Y-%m-%d')})")
            else:
                if source_path.exists():
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.move(str(source_path), str(target_path))
                    logger.debug(f"üì¶ Archiv√©: {run_id}")

            archived_count += 1

    logger.info(f"üì¶ {archived_count} r√©sultats archiv√©s (> {days_threshold} jours)")
    return archived_count


# =============================================================================
# SCRIPT PRINCIPAL
# =============================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Organiser et archiver les r√©sultats de backtest"
    )
    parser.add_argument(
        "--organize",
        action="store_true",
        help="Organiser les r√©sultats par cat√©gorie",
    )
    parser.add_argument(
        "--archive",
        action="store_true",
        help="Archiver les r√©sultats anciens",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Mode simulation (ne modifie rien)",
    )
    parser.add_argument(
        "--days",
        type=int,
        default=ARCHIVE_AFTER_DAYS,
        help=f"Seuil en jours pour archivage (d√©faut: {ARCHIVE_AFTER_DAYS})",
    )

    args = parser.parse_args()

    if args.organize:
        print("=== Organisation des r√©sultats ===\n")
        stats = organize_results(dry_run=args.dry_run)
        print(f"\n‚úÖ Organisation {'simul√©e' if args.dry_run else 'termin√©e'}")

    if args.archive:
        print("\n=== Archivage des r√©sultats anciens ===\n")
        count = archive_old_results(days_threshold=args.days, dry_run=args.dry_run)
        print(f"\nüì¶ {count} r√©sultats {'√† archiver' if args.dry_run else 'archiv√©s'}")

    if not args.organize and not args.archive:
        parser.print_help()