"""
Module-ID: ui.components.archive.validation_viewer

Purpose: Afficheur rapports walk-forward Phase 5.5 - visualiser r√©sultats validation anti-overfitting par fen√™tre.

Role in pipeline: visualization (archive)

Key components: ValidationStatus, render_validation_report(), fenetre details, m√©triques

Inputs: WalkForwardResult, metriques, overfitting ratio

Outputs: Interface Streamlit multi-onglets (overview, details, metrics)

Dependencies: streamlit, plotly, backtest.validation

Conventions: Composant optionnel archive

Read-if: Afficher rapports walk-forward d√©taill√©s.

Skip-if: Archive - utiliser composants actifs validation.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional
from enum import Enum

try:
    import streamlit as st
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False


class ValidationStatus(Enum):
    """Statut de validation."""
    PASSED = "passed"
    WARNING = "warning"
    FAILED = "failed"
    OVERFITTING = "overfitting"


@dataclass
class WindowResult:
    """R√©sultat d'une fen√™tre de validation."""
    window_id: int
    train_start: datetime
    train_end: datetime
    test_start: datetime
    test_end: datetime

    # M√©triques train
    train_sharpe: float
    train_return: float
    train_drawdown: float
    train_trades: int

    # M√©triques test
    test_sharpe: float
    test_return: float
    test_drawdown: float
    test_trades: int

    # Param√®tres optimaux
    params: Dict[str, Any] = field(default_factory=dict)

    @property
    def sharpe_degradation(self) -> float:
        """D√©gradation du Sharpe entre train et test."""
        if self.train_sharpe == 0:
            return 0.0
        return (self.train_sharpe - self.test_sharpe) / abs(self.train_sharpe)

    @property
    def return_degradation(self) -> float:
        """D√©gradation du return entre train et test."""
        if self.train_return == 0:
            return 0.0
        return (self.train_return - self.test_return) / abs(self.train_return)

    @property
    def is_overfitting(self) -> bool:
        """D√©tecte si cette fen√™tre montre de l'overfitting."""
        # Overfitting si d√©gradation > 50% ou test n√©gatif avec train positif
        if self.sharpe_degradation > 0.5:
            return True
        if self.train_sharpe > 0 and self.test_sharpe < 0:
            return True
        if self.train_return > 0 and self.test_return < 0:
            return True
        return False

    @property
    def status(self) -> ValidationStatus:
        """D√©termine le statut de validation."""
        if self.is_overfitting:
            return ValidationStatus.OVERFITTING
        if self.sharpe_degradation > 0.3:
            return ValidationStatus.WARNING
        if self.test_sharpe < 0:
            return ValidationStatus.FAILED
        return ValidationStatus.PASSED

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire."""
        return {
            "window_id": self.window_id,
            "train_period": f"{self.train_start.date()} ‚Üí {self.train_end.date()}",
            "test_period": f"{self.test_start.date()} ‚Üí {self.test_end.date()}",
            "train_sharpe": self.train_sharpe,
            "test_sharpe": self.test_sharpe,
            "train_return": self.train_return,
            "test_return": self.test_return,
            "train_drawdown": self.train_drawdown,
            "test_drawdown": self.test_drawdown,
            "sharpe_degradation": self.sharpe_degradation,
            "status": self.status.value,
            "params": self.params,
        }


@dataclass
class ValidationReport:
    """Rapport complet de validation Walk-Forward."""
    strategy_name: str
    created_at: datetime
    windows: List[WindowResult]

    # Configuration
    n_splits: int = 5
    train_ratio: float = 0.8
    purge_gap: int = 0

    # M√©triques globales (calcul√©es)
    _aggregate_metrics: Optional[Dict[str, float]] = field(default=None, repr=False)

    @property
    def aggregate_metrics(self) -> Dict[str, float]:
        """Calcule les m√©triques agr√©g√©es."""
        if self._aggregate_metrics is not None:
            return self._aggregate_metrics

        if not self.windows:
            return {}

        train_sharpes = [w.train_sharpe for w in self.windows]
        test_sharpes = [w.test_sharpe for w in self.windows]
        train_returns = [w.train_return for w in self.windows]
        test_returns = [w.test_return for w in self.windows]
        degradations = [w.sharpe_degradation for w in self.windows]

        import numpy as np

        self._aggregate_metrics = {
            "avg_train_sharpe": float(np.mean(train_sharpes)),
            "avg_test_sharpe": float(np.mean(test_sharpes)),
            "std_test_sharpe": float(np.std(test_sharpes)),
            "avg_train_return": float(np.mean(train_returns)),
            "avg_test_return": float(np.mean(test_returns)),
            "avg_degradation": float(np.mean(degradations)),
            "max_degradation": float(np.max(degradations)),
            "consistency_ratio": float(np.mean([1 if w.test_sharpe > 0 else 0 for w in self.windows])),
            "overfitting_windows": sum(1 for w in self.windows if w.is_overfitting),
        }

        return self._aggregate_metrics

    @property
    def overall_status(self) -> ValidationStatus:
        """Statut global de la validation."""
        metrics = self.aggregate_metrics

        # √âchec si trop de fen√™tres overfitting
        if metrics.get("overfitting_windows", 0) >= len(self.windows) // 2:
            return ValidationStatus.OVERFITTING

        # Warning si d√©gradation moyenne > 30%
        if metrics.get("avg_degradation", 0) > 0.3:
            return ValidationStatus.WARNING

        # √âchec si Sharpe test moyen n√©gatif
        if metrics.get("avg_test_sharpe", 0) < 0:
            return ValidationStatus.FAILED

        # Pass√© si consistance > 70%
        if metrics.get("consistency_ratio", 0) >= 0.7:
            return ValidationStatus.PASSED

        return ValidationStatus.WARNING

    @property
    def is_valid(self) -> bool:
        """La strat√©gie est-elle valid√©e?"""
        return self.overall_status == ValidationStatus.PASSED

    def get_best_params(self) -> Dict[str, Any]:
        """Retourne les param√®tres les plus robustes."""
        if not self.windows:
            return {}

        # Prendre les params de la fen√™tre avec le meilleur test_sharpe
        # tout en ayant une bonne consistance train/test
        valid_windows = [w for w in self.windows if not w.is_overfitting]

        if not valid_windows:
            valid_windows = self.windows

        best = max(valid_windows, key=lambda w: w.test_sharpe)
        return best.params

    def to_dict(self) -> Dict[str, Any]:
        """S√©rialise en dictionnaire."""
        return {
            "strategy_name": self.strategy_name,
            "created_at": self.created_at.isoformat(),
            "n_splits": self.n_splits,
            "train_ratio": self.train_ratio,
            "purge_gap": self.purge_gap,
            "overall_status": self.overall_status.value,
            "is_valid": self.is_valid,
            "aggregate_metrics": self.aggregate_metrics,
            "windows": [w.to_dict() for w in self.windows],
            "best_params": self.get_best_params(),
        }


# Couleurs par statut
STATUS_COLORS = {
    ValidationStatus.PASSED: "#4caf50",
    ValidationStatus.WARNING: "#ff9800",
    ValidationStatus.FAILED: "#f44336",
    ValidationStatus.OVERFITTING: "#9c27b0",
}

STATUS_ICONS = {
    ValidationStatus.PASSED: "‚úÖ",
    ValidationStatus.WARNING: "‚ö†Ô∏è",
    ValidationStatus.FAILED: "‚ùå",
    ValidationStatus.OVERFITTING: "üìà‚ùå",
}


def create_validation_figure(report: ValidationReport) -> go.Figure:
    """
    Cr√©e une figure Plotly du rapport de validation.

    Args:
        report: Rapport √† visualiser

    Returns:
        Figure Plotly
    """
    fig = make_subplots(
        rows=2,
        cols=2,
        subplot_titles=(
            "Sharpe Ratio: Train vs Test",
            "Return: Train vs Test",
            "D√©gradation par fen√™tre",
            "Statut par fen√™tre",
        ),
        vertical_spacing=0.15,
        horizontal_spacing=0.1,
    )

    window_ids = [w.window_id for w in report.windows]

    # Sharpe comparison
    fig.add_trace(
        go.Bar(
            x=window_ids,
            y=[w.train_sharpe for w in report.windows],
            name="Train Sharpe",
            marker_color="#2196f3",
            opacity=0.7,
        ),
        row=1,
        col=1,
    )
    fig.add_trace(
        go.Bar(
            x=window_ids,
            y=[w.test_sharpe for w in report.windows],
            name="Test Sharpe",
            marker_color="#4caf50",
            opacity=0.7,
        ),
        row=1,
        col=1,
    )

    # Return comparison
    fig.add_trace(
        go.Bar(
            x=window_ids,
            y=[w.train_return * 100 for w in report.windows],
            name="Train Return %",
            marker_color="#2196f3",
            opacity=0.7,
            showlegend=False,
        ),
        row=1,
        col=2,
    )
    fig.add_trace(
        go.Bar(
            x=window_ids,
            y=[w.test_return * 100 for w in report.windows],
            name="Test Return %",
            marker_color="#4caf50",
            opacity=0.7,
            showlegend=False,
        ),
        row=1,
        col=2,
    )

    # Degradation
    degradations = [w.sharpe_degradation * 100 for w in report.windows]
    colors = ["#f44336" if d > 50 else "#ff9800" if d > 30 else "#4caf50" for d in degradations]

    fig.add_trace(
        go.Bar(
            x=window_ids,
            y=degradations,
            name="D√©gradation %",
            marker_color=colors,
        ),
        row=2,
        col=1,
    )

    # Ligne seuil 30%
    fig.add_hline(y=30, line_dash="dash", line_color="orange", row=2, col=1)
    fig.add_hline(y=50, line_dash="dash", line_color="red", row=2, col=1)

    # Status indicators
    status_values = []
    status_colors = []
    for w in report.windows:
        status = w.status
        status_values.append(list(ValidationStatus).index(status))
        status_colors.append(STATUS_COLORS[status])

    fig.add_trace(
        go.Bar(
            x=window_ids,
            y=[1] * len(window_ids),
            name="Statut",
            marker_color=status_colors,
            text=[STATUS_ICONS[w.status] for w in report.windows],
            textposition="inside",
        ),
        row=2,
        col=2,
    )

    # Styling
    fig.update_layout(
        height=600,
        template="plotly_dark",
        barmode="group",
        showlegend=True,
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,
        ),
    )

    fig.update_xaxes(title_text="Fen√™tre", row=2, col=1)
    fig.update_xaxes(title_text="Fen√™tre", row=2, col=2)
    fig.update_yaxes(title_text="Sharpe Ratio", row=1, col=1)
    fig.update_yaxes(title_text="Return (%)", row=1, col=2)
    fig.update_yaxes(title_text="D√©gradation (%)", row=2, col=1)

    return fig


def render_validation_report(
    report: ValidationReport,
    key: str = "validation_report",
) -> None:
    """
    Rendu Streamlit du rapport de validation.

    Args:
        report: Rapport √† afficher
        key: Cl√© unique pour les widgets
    """
    if not STREAMLIT_AVAILABLE:
        return

    # En-t√™te avec statut global
    status = report.overall_status
    status_color = STATUS_COLORS[status]
    status_icon = STATUS_ICONS[status]

    st.markdown(
        f"## {status_icon} Rapport de Validation - {report.strategy_name}"
    )

    # Badge statut
    st.markdown(
        f"<span style='background-color:{status_color};color:white;padding:5px 15px;"
        f"border-radius:15px;font-weight:bold'>{status.value.upper()}</span>",
        unsafe_allow_html=True,
    )

    st.caption(f"Cr√©√© le {report.created_at.strftime('%d/%m/%Y %H:%M')}")

    # M√©triques r√©sum√©
    st.markdown("### üìä M√©triques Globales")

    metrics = report.aggregate_metrics

    col1, col2, col3, col4 = st.columns(4)

    col1.metric(
        "Sharpe Train (moy)",
        f"{metrics.get('avg_train_sharpe', 0):.3f}",
    )
    col2.metric(
        "Sharpe Test (moy)",
        f"{metrics.get('avg_test_sharpe', 0):.3f}",
        delta=f"{-metrics.get('avg_degradation', 0):.1%}",
        delta_color="inverse",
    )
    col3.metric(
        "Consistance",
        f"{metrics.get('consistency_ratio', 0):.0%}",
    )
    col4.metric(
        "Fen√™tres Overfitting",
        f"{metrics.get('overfitting_windows', 0)}/{len(report.windows)}",
    )

    col1, col2, col3, col4 = st.columns(4)

    col1.metric(
        "Return Train (moy)",
        f"{metrics.get('avg_train_return', 0):.2%}",
    )
    col2.metric(
        "Return Test (moy)",
        f"{metrics.get('avg_test_return', 0):.2%}",
    )
    col3.metric(
        "D√©gradation Max",
        f"{metrics.get('max_degradation', 0):.1%}",
    )
    col4.metric(
        "√âcart-type Test",
        f"{metrics.get('std_test_sharpe', 0):.3f}",
    )

    # Graphique
    st.markdown("### üìà Visualisation")
    fig = create_validation_figure(report)
    st.plotly_chart(fig, width='stretch', key=f"{key}_chart")

    # D√©tails par fen√™tre
    st.markdown("### üîç D√©tails par Fen√™tre")

    # Tableau r√©capitulatif
    data = []
    for w in report.windows:
        data.append({
            "Fen√™tre": w.window_id,
            "Train": f"{w.train_start.date()} ‚Üí {w.train_end.date()}",
            "Test": f"{w.test_start.date()} ‚Üí {w.test_end.date()}",
            "Sharpe (T)": f"{w.train_sharpe:.3f}",
            "Sharpe (V)": f"{w.test_sharpe:.3f}",
            "D√©gr.": f"{w.sharpe_degradation:.1%}",
            "Statut": f"{STATUS_ICONS[w.status]} {w.status.value}",
        })

    st.dataframe(data, width='stretch')

    # D√©tails expandables
    for w in report.windows:
        status_icon = STATUS_ICONS[w.status]
        with st.expander(f"Fen√™tre {w.window_id} {status_icon}", expanded=False):
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("**üìà Train**")
                st.write(f"P√©riode: {w.train_start.date()} ‚Üí {w.train_end.date()}")
                st.write(f"Sharpe: {w.train_sharpe:.3f}")
                st.write(f"Return: {w.train_return:.2%}")
                st.write(f"Drawdown: {w.train_drawdown:.2%}")
                st.write(f"Trades: {w.train_trades}")

            with col2:
                st.markdown("**üß™ Test**")
                st.write(f"P√©riode: {w.test_start.date()} ‚Üí {w.test_end.date()}")
                st.write(f"Sharpe: {w.test_sharpe:.3f}")
                st.write(f"Return: {w.test_return:.2%}")
                st.write(f"Drawdown: {w.test_drawdown:.2%}")
                st.write(f"Trades: {w.test_trades}")

            if w.params:
                st.markdown("**üîß Param√®tres optimaux**")
                st.json(w.params)

    # Recommandation
    st.markdown("### üí° Recommandation")

    if report.overall_status == ValidationStatus.PASSED:
        st.success(
            "‚úÖ **Strat√©gie valid√©e** - Les performances sont consistantes entre "
            "l'entra√Ænement et le test. La strat√©gie peut √™tre utilis√©e en production."
        )
        best_params = report.get_best_params()
        if best_params:
            st.markdown("**Param√®tres recommand√©s:**")
            st.json(best_params)

    elif report.overall_status == ValidationStatus.WARNING:
        st.warning(
            "‚ö†Ô∏è **Attention** - D√©gradation significative entre train et test. "
            "Consid√©rez d'ajuster les param√®tres ou de r√©duire la complexit√©."
        )

    elif report.overall_status == ValidationStatus.OVERFITTING:
        st.error(
            "üìà‚ùå **Overfitting d√©tect√©** - Les performances train ne se g√©n√©ralisent pas "
            "sur les donn√©es de test. Simplifiez la strat√©gie ou utilisez moins de param√®tres."
        )

    else:
        st.error(
            "‚ùå **√âchec de validation** - La strat√©gie ne performe pas de mani√®re "
            "satisfaisante sur les donn√©es de test."
        )

    # Export
    with st.expander("üì• Export"):
        import json
        report_json = json.dumps(report.to_dict(), indent=2, default=str)

        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                "T√©l√©charger JSON",
                report_json,
                f"validation_{report.strategy_name}.json",
                "application/json",
                key=f"{key}_download",
            )
        with col2:
            if st.button("Afficher JSON", key=f"{key}_show_json"):
                st.code(report_json, language="json")


def render_validation_summary_card(
    report: ValidationReport,
    key: str = "validation_card",
) -> None:
    """
    Carte r√©sum√© compacte pour le dashboard.

    Args:
        report: Rapport √† afficher
        key: Cl√© unique
    """
    if not STREAMLIT_AVAILABLE:
        return

    status = report.overall_status
    status_color = STATUS_COLORS[status]
    status_icon = STATUS_ICONS[status]
    metrics = report.aggregate_metrics

    with st.container():
        st.markdown(
            f"<div style='border-left: 4px solid {status_color}; padding-left: 10px;'>"
            f"<strong>{status_icon} {report.strategy_name}</strong><br/>"
            f"<small>Sharpe Test: {metrics.get('avg_test_sharpe', 0):.3f} | "
            f"Consistance: {metrics.get('consistency_ratio', 0):.0%}</small>"
            f"</div>",
            unsafe_allow_html=True,
        )


def create_sample_report() -> ValidationReport:
    """Cr√©e un rapport exemple pour les tests."""
    from datetime import timedelta
    import random

    windows = []
    base_date = datetime(2024, 1, 1)

    for i in range(5):
        train_start = base_date + timedelta(days=i * 60)
        train_end = train_start + timedelta(days=180)
        test_start = train_end + timedelta(days=1)
        test_end = test_start + timedelta(days=45)

        train_sharpe = random.uniform(0.8, 2.5)
        degradation = random.uniform(0.1, 0.6)

        windows.append(WindowResult(
            window_id=i + 1,
            train_start=train_start,
            train_end=train_end,
            test_start=test_start,
            test_end=test_end,
            train_sharpe=train_sharpe,
            train_return=random.uniform(0.05, 0.25),
            train_drawdown=random.uniform(0.05, 0.15),
            train_trades=random.randint(50, 150),
            test_sharpe=train_sharpe * (1 - degradation),
            test_return=random.uniform(-0.02, 0.15),
            test_drawdown=random.uniform(0.05, 0.20),
            test_trades=random.randint(10, 40),
            params={"fast_period": 10 + i, "slow_period": 20 + i * 2},
        ))

    return ValidationReport(
        strategy_name="ema_cross",
        created_at=datetime.now(),
        windows=windows,
        n_splits=5,
        train_ratio=0.8,
    )
