"""
Backtest Core - LLM Model Selector
===================================

Helpers pour connecter l'interface Streamlit aux mod√®les Ollama.

Features:
- R√©cup√®re dynamiquement la liste des mod√®les disponibles via Ollama
- Fournit un fallback coh√©rent quand Ollama n'est pas accessible
- Centralise la logique pour que toutes les pages utilisent la m√™me source

Usage:
    >>> from ui.components.model_selector import get_available_models_for_ui
    >>>
    >>> # Dans Streamlit
    >>> models = get_available_models_for_ui()
    >>> selected_model = st.selectbox("Mod√®le LLM", models)
"""

from __future__ import annotations

from typing import Iterable, List, Sequence

from agents.ollama_manager import list_ollama_models
from utils.log import get_logger

logger = get_logger(__name__)

# Liste de mod√®les recommand√©s utilis√©e comme fallback
FALLBACK_LLM_MODELS: List[str] = [
    # Mod√®les recommand√©s pour trading (Dec 2025)
    "deepseek-r1:70b",              # 34 GB - Le plus puissant
    "deepseek-r1:32b",              # 19 GB - Excellent rapport qualit√©/prix
    "qwq:32b",                      # 23 GB - Tr√®s bon pour raisonnement
    "qwen2.5:32b",                  # 19 GB - Alternative solide
    "mistral:22b",                  # 13 GB - √âquilibr√©
    "gemma3:27b",                   # 17 GB - Bon pour analyse
    "deepseek-r1-distill:14b",      # 9 GB - √âquilibr√©
    "gemma3:12b",                   # 8 GB - Rapide
    "deepseek-r1:8b",               # 5 GB - Builder rapide
    "mistral:7b-instruct",          # 4 GB - Ultra rapide
    "llama3.2",                     # 2 GB - L√©ger
    "phi3",                         # 2 GB - L√©ger alternatif
]

# Mod√®les recommand√©s pour diff√©rentes t√¢ches
RECOMMENDED_FOR_ANALYSIS = ["deepseek-r1:32b", "qwq:32b", "qwen2.5:32b"]
RECOMMENDED_FOR_STRATEGY = ["deepseek-r1:70b", "deepseek-r1:32b", "qwq:32b"]
RECOMMENDED_FOR_CRITICISM = ["mistral:22b", "gemma3:27b", "qwen2.5:32b"]
RECOMMENDED_FOR_FAST = ["deepseek-r1:8b", "mistral:7b-instruct", "llama3.2"]


def _sort_with_preferred(
    models: Iterable[str], preferred_order: Sequence[str]
) -> List[str]:
    """
    Trie les mod√®les en mettant ceux de preferred_order en premier.

    Args:
        models: Liste des mod√®les √† trier
        preferred_order: Ordre pr√©f√©r√©

    Returns:
        Liste tri√©e de mod√®les
    """
    preferred_index = {name: i for i, name in enumerate(preferred_order)}

    def sort_key(name: str) -> tuple[int, int | str]:
        if name in preferred_index:
            return 0, preferred_index[name]
        return 1, name

    unique = sorted(set(models), key=sort_key)
    return unique


def get_available_models_for_ui(
    preferred_order: Sequence[str] | None = None,
    fallback: Sequence[str] | None = None,
) -> List[str]:
    """
    Retourne la liste des mod√®les LLM √† proposer dans l'UI.

    - Si Ollama r√©pond, on retourne exactement les mod√®les install√©s.
    - Si Ollama est inaccessible ou ne retourne rien, on utilise une liste fallback.

    Args:
        preferred_order: Ordre conseill√© pour le tri (facultatif).
        fallback: Fallback explicite (sinon FALLBACK_LLM_MODELS).

    Returns:
        list[str]: Noms de mod√®les Ollama.

    Example:
        >>> models = get_available_models_for_ui(
        ...     preferred_order=RECOMMENDED_FOR_STRATEGY
        ... )
        >>> st.selectbox("Mod√®le", models)
    """
    installed = list_ollama_models()

    if installed:
        if preferred_order:
            return _sort_with_preferred(installed, preferred_order)
        # Par d√©faut: tri alphab√©tique des mod√®les install√©s
        return sorted(set(installed))

    # Ollama inaccessible ou aucun mod√®le retourn√©: fallback
    models = list(fallback or FALLBACK_LLM_MODELS)
    logger.warning(
        "‚ö†Ô∏è Ollama ne renvoie aucun mod√®le, utilisation du fallback UI: %s",
        models[:3]
    )
    return models


def get_model_info(model_name: str) -> dict:
    """
    Retourne des informations sur un mod√®le.

    Args:
        model_name: Nom du mod√®le

    Returns:
        dict: Informations du mod√®le (size, description, etc.)
    """
    # Mapping des tailles approximatives (GB)
    model_sizes = {
        "deepseek-r1:70b": 34,
        "deepseek-r1:32b": 19,
        "qwq:32b": 23,
        "qwen2.5:32b": 19,
        "mistral:22b": 13,
        "gemma3:27b": 17,
        "deepseek-r1-distill:14b": 9,
        "gemma3:12b": 8,
        "deepseek-r1:8b": 5,
        "mistral:7b-instruct": 4,
        "llama3.2": 2,
        "phi3": 2,
    }

    # Descriptions
    model_descriptions = {
        "deepseek-r1:70b": "Le plus puissant - Excellent pour strat√©gies complexes",
        "deepseek-r1:32b": "Optimal - Meilleur rapport qualit√©/prix",
        "qwq:32b": "Excellent raisonnement - Id√©al pour analyse",
        "qwen2.5:32b": "Alternative solide - Polyvalent",
        "mistral:22b": "√âquilibr√© - Bon pour critique",
        "gemma3:27b": "Bon pour analyse - Rapide",
        "deepseek-r1-distill:14b": "√âquilibr√© - Compact",
        "gemma3:12b": "Rapide - L√©ger",
        "deepseek-r1:8b": "Ultra rapide - Tests rapides",
        "mistral:7b-instruct": "Ultra rapide - Tr√®s l√©ger",
        "llama3.2": "L√©ger - Pour tests",
        "phi3": "L√©ger - Pour tests",
    }

    return {
        "name": model_name,
        "size_gb": model_sizes.get(model_name, "?"),
        "description": model_descriptions.get(model_name, "Mod√®le LLM"),
    }


def render_model_selector(
    label: str = "Mod√®le LLM",
    key: str = "llm_model",
    preferred_order: Sequence[str] | None = None,
    help_text: str | None = None,
) -> str:
    """
    Rendu d'un s√©lecteur de mod√®le Streamlit avec informations.

    Args:
        label: Label du selectbox
        key: Cl√© du state Streamlit
        preferred_order: Ordre pr√©f√©r√© des mod√®les
        help_text: Texte d'aide optionnel

    Returns:
        str: Nom du mod√®le s√©lectionn√©

    Example:
        >>> import streamlit as st
        >>> model = render_model_selector(
        ...     label="Mod√®le Analyst",
        ...     key="analyst_model",
        ...     preferred_order=RECOMMENDED_FOR_ANALYSIS
        ... )
    """
    import streamlit as st

    models = get_available_models_for_ui(preferred_order=preferred_order)

    if not help_text:
        help_text = "S√©lectionnez un mod√®le LLM Ollama pour l'optimisation"

    selected = st.selectbox(
        label,
        models,
        key=key,
        help=help_text,
    )

    # Afficher les infos du mod√®le s√©lectionn√©
    if selected:
        info = get_model_info(selected)
        size = info["size_gb"]
        desc = info["description"]
        st.caption(f"üì¶ ~{size} GB | {desc}")

    return selected


__all__ = [
    "FALLBACK_LLM_MODELS",
    "RECOMMENDED_FOR_ANALYSIS",
    "RECOMMENDED_FOR_STRATEGY",
    "RECOMMENDED_FOR_CRITICISM",
    "RECOMMENDED_FOR_FAST",
    "get_available_models_for_ui",
    "get_model_info",
    "render_model_selector",
]
