"""
UI Streamlit principale pour le moteur de backtest.

PROTECTION WINDOWS SPAWN:
Ce module cr√©e des ProcessPoolExecutor pour les sweeps grille.
Sous Windows, multiprocessing utilise 'spawn' qui r√©-ex√©cute le module.
Les workers IMPORTENT ce fichier mais NE DOIVENT PAS ex√©cuter Streamlit.
Protection: Tout code Streamlit est dans main() appel√© uniquement par __main__.
"""
from __future__ import annotations

# pylint: disable=import-outside-toplevel,too-many-lines

# ============================================================================
# D√âSACTIVATION GPU POUR SWEEPS STREAMLIT
# ============================================================================
# DOIT √™tre au tout d√©but AVANT tout import pour √©viter chargement VRAM inutile
# GPU queue ne fonctionne pas en multiprocess ‚Üí CPU + cache RAM plus efficace
import os
os.environ["BACKTEST_USE_GPU"] = "0"
os.environ["BACKTEST_GPU_QUEUE_ENABLED"] = "0"
# ============================================================================

import gc
import logging
import math
import os
import time
import traceback
from collections import deque
from itertools import chain, islice, product
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
import streamlit as st

from ui.constants import PARAM_CONSTRAINTS
from ui.context import (
    LLM_AVAILABLE,
    LLM_IMPORT_ERROR,
    OrchestrationLogger,
    BacktestEngine,
    compute_search_space_stats,
    create_llm_client,
    create_optimizer_from_engine,
    create_orchestrator_with_backtest,
    get_strategy_param_bounds,
    get_strategy_param_space,
    generate_session_id,
    render_deep_trace_viewer,
    render_full_orchestration_viewer,
    LiveOrchestrationViewer,
)
from ui.helpers import (
    ProgressMonitor,
    build_strategy_params_for_comparison,
    render_progress_monitor,
    safe_load_data,
    load_selected_data,
    safe_run_backtest,
    safe_copy_cleanup,
    show_status,
    summarize_comparison_results,
    validate_all_params,
    _maybe_auto_save_run,
)
from ui.cache_manager import clear_data_cache
from ui.state import SidebarState
from ui.components.charts import (
    render_comparison_chart,
    render_multi_sweep_heatmap,
    render_multi_sweep_ranking,
    render_strategy_param_diagram,
    render_ohlcv_with_trades_and_indicators,
)
from ui.components.sweep_monitor import (
    SweepMonitor,
    render_sweep_progress,
    render_sweep_summary,
)
from ui.helpers import build_indicator_overlays
from utils.run_tracker import RunSignature, get_global_tracker

# Import du worker isol√© pour √©viter les probl√®mes de pickling avec hot-reload Streamlit
from backtest.worker import run_backtest_worker as _isolated_worker


# Fonction _run_backtest_multiprocess SUPPRIM√âE (obsol√®te)
# Utilisez run_backtest_worker de backtest.worker √† la place
# Voir commit pour restauration si n√©cessaire

def _apply_thread_limit(thread_limit: int, label: str = "") -> None:
    if thread_limit <= 0:
        return

    os.environ["BACKTEST_WORKER_THREADS"] = str(thread_limit)
    for var in (
        "OMP_NUM_THREADS",
        "MKL_NUM_THREADS",
        "OPENBLAS_NUM_THREADS",
        "NUMEXPR_NUM_THREADS",
        "VECLIB_MAXIMUM_THREADS",
        "BLIS_NUM_THREADS",
    ):
        os.environ[var] = str(thread_limit)

    try:
        from threadpoolctl import threadpool_limits

        threadpool_limits(thread_limit)
    except Exception:
        pass

    try:
        import torch

        torch.set_num_threads(thread_limit)
        torch.set_num_interop_threads(max(1, thread_limit // 2))
    except Exception:
        pass

    if label:
        logger = logging.getLogger(__name__)
        logger.info("Thread limit %s appliqu√©: %s", label, thread_limit)


def _init_sweep_worker(thread_limit: int) -> None:
    """Initializer ProcessPoolExecutor - applique limites threads AVANT tout calcul."""
    _apply_thread_limit(thread_limit, label="worker")

    # Forcer avec threadpoolctl (plus efficace que les env vars seules)
    try:
        import threadpoolctl
        info_before = threadpoolctl.threadpool_info()
        threadpoolctl.threadpool_limits(limits=max(1, thread_limit), user_api="blas")
        info_after = threadpoolctl.threadpool_info()

        # Log pour debug
        import logging
        logger = logging.getLogger(__name__)
        num_threads_before = sum(pool.get("num_threads", 0) for pool in info_before)
        num_threads_after = sum(pool.get("num_threads", 0) for pool in info_after)
        logger.debug(f"Worker threads BLAS: {num_threads_before} ‚Üí {num_threads_after}")
    except ImportError:
        pass  # threadpoolctl non install√© - les env vars suffiront


def _timeframe_to_minutes(timeframe: str) -> int:
    """Convertit un timeframe en minutes pour tri/estimation."""
    if not timeframe or len(timeframe) < 2:
        return 0
    unit = timeframe[-1]
    try:
        amount = int(timeframe[:-1])
    except ValueError:
        return 0
    multipliers = {"m": 1, "h": 60, "d": 1440, "w": 10080, "M": 43200}
    return amount * multipliers.get(unit, 60)


def _build_multi_sweep_plan(symbols: List[str], timeframes: List[str]) -> List[tuple[str, str]]:
    """Construit un plan multi-sweep avec un ordre stable et l√©ger."""
    combos = [(symbol, tf) for symbol in symbols for tf in timeframes]
    combos.sort(key=lambda item: (_timeframe_to_minutes(item[1]), item[0]), reverse=True)
    return combos


def _estimate_grid_size(param_ranges: Dict[str, Any]) -> int:
    """Estime le nombre de combinaisons de la grille sans la mat√©rialiser."""
    if not param_ranges:
        return 1
    total = 1
    for r in param_ranges.values():
        try:
            pmin, pmax, step = r["min"], r["max"], r["step"]
            if isinstance(pmin, int) and isinstance(step, int):
                count = max(1, ((int(pmax) - int(pmin)) // max(1, int(step))) + 1)
            else:
                if float(step) <= 0:
                    count = 1
                else:
                    span = float(pmax) - float(pmin)
                    count = int(math.floor(span / float(step))) + 1
            total *= max(1, int(count))
        except Exception:
            total *= 1
    return max(1, int(total))


def _compute_max_safe_combos(total_sweeps: int, max_combos: int) -> int:
    """Limite adaptative pour multi-sweep (m√©moire)."""
    if total_sweeps <= 0:
        return max_combos
    adaptive = max(50_000, 500_000 // max(1, total_sweeps))
    if max_combos and max_combos > 0:
        return min(max_combos, adaptive)
    return adaptive


def _build_param_combo_iter(
    params: Dict[str, Any],
    param_ranges: Dict[str, Any],
    max_runs: Optional[int],
) -> tuple[Any, int, int]:
    """Construit un it√©rateur lazy de combinaisons + stats."""
    param_names = list(param_ranges.keys())
    param_values_lists = []

    if param_names:
        for pname in param_names:
            r = param_ranges[pname]
            pmin, pmax, step = r["min"], r["max"], r["step"]

            if isinstance(pmin, int) and isinstance(step, int):
                values = list(range(int(pmin), int(pmax) + 1, max(1, int(step))))
            else:
                values = list(
                    np.arange(float(pmin), float(pmax) + float(step) / 2, float(step))
                )
                values = [round(v, 2) for v in values if v <= pmax]

            if not values:
                values = [pmin]

            param_values_lists.append(values)

        total_combinations = max(
            1, math.prod(len(values) for values in param_values_lists)
        )
        combo_iter = (
            {**params, **dict(zip(param_names, combo))}
            for combo in product(*param_values_lists)
        )
    else:
        total_combinations = 1
        combo_iter = iter([params.copy()])

    if max_runs and max_runs > 0 and total_combinations > max_runs:
        combo_iter = islice(combo_iter, max_runs)
        total_runs = max_runs
    else:
        total_runs = total_combinations

    return combo_iter, total_runs, total_combinations


def _run_grid_sequential(
    df: pd.DataFrame,
    engine: BacktestEngine,
    strategy_key: str,
    symbol: str,
    timeframe: str,
    params: Dict[str, Any],
    param_ranges: Dict[str, Any],
    max_runs: Optional[int],
    debug_enabled: bool,
    progress_placeholder: Any,
) -> Dict[str, Any]:
    """Ex√©cute une grille s√©quentielle et retourne le meilleur r√©sultat."""
    combo_iter, total_runs, total_combinations = _build_param_combo_iter(
        params=params,
        param_ranges=param_ranges,
        max_runs=max_runs,
    )

    best_params: Dict[str, Any] = {}
    best_metrics: Dict[str, Any] = {}
    best_score = (float("-inf"), float("-inf"))
    completed = 0
    failed = 0
    last_render = time.perf_counter()

    fast_metrics = False
    try:
        fast_threshold = int(os.getenv("BACKTEST_SWEEP_FAST_METRICS_THRESHOLD", "500"))
        fast_metrics = total_runs >= fast_threshold
    except (TypeError, ValueError):
        fast_metrics = False

    for param_combo in combo_iter:
        if st.session_state.get("stop_requested", False):
            break

        completed += 1
        result, msg = safe_run_backtest(
            engine,
            df,
            strategy_key,
            param_combo,
            symbol,
            timeframe,
            silent_mode=not debug_enabled,
            fast_metrics=fast_metrics,
        )

        if result is None:
            failed += 1
        else:
            metrics = result.metrics or {}
            sharpe = metrics.get("sharpe_ratio", float("-inf"))
            pnl = metrics.get("total_pnl", float("-inf"))
            score = (sharpe, pnl)
            if score > best_score:
                best_score = score
                best_params = param_combo
                best_metrics = metrics

        now = time.perf_counter()
        if completed == 1 or completed % 200 == 0 or now - last_render >= 5.0:
            progress_placeholder.caption(
                f"Grille en cours: {completed}/{total_runs} (max {total_combinations:,})"
            )
            last_render = now

    return {
        "best_params": best_params,
        "best_metrics": best_metrics,
        "completed": completed,
        "failed": failed,
        "total_runs": total_runs,
        "total_combinations": total_combinations,
    }


def _run_grid_parallel_basic(
    df: pd.DataFrame,
    strategy_key: str,
    symbol: str,
    timeframe: str,
    params: Dict[str, Any],
    param_ranges: Dict[str, Any],
    max_runs: Optional[int],
    initial_capital: float,
    n_workers: int,
    worker_thread_limit: int,
    debug_enabled: bool,
    progress_placeholder: Any,
    stats_placeholder: Any,
) -> Dict[str, Any]:
    """Ex√©cute une grille en parall√®le (pool par sweep) avec progress live."""
    from concurrent.futures import FIRST_COMPLETED, ProcessPoolExecutor, wait
    from backtest.worker import init_worker_with_dataframe

    combo_iter, total_runs, total_combinations = _build_param_combo_iter(
        params=params,
        param_ranges=param_ranges,
        max_runs=max_runs,
    )

    try:
        fast_threshold = int(os.getenv("BACKTEST_SWEEP_FAST_METRICS_THRESHOLD", "500"))
    except (TypeError, ValueError):
        fast_threshold = 500
    fast_metrics = total_runs >= fast_threshold

    monitor = ProgressMonitor(total_runs=total_runs)
    best_params: Dict[str, Any] = {}
    best_metrics: Dict[str, Any] = {}
    best_score = (float("-inf"), float("-inf"))

    completed = 0
    failed = 0
    last_render = time.perf_counter()

    max_inflight = max(1, min(total_runs, n_workers * 2))
    pending: Dict[Any, Dict[str, Any]] = {}

    def submit_next(executor: ProcessPoolExecutor) -> bool:
        try:
            param_combo = next(combo_iter)
        except StopIteration:
            return False
        future = executor.submit(_isolated_worker, param_combo)
        pending[future] = param_combo
        return True

    with ProcessPoolExecutor(
        max_workers=n_workers,
        initializer=init_worker_with_dataframe,
        initargs=(
            df,
            strategy_key,
            symbol,
            timeframe,
            initial_capital,
            debug_enabled,
            worker_thread_limit,
            fast_metrics,
            False,  # ‚úÖ CRITIQUE: is_path (DataFrame fourni directement, pas un chemin)
        ),
    ) as executor:
        for _ in range(max_inflight):
            if not submit_next(executor):
                break

        while pending:
            if st.session_state.get("stop_requested", False):
                break

            # ‚úÖ FIX #12: R√©duire timeout de 0.25s √† 0.05s
            done, _ = wait(pending, timeout=0.05, return_when=FIRST_COMPLETED)
            if not done:
                continue

            for future in done:
                param_combo = pending.pop(future)
                result = None
                try:
                    result = future.result(timeout=300)
                except Exception as exc:
                    result = {"params_dict": param_combo, "error": f"{type(exc).__name__}: {exc}"}

                completed += 1
                monitor.runs_completed = completed

                if result and "error" not in result:
                    metrics = {
                        "total_pnl": result.get("total_pnl", 0.0),
                        "sharpe_ratio": result.get("sharpe", 0.0),
                        "max_drawdown": result.get("max_dd", 0.0),
                        "win_rate": result.get("win_rate", 0.0),
                        "profit_factor": result.get("profit_factor", 0.0),
                    }
                    score = (metrics.get("sharpe_ratio", float("-inf")),
                             metrics.get("total_pnl", float("-inf")))
                    if score > best_score:
                        best_score = score
                        best_params = param_combo
                        best_metrics = metrics
                else:
                    failed += 1

                submit_next(executor)

                now = time.perf_counter()
                if completed == 1 or completed % 200 == 0 or now - last_render >= 2.0:
                    render_progress_monitor(monitor, progress_placeholder)
                    if best_metrics:
                        stats_placeholder.caption(
                            f"‚ö° {completed}/{total_runs} | "
                            f"Sharpe {best_metrics.get('sharpe_ratio', 0):.2f} | "
                            f"PnL ${best_metrics.get('total_pnl', 0):,.2f}"
                        )
                    last_render = now

    return {
        "best_params": best_params,
        "best_metrics": best_metrics,
        "completed": completed,
        "failed": failed,
        "total_runs": total_runs,
        "total_combinations": total_combinations,
    }


def render_controls() -> tuple[bool, Any]:
    st.title("üìà Backtest Core - Moteur Simplifi√©")

    status_container = st.container()

    st.markdown(
        """
Interface avec validation des param√®tres et feedback utilisateur.
Le syst√®me de granularit√© limite le nombre de valeurs testables.
"""
    )

    if "is_running" not in st.session_state:
        st.session_state.is_running = False
    if "stop_requested" not in st.session_state:
        st.session_state.stop_requested = False

    st.markdown("---")
    stop_button = False
    if st.session_state.is_running:
        stop_button = st.button(
            "‚õî Arr√™t d'urgence",
            type="secondary",
            use_container_width=True,
            key="btn_stop_backtest",
        )

    if stop_button:
        st.session_state.stop_requested = True
        st.session_state.is_running = False

        gc.collect()

        try:
            import torch

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                st.success("‚úÖ VRAM GPU vid√©e")
        except ImportError:
            pass

        logger = logging.getLogger(__name__)
        safe_copy_cleanup(logger)

        st.success("‚úÖ RAM syst√®me vid√©e")
        st.info("üí° Syst√®me pr√™t pour un nouveau test")
        st.session_state.stop_requested = False
        st.rerun()

    st.markdown("---")

    run_requested = bool(st.session_state.get("run_backtest_requested", False))
    if run_requested:
        st.session_state.run_backtest_requested = False

    return run_requested, status_container


def render_setup_previews(state: SidebarState) -> None:
    st.markdown("---")
    st.subheader("Schema indicateurs & parametres")
    if state.strategy_instance is None:
        st.info("Selectionnez une strategie pour afficher le schema.")
    else:
        diagram_params = {
            **state.strategy_instance.default_params,
            **state.params,
        }
        render_strategy_param_diagram(
            state.strategy_key,
            diagram_params,
            key=f"diagram_{state.strategy_key}",
        )

    st.markdown("---")
    st.subheader("Apercu OHLCV + indicateurs")
    preview_df = st.session_state.get("ohlcv_df")
    if preview_df is None:
        st.info("Chargez les donnees pour afficher l'apercu.")
    else:
        preview_overlays = build_indicator_overlays(
            state.strategy_key,
            preview_df,
            state.params,
        )
        render_ohlcv_with_trades_and_indicators(
            df=preview_df,
            trades_df=pd.DataFrame(),
            overlays=preview_overlays,
            active_indicators=state.active_indicators,
            title="OHLCV + indicateurs (apercu)",
            key="ohlcv_indicator_preview",
            height=650,
        )


def render_main(
    state: SidebarState,
    run_button: bool,
    status_container: Any,
) -> None:
    result = st.session_state.get("last_run_result")
    winner_params = st.session_state.get("last_winner_params")
    winner_metrics = st.session_state.get("last_winner_metrics")
    winner_origin = st.session_state.get("last_winner_origin")
    winner_meta = st.session_state.get("last_winner_meta")

    params = state.params
    param_ranges = state.param_ranges
    strategy_key = state.strategy_key
    symbol = state.symbol
    timeframe = state.timeframe
    optimization_mode = state.optimization_mode
    debug_enabled = state.debug_enabled
    max_combos = state.max_combos
    n_workers = state.n_workers

    llm_config = state.llm_config
    llm_model = state.llm_model
    llm_use_multi_agent = state.llm_use_multi_agent
    llm_max_iterations = state.llm_max_iterations
    llm_use_walk_forward = state.llm_use_walk_forward
    llm_unload_during_backtest = state.llm_unload_during_backtest
    llm_compare_enabled = state.llm_compare_enabled
    llm_compare_auto_run = state.llm_compare_auto_run
    llm_compare_strategies = state.llm_compare_strategies
    llm_compare_tokens = state.llm_compare_tokens
    llm_compare_timeframes = state.llm_compare_timeframes
    llm_compare_metric = state.llm_compare_metric
    llm_compare_aggregate = state.llm_compare_aggregate
    llm_compare_max_runs = state.llm_compare_max_runs
    llm_compare_use_preset = state.llm_compare_use_preset
    llm_compare_generate_report = state.llm_compare_generate_report

    use_gpu_indicators = bool(st.session_state.get("use_gpu_indicators", False))
    gpu_workers_override = bool(st.session_state.get("gpu_workers_override", False))

    def _resolve_workers(default_workers: int) -> int:
        if use_gpu_indicators and gpu_workers_override:
            try:
                return max(1, int(st.session_state.get("gpu_n_workers", default_workers)))
            except (TypeError, ValueError):
                return max(1, int(default_workers)) if default_workers else 1
        try:
            return max(1, int(default_workers))
        except (TypeError, ValueError):
            return 1

    def _resolve_threads(default_threads: int) -> int:
        if use_gpu_indicators and gpu_workers_override:
            try:
                return max(1, int(st.session_state.get("gpu_worker_threads", default_threads)))
            except (TypeError, ValueError):
                return max(1, int(default_threads)) if default_threads else 1
        try:
            return max(1, int(default_threads))
        except (TypeError, ValueError):
            return 1

    def _format_combo_limit(value: int) -> str:
        return "illimit√©e" if value >= 1_000_000_000_000 else f"{value:,}"

    if run_button:
        st.session_state.is_running = True
        st.session_state.stop_requested = False
        winner_params = None
        winner_metrics = None
        winner_origin = None
        winner_meta = None

        is_valid, errors = validate_all_params(params)

        if not is_valid:
            with status_container:
                show_status("error", "Param√®tres invalides")
                for err in errors:
                    st.error(f"  ‚Ä¢ {err}")
            st.session_state.is_running = False
            st.stop()

        is_multi_sweep = (len(state.symbols) > 1 or len(state.timeframes) > 1)
        if is_multi_sweep and optimization_mode in ("Backtest Simple", "Grille de Param√®tres"):
            sweep_plan = _build_multi_sweep_plan(state.symbols, state.timeframes)
            total_sweeps = len(sweep_plan)
            st.session_state["multi_sweep_plan"] = sweep_plan

            st.info(
                f"üîÑ **Mode multi-sweep s√©quentiel**\n\n"
                f"- {len(state.symbols)} token(s)\n"
                f"- {len(state.timeframes)} timeframe(s)\n"
                f"- {total_sweeps} sweep(s) au total\n\n"
                "Ex√©cution **un par un** pour √©viter la saturation m√©moire."
            )

            with st.expander("üìã Plan des sweeps", expanded=False):
                plan_df = pd.DataFrame(
                    [{"symbol": sym, "timeframe": tf} for sym, tf in sweep_plan]
                )
                st.dataframe(plan_df, width="stretch")

            if optimization_mode == "Grille de Param√®tres":
                n_workers_effective = _resolve_workers(n_workers)
                try:
                    worker_thread_limit = int(
                        st.session_state.get(
                            "grid_worker_threads",
                            int(os.environ.get("BACKTEST_WORKER_THREADS", "1")),
                        )
                    )
                except (TypeError, ValueError):
                    worker_thread_limit = 1
                worker_thread_limit = _resolve_threads(worker_thread_limit)
                _apply_thread_limit(worker_thread_limit, label="main")
                max_runs_per_sweep = None
            else:
                max_runs_per_sweep = None

            overall_progress = st.progress(0.0)
            status_placeholder = st.empty()
            sweep_results: List[Dict[str, Any]] = []
            logger = logging.getLogger(__name__)

            start_str = None
            end_str = None
            if state.use_date_filter and state.start_date and state.end_date:
                start_str = state.start_date.strftime("%Y-%m-%d")
                end_str = state.end_date.strftime("%Y-%m-%d")

            for idx, (sym, tf) in enumerate(sweep_plan, start=1):
                if st.session_state.get("stop_requested", False):
                    st.warning("üõë Arr√™t demand√© par l'utilisateur")
                    break

                status_placeholder.info(
                    f"‚è≥ Sweep {idx}/{total_sweeps}: {strategy_key} √ó {sym} √ó {tf}"
                )

                df, msg = safe_load_data(sym, tf, start=start_str, end=end_str)
                if df is None:
                    sweep_results.append({
                        "strategy": strategy_key,
                        "symbol": sym,
                        "timeframe": tf,
                        "status": "error",
                        "error": msg,
                    })
                    overall_progress.progress(idx / total_sweeps)
                    continue

                combo_engine = BacktestEngine(initial_capital=state.initial_capital)

                if optimization_mode == "Backtest Simple":
                    result, result_msg = safe_run_backtest(
                        combo_engine,
                        df,
                        strategy_key,
                        params,
                        sym,
                        tf,
                        silent_mode=not debug_enabled,
                    )

                    if result is None:
                        sweep_results.append({
                            "strategy": strategy_key,
                            "symbol": sym,
                            "timeframe": tf,
                            "status": "error",
                            "error": result_msg,
                        })
                    else:
                        sweep_results.append({
                            "strategy": strategy_key,
                            "symbol": sym,
                            "timeframe": tf,
                            "status": "ok",
                            "best_params": result.meta.get("params", params),
                            "metrics": result.metrics or {},
                        })
                        _maybe_auto_save_run(result)
                else:
                    progress_placeholder = st.empty()
                    stats_placeholder = st.empty()
                    if n_workers_effective > 1 and max_runs_per_sweep != 1:
                        sweep_summary = _run_grid_parallel_basic(
                            df=df,
                            strategy_key=strategy_key,
                            symbol=sym,
                            timeframe=tf,
                            params=params,
                            param_ranges=param_ranges,
                            max_runs=max_runs_per_sweep,
                            initial_capital=state.initial_capital,
                            n_workers=n_workers_effective,
                            worker_thread_limit=worker_thread_limit,
                            debug_enabled=debug_enabled,
                            progress_placeholder=progress_placeholder,
                            stats_placeholder=stats_placeholder,
                        )
                    else:
                        sweep_summary = _run_grid_sequential(
                            df=df,
                            engine=combo_engine,
                            strategy_key=strategy_key,
                            symbol=sym,
                            timeframe=tf,
                            params=params,
                            param_ranges=param_ranges,
                            max_runs=max_runs_per_sweep,
                            debug_enabled=debug_enabled,
                            progress_placeholder=progress_placeholder,
                        )
                    progress_placeholder.empty()
                    stats_placeholder.empty()

                    sweep_results.append({
                        "strategy": strategy_key,
                        "symbol": sym,
                        "timeframe": tf,
                        "status": "ok",
                        "best_params": sweep_summary.get("best_params", {}),
                        "metrics": sweep_summary.get("best_metrics", {}),
                        "completed": sweep_summary.get("completed", 0),
                        "failed": sweep_summary.get("failed", 0),
                        "total_runs": sweep_summary.get("total_runs", 0),
                    })

                # Nettoyage m√©moire entre sweeps
                try:
                    del df
                except Exception:
                    pass
                clear_data_cache()
                safe_copy_cleanup(logger)
                gc.collect()

                overall_progress.progress(idx / total_sweeps)

            status_placeholder.empty()
            st.session_state["multi_sweep_results"] = sweep_results

            if sweep_results:
                summary_rows = []
                for item in sweep_results:
                    metrics = item.get("metrics", {}) or {}
                    summary_rows.append({
                        "strategy": item.get("strategy"),
                        "symbol": item.get("symbol"),
                        "timeframe": item.get("timeframe"),
                        "status": item.get("status"),
                        "total_pnl": metrics.get("total_pnl", 0.0),
                        "sharpe_ratio": metrics.get("sharpe_ratio", 0.0),
                        "max_drawdown": metrics.get("max_drawdown_pct", metrics.get("max_drawdown", 0.0)),
                        "win_rate": metrics.get("win_rate_pct", metrics.get("win_rate", 0.0)),
                        "total_runs": item.get("total_runs"),
                        "completed": item.get("completed"),
                        "failed": item.get("failed"),
                        "error": item.get("error"),
                    })

                results_df = pd.DataFrame(summary_rows)
                st.markdown("### ‚úÖ R√©sum√© Multi-Sweep")
                st.dataframe(results_df, width="stretch")

                ok_df = results_df[results_df["status"] == "ok"].copy()
                if not ok_df.empty:
                    best_row = ok_df.loc[ok_df["total_pnl"].idxmax()]
                    st.success(
                        f"üèÜ Meilleur r√©sultat: {best_row['symbol']} {best_row['timeframe']} "
                        f"| PnL ${best_row['total_pnl']:,.2f} | Sharpe {best_row['sharpe_ratio']:.2f}"
                    )

                    tab_table, tab_heatmap, tab_rank = st.tabs(
                        ["üìä Tableau", "üî• Heatmap", "üèÜ Classement"]
                    )
                    with tab_table:
                        st.dataframe(ok_df, width="stretch")
                    with tab_heatmap:
                        render_multi_sweep_heatmap(ok_df, metric="total_pnl")
                    with tab_rank:
                        render_multi_sweep_ranking(ok_df, metric="total_pnl", top_n=min(15, len(ok_df)))
                else:
                    st.warning("Aucun sweep r√©ussi.")

            st.session_state.is_running = False
            return

        with st.spinner("üì• Chargement des donn√©es..."):
            df = st.session_state.get("ohlcv_df")
            data_msg = st.session_state.get("ohlcv_status_msg", "")

            if df is None:
                df, data_msg = load_selected_data(
                    symbol,
                    timeframe,
                    state.start_date,
                    state.end_date,
                )

            if df is None:
                with status_container:
                    show_status("error", f"√âchec chargement: {data_msg}")
                    st.info(
                        "üí° V√©rifiez les fichiers dans "
                        "`D:\\ThreadX_big\\data\\crypto\\processed\\parquet\\`"
                    )
                st.session_state.is_running = False
                st.stop()

            if df is not None:
                with status_container:
                    show_status("success", f"Donn√©es charg√©es: {data_msg}")

        engine = BacktestEngine(initial_capital=state.initial_capital)

        if optimization_mode == "Backtest Simple":
            with st.spinner("‚öôÔ∏è Ex√©cution du backtest..."):
                result, result_msg = safe_run_backtest(
                    engine,
                    df,
                    strategy_key,
                    params,
                    symbol,
                    timeframe,
                    silent_mode=not debug_enabled,
                )

            if result is None:
                with status_container:
                    show_status("error", f"√âchec backtest: {result_msg}")
                st.session_state.is_running = False
                st.stop()

            with status_container:
                show_status("success", f"Backtest termin√©: {result_msg}")
            winner_params = result.meta.get("params", params)
            winner_metrics = result.metrics
            winner_origin = "backtest"
            winner_meta = result.meta
            st.session_state["last_run_result"] = result
            st.session_state["last_winner_params"] = winner_params
            st.session_state["last_winner_metrics"] = winner_metrics
            st.session_state["last_winner_origin"] = winner_origin
            st.session_state["last_winner_meta"] = winner_meta
            _maybe_auto_save_run(result)

        elif optimization_mode == "Grille de Param√®tres":
            n_workers_effective = _resolve_workers(n_workers)
            # Lire threads depuis UI ou fallback env
            try:
                worker_thread_limit = int(st.session_state.get("grid_worker_threads",
                                                                 int(os.environ.get("BACKTEST_WORKER_THREADS", "1"))))
            except (TypeError, ValueError):
                worker_thread_limit = 1
            worker_thread_limit = _resolve_threads(worker_thread_limit)
            _apply_thread_limit(worker_thread_limit, label="main")

            with st.spinner("üìä G√©n√©ration de la grille..."):
                try:
                    param_names = list(param_ranges.keys())
                    param_values_lists = []

                    if param_names:
                        for pname in param_names:
                            r = param_ranges[pname]
                            pmin, pmax, step = r["min"], r["max"], r["step"]

                            if isinstance(pmin, int) and isinstance(step, int):
                                values = list(range(int(pmin), int(pmax) + 1, int(step)))
                            else:
                                values = list(
                                    np.arange(float(pmin), float(pmax) + float(step) / 2, float(step))
                                )
                                values = [round(v, 2) for v in values if v <= pmax]

                            if not values:
                                values = [pmin]

                            param_values_lists.append(values)

                        total_combinations = max(
                            1, math.prod(len(values) for values in param_values_lists)
                        )
                        combo_iter = (
                            {**params, **dict(zip(param_names, combo))}
                            for combo in product(*param_values_lists)
                        )
                    else:
                        total_combinations = 1
                        combo_iter = iter([params.copy()])

                    total_runs = total_combinations

                    if total_runs < total_combinations:
                        show_status(
                            "info",
                            f"Grille: {total_runs:,} / {total_combinations:,} combinaisons ({n_workers_effective} workers √ó {worker_thread_limit} threads)",
                        )
                    else:
                        show_status("info", f"Grille: {total_runs:,} combinaisons ({n_workers_effective} workers √ó {worker_thread_limit} threads)")

                except Exception as exc:
                    show_status("error", f"√âchec g√©n√©ration grille: {exc}")
                    st.session_state.is_running = False
                    st.stop()

            # ‚úÖ CRITIQUE: D√©finir fast_metrics ICI pour qu'il soit accessible aux fonctions imbriqu√©es
            # D√©terminer si on utilise les m√©triques rapides (sweeps >500 runs)
            try:
                fast_threshold = int(os.getenv("BACKTEST_SWEEP_FAST_METRICS_THRESHOLD", "500"))
            except (TypeError, ValueError):
                fast_threshold = 500
            fast_metrics = total_runs >= fast_threshold

            results_list = []
            param_combos_map = {}

            monitor = ProgressMonitor(total_runs=total_runs)
            monitor_placeholder = st.empty()

            sweep_monitor = SweepMonitor(
                total_combinations=total_runs,
                objectives=["total_pnl", "theoretical_pnl", "sharpe_ratio", "total_return_pct", "max_drawdown"],
                top_k=15,
            )
            sweep_monitor.start()
            sweep_placeholder = st.empty()

            logger = logging.getLogger(__name__)
            error_counts: Dict[str, int] = {}
            error_logged: set[str] = set()
            try:
                error_log_limit = int(os.environ.get("BACKTEST_SWEEP_ERROR_LOG_LIMIT", "3"))
            except (TypeError, ValueError):
                error_log_limit = 3

            st.markdown("### üìä Progression en temps r√©el")
            render_progress_monitor(monitor, monitor_placeholder)


            def _normalize_param_combo(param_combo: Dict[str, Any]) -> Dict[str, Any]:
                return {
                    k: float(v) if hasattr(v, "item") else v for k, v in param_combo.items()
                }

            def _params_to_str(param_combo: Dict[str, Any]) -> str:
                return str(_normalize_param_combo(param_combo))

            def run_single_backtest(param_combo: Dict[str, Any]):
                try:
                    # ‚úÖ CRITIQUE: Utiliser fast_metrics pour sweeps s√©quentiels aussi
                    result_i, msg_i = safe_run_backtest(
                        engine,
                        df,
                        strategy_key,
                        param_combo,
                        symbol,
                        timeframe,
                        silent_mode=not debug_enabled,
                        fast_metrics=fast_metrics,  # ‚úÖ Activer m√©triques rapides
                    )

                    params_str = _params_to_str(param_combo)

                    if result_i:
                        m = result_i.metrics
                        # Log des cl√©s disponibles si debug activ√©
                        if debug_enabled and not m:
                            logger.warning("Metrics vides pour params=%s", params_str)
                        return {
                            "params": params_str,
                            "params_dict": param_combo,
                            "total_pnl": m.get("total_pnl", 0.0),
                            "theoretical_pnl": m.get("theoretical_pnl", 0.0),
                            "sharpe": m.get("sharpe_ratio", 0.0),
                            "max_dd": m.get("max_drawdown_pct", m.get("max_drawdown", 0.0)),
                            "win_rate": m.get("win_rate", 0.0),
                            "trades": m.get("total_trades", 0),
                            "profit_factor": m.get("profit_factor", 0.0),
                        }
                    return {
                        "params": params_str,
                        "params_dict": param_combo,
                        "error": msg_i,
                    }
                except Exception as exc:
                    params_str = _params_to_str(param_combo)
                    # Log complet de l'erreur avec traceback
                    if debug_enabled:
                        logger.error("Backtest error params=%s: %s", params_str, traceback.format_exc())
                    return {
                        "params": params_str,
                        "params_dict": param_combo,
                        "error": f"{type(exc).__name__}: {exc}",
                    }

            def record_sweep_result(
                result: Dict[str, Any],
                fallback_params: Dict[str, Any],
            ) -> str:
                param_combo_result = result.get("params_dict") or fallback_params
                params_str = result.get("params") or _params_to_str(param_combo_result)
                result["params"] = params_str
                param_combos_map[params_str] = param_combo_result

                if "error" not in result:
                    metrics = {
                        "sharpe_ratio": result.get("sharpe", 0.0),
                        "total_pnl": result.get("total_pnl", 0.0),
                        "theoretical_pnl": result.get("theoretical_pnl", 0.0),
                        "total_return_pct": result.get("total_pnl", 0.0) / state.initial_capital * 100 if state.initial_capital else 0.0,
                        "max_drawdown": abs(result.get("max_dd", 0.0)),
                        "win_rate": result.get("win_rate", 0.0),
                        "total_trades": result.get("trades", 0),
                        "profit_factor": result.get("profit_factor", 0.0),
                        "consecutive_losses_max": result.get("consecutive_losses_max", 0),
                        "avg_win_loss_ratio": result.get("avg_win_loss_ratio", 0.0),
                        "robustness_score": result.get("robustness_score", 0.0),
                    }
                    sweep_monitor.update(params=param_combo_result, metrics=metrics)
                else:
                    error_msg = str(result.get("error") or "Erreur inconnue")
                    error_counts[error_msg] = error_counts.get(error_msg, 0) + 1
                    if len(error_logged) < error_log_limit and error_msg not in error_logged:
                        logger.error("Sweep error sample: %s", error_msg)
                        error_logged.add(error_msg)
                    sweep_monitor.update(params=param_combo_result, metrics={}, error=True)

                result_clean = {k: v for k, v in result.items() if k != "params_dict"}
                results_list.append(result_clean)
                return params_str

            completed_params = set()
            completed = 0
            last_render_time = time.perf_counter()

            def run_sequential_combos(combo_source, key_prefix: str) -> None:
                nonlocal completed, last_render_time
                for param_combo in combo_source:
                    params_str = _params_to_str(param_combo)
                    if params_str in completed_params:
                        continue

                    completed += 1
                    monitor.runs_completed = completed

                    result = run_single_backtest(param_combo)
                    params_str = record_sweep_result(result, param_combo)
                    completed_params.add(params_str)

                    current_time = time.perf_counter()
                    # üöÄ FIX FREEZE EDGE: Throttling ULTRA lent pour garder les graphiques sans freeze
                    # Update tous les 1000 runs OU toutes les 30 secondes (au lieu de 50/2s initialement)
                    # Cela r√©duit les messages WebSocket de 300+ √† ~10-15 en 10 minutes
                    if completed % 1000 == 0 or current_time - last_render_time >= 30.0:
                        render_progress_monitor(monitor, monitor_placeholder)
                        # R√©activer graphiques avec throttling ultra lent + mode statique
                        from ui.components.sweep_monitor import render_sweep_progress
                        with sweep_placeholder.container():
                            progress_pct = (completed / total_runs * 100) if total_runs > 0 else 0
                            st.info(f"‚ö° {completed}/{total_runs} runs compl√©t√©s ({progress_pct:.1f}%)")

                            # Afficher le meilleur PnL actuel + m√©triques critiques
                            if hasattr(sweep_monitor, '_results') and sweep_monitor._results:
                                # Extraire m√©triques - optimis√© avec un seul parcours
                                best_pnl = float("-inf")
                                best_sharpe = float("-inf")
                                best_pf = 0.0
                                best_robustness = 0.0
                                best_run = None

                                for r in sweep_monitor._results:
                                    pnl = r.metrics.get("total_pnl", float("-inf"))
                                    sharpe = r.metrics.get("sharpe_ratio", float("-inf"))
                                    pf = r.metrics.get("profit_factor", 0.0)
                                    rob = r.metrics.get("robustness_score", 0.0)

                                    if pnl > best_pnl:
                                        best_pnl = pnl
                                    if sharpe > best_sharpe:
                                        best_sharpe = sharpe
                                    if pf > best_pf:
                                        best_pf = pf
                                    if rob > best_robustness:
                                        best_robustness = rob
                                        best_run = r

                                # Fallback sur PnL si aucun robustness valide
                                if best_run is None:
                                    best_run = max(sweep_monitor._results, key=lambda r: r.metrics.get("total_pnl", float("-inf")))

                                best_wl_ratio = best_run.metrics.get("avg_win_loss_ratio", 0.0)
                                best_consec_losses = best_run.metrics.get("consecutive_losses_max", 0)

                                pnl_color = "green" if best_pnl > 0 else "red"

                                # Affichage compact des m√©triques critiques
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.markdown(f"üí∞ **Meilleur PnL**: :{pnl_color}[**${best_pnl:+,.2f}**]")
                                    st.caption(f"üìä Sharpe: **{best_sharpe:.2f}** | PF: **{best_pf:.2f}**")
                                with col2:
                                    robustness_color = "green" if best_robustness > 2.0 else "orange" if best_robustness > 1.0 else "red"
                                    st.markdown(f"üéØ **Robustesse**: :{robustness_color}[**{best_robustness:.2f}**]")
                                    st.caption(f"üìà Sharpe √ó PF (id√©al >3.0)")
                                with col3:
                                    wl_color = "green" if best_wl_ratio > 2.0 else "orange" if best_wl_ratio > 1.5 else "red"
                                    st.markdown(f"‚öñÔ∏è **W/L Ratio**: :{wl_color}[**{best_wl_ratio:.2f}**]")
                                    st.caption(f"üíî Max pertes: **{best_consec_losses}** cons√©cutives")

                            # Graphiques avec mode statique (pas d'interactivit√© = moins de donn√©es WebSocket)
                            render_sweep_progress(
                                sweep_monitor,
                                key=f"sweep_progress_seq_{completed}",
                                static_plots=True  # D√©sactiver interactivit√© Plotly
                            )
                            st.caption(f"üîÑ Rafra√Æchissement: tous les 1000 runs ou 30s (throttling anti-freeze)")
                        last_render_time = current_time
                        time.sleep(0.01)

            if n_workers_effective > 1:
                os.environ.setdefault("BACKTEST_INDICATOR_DISK_CACHE", "0")

            if n_workers_effective > 1 and total_runs > 1:
                from concurrent.futures import FIRST_COMPLETED, ProcessPoolExecutor, TimeoutError as FutureTimeoutError, wait
                try:
                    from concurrent.futures import BrokenProcessPool
                except ImportError:  # pragma: no cover - fallback for older runtimes
                    BrokenProcessPool = RuntimeError

                # Syst√®me de diagnostic
                from utils.sweep_diagnostics import SweepDiagnostics
                diag = SweepDiagnostics(run_id=f"grid_{strategy_key}")
                diag.log_pool_start(n_workers_effective, worker_thread_limit, total_runs)

                logger = logging.getLogger(__name__)
                stall_timeout_sec = float(os.getenv("BACKTEST_SWEEP_STALL_SEC", "60"))
                stall_startup_sec = float(os.getenv("BACKTEST_SWEEP_STALL_STARTUP_SEC", "180"))
                # ‚úÖ FIX #1: Augmenter max_inflight pour alimenter tous les workers
                # Avant: n_workers √ó 2 = 48 t√¢ches pour 24 workers (workers idle 50% du temps)
                # Apr√®s: n_workers √ó 8 = 192 t√¢ches pour 24 workers (workers toujours aliment√©s)
                max_inflight = max(1, min(total_runs, n_workers_effective * 8))
                pending = {}
                failed_pending = []
                pool_failed = False
                pool_fail_reason = None
                pool_error: Exception | None = None
                pool_start_time = time.perf_counter()
                last_completion_time = time.perf_counter()
                recent_durations_sec = deque(maxlen=20)
                pickle_error_count = 0  # Compteur d'erreurs de pickling
                combo_counter = 0  # Compteur pour diagnostics

                # Import de l'initializer optimis√© qui charge le DataFrame une seule fois par worker
                from backtest.worker import init_worker_with_dataframe

                # ‚úÖ FIX #5: D√©finir executor AVANT submit_next() pour √©viter closure sur variable non d√©finie
                executor = ProcessPoolExecutor(
                    max_workers=n_workers_effective,
                    initializer=init_worker_with_dataframe,
                    initargs=(
                        df,  # DataFrame charg√© UNE SEULE FOIS par worker
                        strategy_key,
                        symbol,
                        timeframe,
                        state.initial_capital,
                        debug_enabled,
                        worker_thread_limit,
                        fast_metrics,  # ‚úÖ CRITIQUE: Activer m√©triques rapides pour sweeps
                        False,         # is_path (DataFrame fourni directement, pas un chemin)
                    ),
                )

                # ‚úÖ FIX #5 (suite): D√©finir submit_next() APR√àS executor
                def submit_next() -> bool:
                    nonlocal combo_counter
                    try:
                        param_combo = next(combo_iter)
                    except StopIteration:
                        return False
                    combo_counter += 1
                    diag.log_submit(combo_counter, param_combo)
                    submit_ts = time.perf_counter()
                    future = executor.submit(_isolated_worker, param_combo)
                    pending[future] = (param_combo, submit_ts)
                    return True

                try:
                    for _ in range(max_inflight):
                        if not submit_next():
                            break

                    while pending:
                        # ‚úÖ FIX #2: R√©duire timeout de 0.5s √† 0.05s (10√ó plus rapide)
                        # Avant: Latence de 500ms entre chaque v√©rification
                        # Apr√®s: Latence de 50ms (workers aliment√©s 10√ó plus vite)
                        done, _ = wait(pending, timeout=0.05, return_when=FIRST_COMPLETED)
                        if not done:
                            now = time.perf_counter()
                            if completed == 0:
                                stall_threshold_sec = stall_startup_sec
                                stalled = (now - pool_start_time) >= stall_threshold_sec
                            else:
                                avg_duration = (
                                    sum(recent_durations_sec) / len(recent_durations_sec)
                                    if recent_durations_sec else 0.0
                                )
                                stall_threshold_sec = max(
                                    stall_timeout_sec,
                                    avg_duration * 3 if avg_duration > 0 else stall_timeout_sec,
                                )
                                stalled = (now - last_completion_time) >= stall_threshold_sec

                            if stalled:
                                pool_failed = True
                                pool_fail_reason = "stall"
                                pool_error = TimeoutError(
                                    f"Aucune completion depuis {stall_threshold_sec:.0f}s"
                                )
                                diag.log_stall(stall_threshold_sec, len(pending))
                                logger.error(
                                    "Sweep multiprocess bloque depuis %ss, bascule sequentielle.",
                                    int(stall_threshold_sec),
                                )
                                break
                            continue

                        for future in done:
                            param_combo, submit_ts = pending.pop(future)
                            result = None
                            should_record = True

                            try:
                                # Timeout 300s pour √©viter freeze si Windows interrupt (Task Manager, focus change, etc.)
                                result = future.result(timeout=300)
                                duration_ms = (time.perf_counter() - submit_ts) * 1000
                                recent_durations_sec.append(duration_ms / 1000.0)

                                # Log completion
                                combo_idx = combo_counter - len(pending) - len(failed_pending)
                                diag.log_completion(combo_idx, param_combo, result, duration_ms)

                                # D√©tecter erreur de pickling dans le r√©sultat
                                if isinstance(result, dict) and result.get("error", ""):
                                    error_msg = str(result.get("error", ""))
                                    if "pickle" in error_msg.lower() or "not the same object" in error_msg:
                                        pickle_error_count += 1
                                        if pickle_error_count >= 10:
                                            pool_failed = True
                                            pool_fail_reason = "pickle"
                                            pool_error = RuntimeError(
                                                "Erreur de pickling d√©tect√©e - Streamlit a recharg√© le module. "
                                                "Relancez le sweep apr√®s le rechargement."
                                            )
                                            logger.error(
                                                "Erreur de pickling r√©p√©t√©e (%d fois), arr√™t du sweep.",
                                                pickle_error_count,
                                            )
                                            failed_pending.append(param_combo)
                                            should_record = False
                                            break

                            except BrokenProcessPool as exc:
                                combo_idx = combo_counter - len(pending) - len(failed_pending)
                                diag.log_pool_broken("BrokenProcessPool", exc)
                                pool_failed = True
                                pool_fail_reason = "broken"
                                pool_error = exc
                                failed_pending.append(param_combo)
                                should_record = False

                                break

                            except FutureTimeoutError:
                                # Worker timeout (>300s) - probablement bloqu√© par interruption Windows
                                combo_idx = combo_counter - len(pending) - len(failed_pending)
                                diag.log_timeout(combo_idx, param_combo, 300)
                                logger.warning("Worker timeout (>300s) combo: %s", param_combo)
                                result = {
                                    "params": _params_to_str(param_combo),
                                    "params_dict": param_combo,
                                    "error": "Worker timeout (>300s, probablement bloqu√© par interruption Windows)",
                                }
                                # should_record reste True - on enregistre le timeout comme erreur

                            except Exception as exc:
                                combo_idx = combo_counter - len(pending) - len(failed_pending)
                                diag.log_future_exception(combo_idx, param_combo, exc)
                                error_str = f"{type(exc).__name__}: {exc}"
                                # D√©tecter erreur de pickling dans l'exception
                                if "pickle" in error_str.lower() or "not the same object" in error_str:
                                    pickle_error_count += 1
                                    if pickle_error_count >= 10:
                                        pool_failed = True
                                        pool_fail_reason = "pickle"
                                        pool_error = RuntimeError(
                                            "Erreur de pickling - le module a √©t√© recharg√© pendant le sweep."
                                        )
                                        failed_pending.append(param_combo)
                                        should_record = False
                                        break
                                result = {
                                    "params": _params_to_str(param_combo),
                                    "params_dict": param_combo,
                                    "error": error_str,
                                }
                                # should_record reste True - on enregistre l'erreur

                            # Enregistrer le r√©sultat (sauf si break anticip√©)
                            if should_record and result is not None:
                                completed += 1
                                monitor.runs_completed = completed
                                params_str = record_sweep_result(result, param_combo)
                                completed_params.add(params_str)
                                last_completion_time = time.perf_counter()

                            # ‚ö° CRITIQUE: Soumettre la combinaison suivante UNE SEULE FOIS apr√®s traitement complet
                            # (sauf si pool_failed ou break - dans ce cas on sort de la boucle de toute fa√ßon)
                            if not pool_failed:
                                submit_next()

                            current_time = time.perf_counter()
                            # üöÄ FIX FREEZE EDGE: Throttling ULTRA lent pour garder les graphiques sans freeze
                            # Update tous les 1000 runs OU toutes les 30 secondes
                            if completed % 1000 == 0 or current_time - last_render_time >= 30.0:
                                render_progress_monitor(monitor, monitor_placeholder)
                                # R√©activer graphiques avec throttling ultra lent + mode statique
                                from ui.components.sweep_monitor import render_sweep_progress
                                with sweep_placeholder.container():
                                    progress_pct = (completed / total_runs * 100) if total_runs > 0 else 0
                                    st.info(f"‚ö° {completed}/{total_runs} runs ({progress_pct:.1f}%)")

                                    # Afficher le meilleur PnL actuel + m√©triques critiques
                                    if hasattr(sweep_monitor, '_results') and sweep_monitor._results:
                                        # Extraire m√©triques - optimis√© avec un seul parcours
                                        best_pnl = float("-inf")
                                        best_sharpe = float("-inf")
                                        best_pf = 0.0
                                        best_robustness = 0.0
                                        best_run = None

                                        for r in sweep_monitor._results:
                                            pnl = r.metrics.get("total_pnl", float("-inf"))
                                            sharpe = r.metrics.get("sharpe_ratio", float("-inf"))
                                            pf = r.metrics.get("profit_factor", 0.0)
                                            rob = r.metrics.get("robustness_score", 0.0)

                                            if pnl > best_pnl:
                                                best_pnl = pnl
                                            if sharpe > best_sharpe:
                                                best_sharpe = sharpe
                                            if pf > best_pf:
                                                best_pf = pf
                                            if rob > best_robustness:
                                                best_robustness = rob
                                                best_run = r

                                        # Fallback sur PnL si aucun robustness valide
                                        if best_run is None:
                                            best_run = max(sweep_monitor._results, key=lambda r: r.metrics.get("total_pnl", float("-inf")))

                                        best_wl_ratio = best_run.metrics.get("avg_win_loss_ratio", 0.0)
                                        best_consec_losses = best_run.metrics.get("consecutive_losses_max", 0)

                                        pnl_color = "green" if best_pnl > 0 else "red"

                                        # Affichage compact des m√©triques critiques
                                        col1, col2, col3 = st.columns(3)
                                        with col1:
                                            st.markdown(f"üí∞ **Meilleur PnL**: :{pnl_color}[**${best_pnl:+,.2f}**]")
                                            st.caption(f"üìä Sharpe: **{best_sharpe:.2f}** | PF: **{best_pf:.2f}**")
                                        with col2:
                                            robustness_color = "green" if best_robustness > 2.0 else "orange" if best_robustness > 1.0 else "red"
                                            st.markdown(f"üéØ **Robustesse**: :{robustness_color}[**{best_robustness:.2f}**]")
                                            st.caption(f"üìà Sharpe √ó PF (id√©al >3.0)")
                                        with col3:
                                            wl_color = "green" if best_wl_ratio > 2.0 else "orange" if best_wl_ratio > 1.5 else "red"
                                            st.markdown(f"‚öñÔ∏è **W/L Ratio**: :{wl_color}[**{best_wl_ratio:.2f}**]")
                                            st.caption(f"üíî Max pertes: **{best_consec_losses}** cons√©cutives")

                                    # Graphiques avec mode statique
                                    render_sweep_progress(
                                        sweep_monitor,
                                        key=f"sweep_progress_multi_{completed}",
                                        static_plots=True
                                    )
                                    st.caption(f"üîÑ Rafra√Æchissement: tous les 1000 runs ou 30s (throttling anti-freeze)")
                                last_render_time = current_time
                                time.sleep(0.01)

                        if pool_failed:
                            diag.log_pool_broken(pool_fail_reason or "unknown", pool_error)
                            break
                finally:
                    diag.log_pool_shutdown(success=not pool_failed)
                    try:
                        executor.shutdown(
                            wait=not pool_failed,
                            cancel_futures=pool_failed,
                        )
                    except Exception:
                        logger.exception("Erreur shutdown ProcessPoolExecutor")

                if pool_failed:
                    with status_container:
                        if pool_fail_reason == "pickle":
                            show_status(
                                "error",
                                "‚ö†Ô∏è Erreur de pickling: le module a √©t√© recharg√© par Streamlit pendant le sweep. "
                                "Relancez le sweep - il reprendra depuis les combinaisons non test√©es.",
                            )
                        else:
                            show_status(
                                "warning",
                                "Pool multiprocess interrompu, reprise en mode s√©quentiel.",
                            )
                        if pool_error:
                            st.caption(f"D√©tails: {pool_error}")

                    pending_combos = failed_pending + [item[0] for item in pending.values()]
                    if pool_fail_reason == "stall" and pending_combos:
                        logger.warning(
                            "Stall d√©tect√©: %d combinaisons en attente seront relanc√©es en s√©quentiel.",
                            len(pending_combos),
                        )

                    diag.log_sequential_fallback(pool_fail_reason, len(pending_combos))
                    fallback_iter = chain(pending_combos, combo_iter)
                    run_sequential_combos(fallback_iter, "sweep_fallback")
            else:
                run_sequential_combos(combo_iter, "sweep_sequential")

            render_progress_monitor(monitor, monitor_placeholder)
            sweep_placeholder.empty()
            with sweep_placeholder.container():
                render_sweep_progress(
                    sweep_monitor,
                    key="sweep_final",
                    show_top_results=True,
                    show_evolution=True,
                )

            st.markdown("---")
            st.markdown("### üéØ R√©sum√© de l'Optimisation")
            render_sweep_summary(sweep_monitor, key="sweep_summary")

            # Finalize diagnostics
            diag.log_final_summary()
            st.caption(f"üìã Logs diagnostiques: `{diag.log_file}`")

            monitor_placeholder.empty()
            sweep_placeholder.empty()

            with status_container:
                show_status("success", f"Optimisation: {len(results_list)} tests")

            results_df = pd.DataFrame(results_list)

            if "trades" in results_df.columns:
                logger = logging.getLogger(__name__)
                logger.info("=" * 80)
                logger.info("üîç DEBUG GRID SEARCH - Analyse de la colonne 'trades'")
                logger.info("   Type: %s", results_df["trades"].dtype)
                logger.info("   Shape: %s", results_df["trades"].shape)
                logger.info(
                    "   Premi√®res valeurs: %s",
                    results_df["trades"].head(10).tolist(),
                )
                logger.info(
                    "   Stats: min=%s, max=%s, mean=%.2f",
                    results_df["trades"].min(),
                    results_df["trades"].max(),
                    results_df["trades"].mean(),
                )

                trades_values = results_df["trades"].values
                fractional = [
                    x for x in trades_values if isinstance(x, float) and not x.is_integer()
                ]
                if fractional:
                    logger.warning(
                        "   ‚ö†Ô∏è  %s valeurs fractionnaires d√©tect√©es: %s",
                        len(fractional),
                        fractional[:5],
                    )
                else:
                    logger.info("   ‚úÖ Toutes les valeurs sont des entiers")
                logger.info("=" * 80)

            error_items = []
            if error_counts:
                total_errors = sum(error_counts.values())
                with st.expander("‚ùå Erreurs (extraits)", expanded=True):
                    st.caption(
                        f"{total_errors} erreurs detectees. "
                        "Consultez le terminal pour les premiers messages."
                    )
                    error_items = sorted(
                        error_counts.items(), key=lambda item: item[1], reverse=True
                    )
                    error_df = pd.DataFrame(
                        [
                            {"error": msg, "count": count}
                            for msg, count in error_items[:10]
                        ]
                    )
                    st.dataframe(error_df, use_container_width=True)

            error_column = results_df.get("error")
            if error_column is not None:
                valid_results = results_df[error_column.isna()]
            else:
                valid_results = results_df

            if not valid_results.empty:
                valid_results = valid_results.sort_values("sharpe", ascending=False)

                st.subheader("üèÜ Top 10 Combinaisons")

                with st.expander("üîç Debug Info - Types de donn√©es"):
                    st.text(f"Nombre de r√©sultats: {len(valid_results)}")
                    st.text("Types des colonnes:")
                    st.text(str(valid_results.dtypes))
                    if "trades" in valid_results.columns:
                        st.text("\nStatistiques 'trades':")
                        st.text(f"  Type: {valid_results['trades'].dtype}")
                        st.text(f"  Min: {valid_results['trades'].min()}")
                        st.text(f"  Max: {valid_results['trades'].max()}")
                        st.text(
                            f"  Mean: {valid_results['trades'].mean():.2f}"
                        )

                st.dataframe(valid_results.head(10), use_container_width=True)

                best = valid_results.iloc[0]
                st.info(f"ü•á Meilleure: {best['params']}")

                best_params = param_combos_map.get(best["params"], {})
                result, _ = safe_run_backtest(
                    engine,
                    df,
                    strategy_key,
                    best_params,
                    symbol,
                    timeframe,
                    silent_mode=not debug_enabled,
                )
                if result is not None:
                    winner_params = best_params
                    winner_metrics = result.metrics
                    winner_origin = "grid"
                    winner_meta = result.meta
                    st.session_state["last_run_result"] = result
                    st.session_state["last_winner_params"] = winner_params
                    st.session_state["last_winner_metrics"] = winner_metrics
                    st.session_state["last_winner_origin"] = winner_origin
                    st.session_state["last_winner_meta"] = winner_meta
                    _maybe_auto_save_run(result)
            else:
                show_status("error", "Aucun r√©sultat valide")
                # Afficher diagnostic d√©taill√©
                st.markdown("### üîç Diagnostic")
                st.warning(
                    f"Sur {len(results_list)} combinaisons √©valu√©es, "
                    f"toutes ont √©chou√©."
                )
                if error_items:
                    top_error, top_count = error_items[0]
                    st.error(
                        f"**Erreur principale** ({top_count} occurrences sur {sum(error_counts.values())} erreurs):"
                    )
                    st.code(top_error, language="text")
                elif results_list:
                    # Extraire les erreurs du DataFrame si error_counts vide
                    errors_in_results = [
                        r.get("error") for r in results_list if r.get("error")
                    ]
                    if errors_in_results:
                        st.error(f"**Premi√®re erreur d√©tect√©e:**")
                        st.code(errors_in_results[0], language="text")
                        if len(errors_in_results) > 1:
                            st.caption(f"+ {len(errors_in_results)-1} autres erreurs similaires")
                    else:
                        st.info(
                            "Aucune erreur explicite, mais les r√©sultats sont invalides. "
                            "V√©rifiez que les donn√©es OHLCV sont charg√©es et valides."
                        )
                st.session_state.is_running = False
                st.stop()

        elif optimization_mode == "ü§ñ Optimisation LLM":
            if not LLM_AVAILABLE:
                show_status("error", "Module agents LLM non disponible")
                st.code(LLM_IMPORT_ERROR)
                st.session_state.is_running = False
                st.stop()

            if llm_config is None:
                show_status("error", "Configuration LLM incompl√®te")
                st.info("Configurez le provider LLM dans la sidebar")
                st.session_state.is_running = False
                st.stop()

            session_id = generate_session_id()
            orchestration_logger = OrchestrationLogger(session_id=session_id)

            try:
                param_bounds = get_strategy_param_bounds(strategy_key)
                if not param_bounds:
                    param_bounds = {}
                    for pname in params.keys():
                        if pname in PARAM_CONSTRAINTS:
                            c = PARAM_CONSTRAINTS[pname]
                            param_bounds[pname] = (c["min"], c["max"])
            except Exception as exc:
                show_status("warning", f"Bornes par d√©faut utilis√©es: {exc}")
                param_bounds = {}
                for pname in params.keys():
                    if pname in PARAM_CONSTRAINTS:
                        c = PARAM_CONSTRAINTS[pname]
                        param_bounds[pname] = (c["min"], c["max"])

            try:
                full_param_space = get_strategy_param_space(strategy_key, include_step=True)
                llm_space_stats = compute_search_space_stats(full_param_space)
            except Exception:
                llm_space_stats = None

            max_iterations = min(llm_max_iterations, max_combos)

            comparison_summary: List[Dict[str, Any]] = []
            should_run_comparison = llm_compare_enabled and (
                llm_compare_auto_run or st.session_state.get("llm_compare_run_now", False)
            )
            if should_run_comparison:
                st.subheader("Comparaison multi-strategies")
                if not llm_compare_strategies:
                    st.warning("Aucune strategie selectionnee pour la comparaison.")
                elif not llm_compare_tokens or not llm_compare_timeframes:
                    st.warning("Selectionnez au moins un token et un timeframe.")
                else:
                    start_str = str(state.start_date) if state.start_date else None
                    end_str = str(state.end_date) if state.end_date else None
                    progress_bar = st.progress(0)
                    comparison_results: List[Dict[str, Any]] = []
                    comparison_errors: List[str] = []
                    data_cache: Dict[tuple[str, str], pd.DataFrame] = {}

                    for token in llm_compare_tokens:
                        for tf in llm_compare_timeframes:
                            df_cmp, msg = safe_load_data(token, tf, start_str, end_str)
                            if df_cmp is None:
                                comparison_errors.append(f"{token}/{tf}: {msg}")
                            else:
                                data_cache[(token, tf)] = df_cmp

                    valid_pairs = list(data_cache.keys())
                    total_runs = len(valid_pairs) * len(llm_compare_strategies)
                    total_runs = max(0, min(total_runs, llm_compare_max_runs))
                    run_index = 0

                    with st.spinner("Comparaison en cours..."):
                        for strategy_name_cmp in llm_compare_strategies:
                            params_cmp = build_strategy_params_for_comparison(
                                strategy_name_cmp,
                                use_preset=llm_compare_use_preset,
                            )
                            for token, tf in valid_pairs:
                                if run_index >= total_runs:
                                    break
                                df_cmp = data_cache[(token, tf)]
                                result_cmp, status = safe_run_backtest(
                                    engine,
                                    df_cmp,
                                    strategy_name_cmp,
                                    params_cmp,
                                    token,
                                    tf,
                                    silent_mode=not debug_enabled,
                                )
                                if result_cmp is None:
                                    comparison_errors.append(
                                        f"{strategy_name_cmp} {token}/{tf}: {status}"
                                    )
                                else:
                                    comparison_results.append(
                                        {
                                            "strategy": strategy_name_cmp,
                                            "symbol": token,
                                            "timeframe": tf,
                                            "metrics": result_cmp.metrics,
                                            "trades": len(result_cmp.trades),
                                        }
                                    )
                                run_index += 1
                                if total_runs > 0:
                                    progress_bar.progress(run_index / total_runs)
                            if run_index >= total_runs:
                                break

                    if comparison_errors:
                        st.warning(
                            "Comparaison: "
                            + "; ".join(comparison_errors[:8])
                            + (" ..." if len(comparison_errors) > 8 else "")
                        )

                    if comparison_results:
                        comparison_summary = summarize_comparison_results(
                            comparison_results,
                            aggregate=llm_compare_aggregate,
                            primary_metric=llm_compare_metric,
                            expected_runs=len(valid_pairs),
                        )
                        st.caption(
                            f"Runs effectues: {len(comparison_results)} / {total_runs}"
                        )
                        st.dataframe(pd.DataFrame(comparison_summary), use_container_width=True)

                        chart_rows = []
                        for row in comparison_summary:
                            chart_rows.append(
                                {
                                    "name": row["strategy"],
                                    "metrics": {
                                        llm_compare_metric: row.get(llm_compare_metric)
                                    },
                                }
                            )
                        render_comparison_chart(
                            chart_rows,
                            metric=llm_compare_metric,
                            title="Comparaison agregree",
                            key="llm_strategy_comparison",
                        )

                        if llm_compare_generate_report:
                            try:
                                llm_client = create_llm_client(llm_config)
                                if not llm_client.is_available():
                                    st.warning("LLM indisponible pour la justification.")
                                else:
                                    summary_lines = [
                                        "strategy | runs | sharpe | return_pct | max_drawdown | win_rate"
                                    ]
                                    for row in comparison_summary:
                                        summary_lines.append(
                                            f"{row.get('strategy')} | "
                                            f"{row.get('runs')} | "
                                            f"{row.get('sharpe_ratio', float('nan')):.2f} | "
                                            f"{row.get('total_return_pct', float('nan')):.2f} | "
                                            f"{row.get('max_drawdown', float('nan')):.2f} | "
                                            f"{row.get('win_rate', float('nan')):.1f}"
                                        )

                                    system_prompt = (
                                        "You are a senior quantitative strategist. "
                                        "Compare strategy robustness across assets and timeframes."
                                    )
                                    user_message = (
                                        "Comparison scope:\n"
                                        f"- tokens: {', '.join(llm_compare_tokens)}\n"
                                        f"- timeframes: {', '.join(llm_compare_timeframes)}\n"
                                        f"- aggregation: {llm_compare_aggregate}\n"
                                        f"- primary metric: {llm_compare_metric}\n\n"
                                        "Summary table (metrics are percent where applicable):\n"
                                        + "\n".join(summary_lines)
                                        + "\n\n"
                                        "Provide:\n"
                                        "1) Ranking with short justification.\n"
                                        "2) Notes on robustness and risk.\n"
                                        "3) Which strategies deserve further optimization."
                                    )

                                    response = llm_client.simple_chat(
                                        user_message=user_message,
                                        system_prompt=system_prompt,
                                        temperature=0.3,
                                    )
                                    st.markdown("**Justification LLM**")
                                    st.write(response.content)
                            except Exception as exc:
                                st.warning(f"Justification LLM indisponible: {exc}")
                    st.session_state["llm_compare_run_now"] = False

            st.subheader("ü§ñ Optimisation par Agents LLM")

            col_info, col_timeline = st.columns([1, 2])

            with col_info:
                st.markdown(
                    f"""
            **Strat√©gie:** `{strategy_key}`
            **Param√®tres initiaux:** `{params}`
            **Max it√©rations:** {llm_max_iterations}
            **Walk-Forward:** {'‚úÖ' if llm_use_walk_forward else '‚ùå'}
            """
                )

                st.markdown("**Bornes des param√®tres:**")
                for pname, (pmin, pmax) in param_bounds.items():
                    st.caption(f"‚Ä¢ {pname}: [{pmin}, {pmax}]")

                if llm_space_stats:
                    st.markdown("---")
                    if llm_space_stats.is_continuous:
                        st.info("‚ÑπÔ∏è **Espace continu** : exploration adaptative par LLM")
                    else:
                        st.caption(
                            "üìä Espace discret estim√©: "
                            f"~{llm_space_stats.total_combinations:,} combinaisons"
                        )
                        st.caption("_(Le LLM explore de fa√ßon intelligente sans √©num√©rer)_")

            col_timeline.empty()

            strategist = None
            executor = None
            orchestrator = None

            run_tracker = get_global_tracker()
            data_identifier = (
                f"df_{len(df)}rows_{df.index[0]}_{df.index[-1]}"
                if len(df) > 0
                else "empty_df"
            )
            run_signature = RunSignature(
                strategy_name=strategy_key,
                data_path=data_identifier,
                initial_params=params,
                llm_model=llm_model,
                mode="multi_agents" if llm_use_multi_agent else "autonomous",
                session_id=session_id,
            )

            # Enregistrer le run (pour statistiques) sans bloquer l'ex√©cution
            # Note: Le tracking des duplications durant la session est g√©r√© par session_param_tracker
            run_tracker.register(run_signature)

            with st.spinner("üîå Connexion au LLM..."):
                try:
                    if llm_use_multi_agent:
                        live_events_placeholder = st.empty()
                        live_viewer = LiveOrchestrationViewer(
                            container_key="live_orch_viewer_multi"
                        )

                        def on_orchestration_event(entry):
                            live_viewer.add_event(entry)
                            live_viewer.render(live_events_placeholder, show_header=True)

                        orchestration_logger.set_on_event_callback(on_orchestration_event)

                        n_workers_effective = _resolve_workers(n_workers)
                        orchestrator = create_orchestrator_with_backtest(
                            llm_config=llm_config,
                            strategy_name=strategy_key,
                            data=df,
                            initial_params=params,
                            data_symbol=symbol,
                            data_timeframe=timeframe,
                            role_model_config=state.role_model_config,
                            use_walk_forward=llm_use_walk_forward,
                            orchestration_logger=orchestration_logger,
                            session_id=session_id,
                            n_workers=n_workers_effective,
                            max_iterations=max_iterations,
                            initial_capital=state.initial_capital,
                            config=engine.config,
                        )
                        show_status(
                            "success",
                            "Connexion LLM √©tablie (mode multi-agents)",
                        )
                    else:
                        strategist, executor = create_optimizer_from_engine(
                            llm_config=llm_config,
                            strategy_name=strategy_key,
                            data=df,
                            initial_capital=state.initial_capital,
                            use_walk_forward=llm_use_walk_forward,
                            verbose=True,
                            unload_llm_during_backtest=llm_unload_during_backtest,
                            orchestration_logger=orchestration_logger,
                        )
                        show_status("success", "Connexion LLM √©tablie")
                except Exception as exc:
                    show_status("error", f"Echec connexion LLM: {exc}")
                    st.code(traceback.format_exc())
                    st.session_state.is_running = False
                    st.stop()

            if llm_use_multi_agent:
                st.markdown("---")
                st.markdown("### Progression multi-agents")
                n_workers_effective = _resolve_workers(n_workers)
                st.caption(
                    f"Limite: {_format_combo_limit(max_combos)} backtests max, "
                    f"{n_workers_effective} workers, {max_iterations} iterations max"
                )

                if orchestrator is None:
                    show_status("error", "Orchestrator non initialise")
                    st.session_state.is_running = False
                    st.stop()

                try:
                    with st.spinner("Optimisation multi-agents en cours..."):
                        orchestrator_result = orchestrator.run()

                    try:
                        orchestration_logger.save_to_jsonl()
                    except Exception:
                        pass

                    if orchestrator_result.errors:
                        st.warning(
                            f"Orchestration errors: {len(orchestrator_result.errors)}"
                        )
                    if orchestrator_result.warnings:
                        st.warning(
                            f"Orchestration warnings: {len(orchestrator_result.warnings)}"
                        )

                    if orchestrator_result.success:
                        st.success("Optimisation multi-agents terminee")
                    else:
                        st.warning(
                            "Optimisation multi-agents terminee "
                            f"(decision: {orchestrator_result.decision})"
                        )

                    if orchestrator_result.final_params:
                        st.subheader("Resultat multi-agents")
                        st.json(orchestrator_result.final_params)
                    else:
                        st.warning("Aucun parametre final retourne")

                    if orchestrator_result.final_metrics:
                        metrics = orchestrator_result.final_metrics
                        col_a, col_b, col_c = st.columns(3)
                        with col_a:
                            st.metric("Sharpe", f"{metrics.sharpe_ratio:.3f}")
                        with col_b:
                            st.metric("Return", f"{metrics.total_return:.2%}")
                        with col_c:
                            st.metric("Max Drawdown", f"{metrics.max_drawdown:.2%}")

                    if orchestrator_result.iteration_history:
                        st.markdown("---")
                        st.dataframe(
                            pd.DataFrame(orchestrator_result.iteration_history),
                            use_container_width=True,
                        )

                    best_params = orchestrator_result.final_params or {}
                    if best_params:
                        result, _ = safe_run_backtest(
                            engine,
                            df,
                            strategy_key,
                            best_params,
                            symbol,
                            timeframe,
                            silent_mode=not debug_enabled,
                        )
                        if result is not None:
                            winner_params = best_params
                            winner_metrics = result.metrics
                            winner_origin = "llm"
                            winner_meta = result.meta
                            st.session_state["last_run_result"] = result
                            st.session_state["last_winner_params"] = winner_params
                            st.session_state["last_winner_metrics"] = winner_metrics
                            st.session_state["last_winner_origin"] = winner_origin
                            st.session_state["last_winner_meta"] = winner_meta
                            _maybe_auto_save_run(result)
                except Exception as exc:
                    show_status("error", f"Erreur optimisation multi-agents: {exc}")
                    st.code(traceback.format_exc())
                    st.session_state.is_running = False
                    st.stop()
            else:
                st.markdown("---")
                st.markdown("### üìä Progression de l'optimisation LLM")

                live_status = st.status(
                    "üöÄ D√©marrage de l'optimisation...",
                    expanded=True,
                )
                live_events_placeholder = st.empty()
                orchestration_placeholder = st.empty()

                max_iterations = min(llm_max_iterations, max_combos)

                live_viewer = LiveOrchestrationViewer(
                    container_key="live_orch_viewer"
                )

                def on_orchestration_event(entry):
                    live_viewer.add_event(entry)
                    live_viewer.render(live_events_placeholder, show_header=True)

                orchestration_logger.set_on_event_callback(on_orchestration_event)

                n_workers_effective = _resolve_workers(n_workers)
                st.caption(
                    "üîß Limite: "
                    f"{_format_combo_limit(max_combos)} backtests max, {n_workers_effective} workers, "
                    f"{max_iterations} it√©rations max"
                )

                try:
                    with live_status:
                        st.write("ü§ñ **Agent LLM actif** - Optimisation autonome")
                        st.write(
                            f"üìä Strat√©gie: `{strategy_key}` | Mod√®le: `{llm_model}`"
                        )

                        session = strategist.optimize(
                            executor=executor,
                            initial_params=params,
                            param_bounds=param_bounds,
                            max_iterations=max_iterations,
                            min_sharpe=-5.0,
                            max_drawdown=0.50,
                        )

                        live_status.update(
                            label=(
                                "‚úÖ Optimisation termin√©e en "
                                f"{session.current_iteration} it√©rations"
                            ),
                            state="complete",
                            expanded=False,
                        )

                    st.success(
                        f"‚úÖ Optimisation termin√©e en {session.current_iteration} it√©rations"
                    )

                    with st.expander("üìù Historique des it√©rations", expanded=True):
                        for i, exp in enumerate(session.all_results):
                            icon = "üü¢" if exp.sharpe_ratio > 0 else "üî¥"
                            col_it1, col_it2, col_it3 = st.columns([2, 1, 1])
                            with col_it1:
                                st.markdown(f"**It√©ration {i+1}** {icon}")
                                st.caption(
                                    f"Params: `{exp.request.parameters}`"
                                )
                            with col_it2:
                                st.metric("Sharpe", f"{exp.sharpe_ratio:.3f}")
                            with col_it3:
                                st.metric("Return", f"{exp.total_return:.2%}")

                    try:
                        orchestration_logger.save_to_jsonl()
                    except Exception:
                        pass

                    with orchestration_placeholder:
                        st.markdown("---")

                        tab_simple, tab_deep = st.tabs(
                            ["üìã Logs d'orchestration", "üîç Deep Trace (avanc√©)"]
                        )

                        with tab_simple:
                            render_full_orchestration_viewer(
                                orchestration_logger=orchestration_logger,
                                max_entries=50,
                            )

                        with tab_deep:
                            if LLM_AVAILABLE:
                                render_deep_trace_viewer(
                                    logger=orchestration_logger
                                )
                            else:
                                st.warning(
                                    "Module LLM non disponible pour Deep Trace avanc√©"
                                )

                    st.markdown("---")
                    st.subheader("üèÜ R√©sultat de l'optimisation LLM")

                    col_best, col_improve = st.columns(2)

                    with col_best:
                        st.markdown("**Meilleurs param√®tres trouv√©s:**")
                        st.json(session.best_result.request.parameters)

                        st.metric(
                            "Meilleur Sharpe",
                            f"{session.best_result.sharpe_ratio:.3f}",
                        )
                        st.metric(
                            "Return",
                            f"{session.best_result.total_return:.2%}",
                        )

                    with col_improve:
                        if session.all_results:
                            initial_sharpe = session.all_results[0].sharpe_ratio
                            best_sharpe = session.best_result.sharpe_ratio
                            improvement = (
                                (best_sharpe - initial_sharpe) / abs(initial_sharpe) * 100
                            ) if initial_sharpe != 0 else 0

                            st.metric(
                                "Am√©lioration Sharpe",
                                f"{improvement:+.1f}%",
                                delta=f"{best_sharpe - initial_sharpe:+.3f}",
                            )
                            st.metric("It√©rations utilis√©es", session.current_iteration)

                            if session.final_reasoning:
                                st.info(f"üõë Arr√™t: {session.final_reasoning}")

                    best_params = session.best_result.request.parameters
                    result, _ = safe_run_backtest(
                        engine,
                        df,
                        strategy_key,
                        best_params,
                        symbol,
                        timeframe,
                        silent_mode=not debug_enabled,
                    )
                    if result is not None:
                        winner_params = best_params
                        winner_metrics = result.metrics
                        winner_origin = "llm"
                        winner_meta = result.meta
                        st.session_state["last_run_result"] = result
                        st.session_state["last_winner_params"] = winner_params
                        st.session_state["last_winner_metrics"] = winner_metrics
                        st.session_state["last_winner_origin"] = winner_origin
                        st.session_state["last_winner_meta"] = winner_meta
                        _maybe_auto_save_run(result)

                except Exception as exc:
                    live_status.update(label=f"‚ùå Erreur: {exc}", state="error")
                    show_status("error", f"Erreur optimisation LLM: {exc}")
                    st.code(traceback.format_exc())
                    st.session_state.is_running = False
                    st.stop()

        else:
            show_status("error", f"Mode non reconnu: {optimization_mode}")
            st.session_state.is_running = False
            st.stop()

    st.session_state.is_running = False
