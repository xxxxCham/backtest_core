"""
Module-ID: ui.emergency_stop

Purpose: SystÃ¨me d'arrÃªt d'urgence et nettoyage mÃ©moire complet.

Role in pipeline: ui / cleanup

Key components: EmergencyStopHandler, full_memory_cleanup

Inputs: Session state, logger

Outputs: RAM/VRAM libÃ©rÃ©e, processus arrÃªtÃ©s

Dependencies: gc, psutil, torch, cupy, MemoryManager, GPUMemoryManager

Conventions: Nettoyage agressif tous composants; logs dÃ©taillÃ©s; fallback sur erreurs.

Read-if: Modification logique arrÃªt d'urgence

Skip-if: Utilisation normale du bouton stop
"""

from __future__ import annotations

import gc
import logging
import threading
import time
from typing import Any, Dict, Optional

logger = logging.getLogger(__name__)


class EmergencyStopHandler:
    """
    Gestionnaire d'arrÃªt d'urgence avec nettoyage mÃ©moire complet.

    ResponsabilitÃ©s:
    - ArrÃªter tous les threads/processus en cours
    - Vider les caches (indicateurs, LLM, GPU)
    - LibÃ©rer RAM et VRAM
    - RÃ©initialiser les Ã©tats Streamlit
    """

    def __init__(self):
        self._stop_event = threading.Event()
        self._cleanup_stats: Dict[str, Any] = {}

    def request_stop(self) -> None:
        """Demande l'arrÃªt de tous les processus en cours."""
        self._stop_event.set()
        logger.warning("ğŸ›‘ ARRÃŠT D'URGENCE DEMANDÃ‰")

    def is_stop_requested(self) -> bool:
        """VÃ©rifie si un arrÃªt a Ã©tÃ© demandÃ©."""
        return self._stop_event.is_set()

    def reset_stop(self) -> None:
        """RÃ©initialise le flag d'arrÃªt."""
        self._stop_event.clear()

    def full_cleanup(self, session_state: Optional[Any] = None) -> Dict[str, Any]:
        """
        Effectue un nettoyage mÃ©moire complet de tous les composants.

        Args:
            session_state: Ã‰tat de session Streamlit (optionnel)

        Returns:
            Dict avec statistiques de nettoyage
        """
        stats = {
            "timestamp": time.time(),
            "components_cleaned": [],
            "errors": [],
            "ram_freed_mb": 0.0,
            "vram_freed_mb": 0.0,
        }

        # 1. ArrÃªter les opÃ©rations en cours
        self._stop_running_operations(session_state, stats)

        # 2. Nettoyer les LLM (dÃ©charger de la VRAM)
        self._cleanup_llm_models(stats)

        # 3. Nettoyer le cache d'indicateurs
        self._cleanup_indicator_cache(stats)

        # 4. Nettoyer CuPy (GPU arrays)
        self._cleanup_cupy(stats)

        # 5. Nettoyer PyTorch (si utilisÃ©)
        self._cleanup_pytorch(stats)

        # 6. Nettoyer le MemoryManager
        self._cleanup_memory_manager(stats)

        # 7. Garbage collection agressif
        self._aggressive_gc(stats)

        # 8. RÃ©initialiser les Ã©tats Streamlit
        if session_state is not None:
            self._reset_session_state(session_state, stats)

        # 9. Mesurer la mÃ©moire libÃ©rÃ©e
        self._measure_freed_memory(stats)

        self._cleanup_stats = stats
        return stats

    def _stop_running_operations(
        self,
        session_state: Optional[Any],
        stats: Dict[str, Any]
    ) -> None:
        """ArrÃªte toutes les opÃ©rations en cours."""
        try:
            # Signaler l'arrÃªt via le flag global
            self.request_stop()

            # ArrÃªter SweepEngine si en cours
            # Note: On ne peut pas accÃ©der Ã  l'instance en cours directement,
            # mais le flag _stop_requested sera vÃ©rifiÃ© dans la boucle
            stats["components_cleaned"].append("sweep_engine_signal")

            # ArrÃªter les agents LLM en cours
            if session_state is not None:
                if hasattr(session_state, "is_running"):
                    session_state.is_running = False
                if hasattr(session_state, "stop_requested"):
                    session_state.stop_requested = True
                stats["components_cleaned"].append("session_flags")

            logger.info("âœ… OpÃ©rations arrÃªtÃ©es")

        except Exception as e:
            logger.error(f"âŒ Erreur arrÃªt opÃ©rations: {e}")
            stats["errors"].append(f"stop_operations: {e}")

    def _cleanup_llm_models(self, stats: Dict[str, Any]) -> None:
        """DÃ©charge tous les modÃ¨les LLM de la VRAM avec vÃ©rification complÃ¨te."""
        try:
            import httpx

            from agents.ollama_manager import unload_model

            # RÃ©cupÃ©rer la liste de TOUS les modÃ¨les chargÃ©s
            try:
                response = httpx.get("http://127.0.0.1:11434/api/ps", timeout=5.0)
                if response.status_code == 200:
                    data = response.json()
                    models = data.get("models", [])

                    if not models:
                        logger.debug("Aucun modÃ¨le LLM chargÃ©")
                        stats["components_cleaned"].append("llm_none_loaded")
                        return

                    # DÃ©charger chaque modÃ¨le avec vÃ©rification
                    n_unloaded = 0
                    for model_info in models:
                        model_name = model_info.get("name", "")
                        if model_name:
                            try:
                                success = unload_model(model_name)
                                if success:
                                    n_unloaded += 1
                                    logger.info(f"ğŸ—‘ï¸ LLM dÃ©chargÃ©: {model_name}")
                                    stats["components_cleaned"].append(f"llm_{model_name}")
                                else:
                                    logger.warning(f"âš ï¸ Ã‰chec dÃ©chargement: {model_name}")
                                    stats["errors"].append(f"llm_unload_{model_name}_failed")
                            except Exception as e:
                                logger.warning(f"âš ï¸ Erreur dÃ©chargement {model_name}: {e}")
                                stats["errors"].append(f"llm_{model_name}: {e}")

                    # VÃ©rification finale : aucun modÃ¨le ne devrait rester
                    response_check = httpx.get("http://127.0.0.1:11434/api/ps", timeout=3.0)
                    if response_check.status_code == 200:
                        remaining = response_check.json().get("models", [])
                        if remaining:
                            logger.warning(f"âš ï¸ {len(remaining)} modÃ¨le(s) encore en mÃ©moire aprÃ¨s cleanup")
                            stats["errors"].append(f"llm_remaining_{len(remaining)}_models")
                        else:
                            logger.info(f"âœ… Tous les LLM dÃ©chargÃ©s ({n_unloaded} modÃ¨les)")

                else:
                    logger.debug(f"Ollama API non accessible (status {response.status_code})")
                    stats["errors"].append(f"ollama_api_status_{response.status_code}")

            except httpx.TimeoutException:
                logger.warning("âš ï¸ Timeout connexion Ollama API")
                stats["errors"].append("ollama_api_timeout")
            except Exception as e:
                logger.debug(f"Impossible de dÃ©charger LLM via API: {e}")
                stats["errors"].append(f"llm_api_error: {e}")

        except ImportError:
            logger.debug("Module agents.ollama_manager non disponible")
            # Pas d'erreur si module non installÃ©
        except Exception as e:
            stats["errors"].append(f"llm_cleanup: {e}")

    def _cleanup_indicator_cache(self, stats: Dict[str, Any]) -> None:
        """Vide le cache d'indicateurs."""
        try:
            from data.indicator_bank import IndicatorBank

            bank = IndicatorBank()

            # D'abord nettoyer les entrÃ©es expirÃ©es
            n_expired = bank.cleanup_expired()
            if n_expired > 0:
                stats["components_cleaned"].append(f"indicator_expired_{n_expired}")

            # Ensuite vider complÃ¨tement le cache si nÃ©cessaire
            # Note: Garder commentÃ© par dÃ©faut pour ne pas perdre le cache
            # DÃ©commenter si vous voulez un nettoyage vraiment complet
            # bank.clear()
            # stats["components_cleaned"].append("indicator_bank_full_clear")

            # Vider le cache mÃ©moire interne
            if hasattr(bank, "_memory_cache"):
                bank._memory_cache.clear()
                stats["components_cleaned"].append("indicator_memory_cache")

            logger.info(f"ğŸ—‘ï¸ Cache indicateurs nettoyÃ© ({n_expired} expirÃ©s)")

        except Exception as e:
            stats["errors"].append(f"indicator_cache: {e}")

    def _cleanup_cupy(self, stats: Dict[str, Any]) -> None:
        """LibÃ¨re toute la mÃ©moire CuPy."""
        try:
            import cupy as cp

            # Vider tous les memory pools
            if hasattr(cp, "get_default_memory_pool"):
                mempool = cp.get_default_memory_pool()
                mempool.free_all_blocks()
                stats["components_cleaned"].append("cupy_memory_pool")

            if hasattr(cp, "get_default_pinned_memory_pool"):
                pinned = cp.get_default_pinned_memory_pool()
                pinned.free_all_blocks()
                stats["components_cleaned"].append("cupy_pinned_pool")

            # Synchroniser tous les devices
            if hasattr(cp.cuda, "Device"):
                n_devices = cp.cuda.runtime.getDeviceCount()
                for i in range(n_devices):
                    with cp.cuda.Device(i):
                        cp.cuda.Stream.null.synchronize()

            logger.info("ğŸ—‘ï¸ CuPy VRAM vidÃ©e")

        except ImportError:
            pass  # CuPy non installÃ©
        except Exception as e:
            stats["errors"].append(f"cupy: {e}")

    def _cleanup_pytorch(self, stats: Dict[str, Any]) -> None:
        """LibÃ¨re le cache PyTorch CUDA."""
        try:
            import torch

            if torch.cuda.is_available():
                # Vider le cache pour tous les devices
                n_devices = torch.cuda.device_count()
                for i in range(n_devices):
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()

                stats["components_cleaned"].append("pytorch_cuda")
                logger.info("ğŸ—‘ï¸ PyTorch VRAM vidÃ©e")

        except ImportError:
            pass  # PyTorch non installÃ©
        except Exception as e:
            stats["errors"].append(f"pytorch: {e}")

    def _cleanup_memory_manager(self, stats: Dict[str, Any]) -> None:
        """Nettoie le MemoryManager central."""
        try:
            from utils.memory import MemoryManager

            # CrÃ©er une instance pour forcer le cleanup
            manager = MemoryManager()
            n_freed = manager.cleanup(aggressive=True)

            if n_freed > 0:
                stats["ram_freed_mb"] += n_freed / (1024**2)
                stats["components_cleaned"].append(f"memory_manager_{n_freed}_bytes")

            # Vider tous les caches managÃ©s
            for cache_name in list(manager._caches.keys()):
                cache = manager._caches[cache_name]
                freed = cache.clear()
                if freed > 0:
                    stats["ram_freed_mb"] += freed / (1024**2)
                    stats["components_cleaned"].append(f"managed_cache_{cache_name}")

            logger.info("ğŸ—‘ï¸ MemoryManager nettoyÃ©")

        except Exception as e:
            stats["errors"].append(f"memory_manager: {e}")

    def _aggressive_gc(self, stats: Dict[str, Any]) -> None:
        """Garbage collection agressif multi-passes."""
        try:
            # 3 passes de GC pour collecter les rÃ©fÃ©rences cycliques
            for i in range(3):
                collected = gc.collect(generation=2)  # Full collection
                if i == 0:
                    stats["gc_collected_objects"] = collected

            stats["components_cleaned"].append("garbage_collector")
            logger.info(f"ğŸ—‘ï¸ GC: {stats.get('gc_collected_objects', 0)} objets collectÃ©s")

        except Exception as e:
            stats["errors"].append(f"gc: {e}")

    def _reset_session_state(
        self,
        session_state: Any,
        stats: Dict[str, Any]
    ) -> None:
        """RÃ©initialise les Ã©tats Streamlit pertinents et nettoie le contexte LLM."""
        try:
            # Flags de contrÃ´le
            if hasattr(session_state, "is_running"):
                session_state.is_running = False
            if hasattr(session_state, "stop_requested"):
                session_state.stop_requested = False

            # Nettoyer TOUS les rÃ©sultats et contextes prÃ©cÃ©dents
            llm_keys_to_clean = [
                "last_run_result",
                "last_winner_params",
                "last_winner_metrics",
                "last_winner_origin",
                "last_winner_meta",
                "orchestration_logs",
                "llm_optimizer",
                "llm_session",
                "current_optimization",
            ]

            n_cleaned = 0
            for key in llm_keys_to_clean:
                if hasattr(session_state, key):
                    delattr(session_state, key)
                    n_cleaned += 1

            if n_cleaned > 0:
                stats["components_cleaned"].append(f"session_llm_context_{n_cleaned}_keys")
                logger.info(f"ğŸ—‘ï¸ Contexte LLM nettoyÃ© ({n_cleaned} clÃ©s)")

            stats["components_cleaned"].append("session_state")
            logger.info("âœ… Session state rÃ©initialisÃ©")

        except Exception as e:
            stats["errors"].append(f"session_state: {e}")

    def _measure_freed_memory(self, stats: Dict[str, Any]) -> None:
        """Mesure approximative de la mÃ©moire libÃ©rÃ©e."""
        try:
            import psutil

            process = psutil.Process()
            mem_info = process.memory_info()

            # Note: On ne peut pas mesurer directement la RAM libÃ©rÃ©e
            # car le GC ne rend pas forcÃ©ment la mÃ©moire au systÃ¨me
            # On indique juste l'usage actuel
            stats["current_ram_mb"] = mem_info.rss / (1024**2)

        except ImportError:
            pass
        except Exception as e:
            stats["errors"].append(f"measure_memory: {e}")

    def get_last_cleanup_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du dernier nettoyage."""
        return self._cleanup_stats.copy()


# Instance singleton
_emergency_handler: Optional[EmergencyStopHandler] = None


def get_emergency_handler() -> EmergencyStopHandler:
    """Retourne le gestionnaire d'arrÃªt d'urgence singleton."""
    global _emergency_handler
    if _emergency_handler is None:
        _emergency_handler = EmergencyStopHandler()
    return _emergency_handler


def execute_emergency_stop(session_state: Optional[Any] = None) -> Dict[str, Any]:
    """
    Raccourci pour exÃ©cuter un arrÃªt d'urgence complet.

    Args:
        session_state: Ã‰tat de session Streamlit

    Returns:
        Dict avec statistiques de nettoyage

    Example:
        >>> stats = execute_emergency_stop(st.session_state)
        >>> st.success(f"âœ… {len(stats['components_cleaned'])} composants nettoyÃ©s")
    """
    handler = get_emergency_handler()
    return handler.full_cleanup(session_state)